
\section{Threat Model \& Assumptions}

\wajih{To give more confidence into that the dual server architecture is realistic, cite more papers. I think this paper also uses https://eprint.iacr.org/2024/081, Also see how they justify dual server.}

\wajih{Add a separate paragraph with a heading "Why Dual-Server Architecture is Realistic" and move all the arguments into that paragraph.}

Our threat model assumes that the central server operates with integrity, conducting the federated averaging process without malicious objectives. However, we recognize the risk that the central server could compromise the privacy of client logs if raw \logs is transmitted to it~\cite{man2021intelligent,li2023efficient}. We also consider the possibility of a curious central server attempting membership inference attacks by utilizing the model weights.

Similar to other works in cryptography and federated learning~\cite{roy2020crypte,wu2022federated}, we assume that the utility server is trusted and that there is no collusion between the central and utility servers, meaning they do not exchange information. To further ensure non-collusion, the utility server can be deployed within a trusted execution environment (TEE)~\cite{mckeen2016intel} or monitored by a trusted mediator overseeing communications between the servers~\cite{alwen2009collusion}. Alternatively, since the utility server's role is limited to processing encrypted data, it can be implemented as a secure cloud compute instance~\cite{cloudinstance} managed by the organization, while the MSSP manages the central server. In this architecture, the central server is responsible for aggregating and coordinating tasks, while the utility server remains under the organization's control. This separation allows the utility server to be operated by trusted internal IT teams, specialized third-party security firms, or a dedicated cloud provider, all of whom prioritize data privacy and encryption. Non-collusion could also be ensured through strict legal agreements.

For individual clients, we expect that attackers could disguise their harmful activities within benign data, making it difficult to distinguish between legitimate and harmful actions. Our model also considers the threat posed by zero-day vulnerabilities. Despite these challenges, we assume that the activities of attackers will be detectable in the system's records (\logs). In line with prior studies on data provenance~\cite{nodoze2019, priotracker2018, mzx2016, bates2017transparent, omegalog, rapsheet2020, provthings2018, dossier, inam2023sok, poirot2019, kwon18mci, winnower2018, lzx2013, ma2015accurate, ma2018kernel, mpi}, our approach relies on the provenance collection system's ability to accurately record all system activities and changes. Additionally, we ensure the integrity of audit logs is maintained through the use of established tamper-resistant storage solutions, such as those described by Paccagnella et al.~\cite{paccagnella2020custos} and the Hardlog system~\cite{hardlog}. Similar to other PIDS works~\cite{cheng2023kairos, flash2024, yangprographer, wang2022threatrace, provdetector2020}, we assume the absence of any attack activities during the training phase. 

% We consider the potential of a curious central server attempting membership inference attacks by utilizing the model weights. The central server could theoretically perform this by generating diverse node input features through various combinations of graph structures and node attributes, and then observing the model's output. However, the search space for such an operation is large, necessitating substantial resources. Moreover, the central server's lack of access to the semantic encoder model for feature encoding significantly impedes the feasibility of this attack. Nevertheless, our system remains susceptible to model poisoning attacks perpetrated by malicious clients.

%\wajih{In the threat model, I would like to know about how you deal with membership inference, poisoning, gradient attacks or not deal with. In general, it should specify the limitations of privacy of our privacy-preserving techniques. Also, specifying what specific information (host interactions, process names, etc.) the attacker can learn from gradients. More specific you are the better it is. I have added some papers in the related that will help you think about privacy more in FL settings.}

% \wajih{Reading Nature's paper they have the following limitations:
% "However, FedPerGNN has the following limitations. First,
% FedPerGNN relies on the assumption that third-party server is
% trusted and does not collude with the recommendation server,
% which is somewhat strong. Second, FedPerGNN may be brittle to
% attackers with a large number of malicious clients. Thus, in our
% future work, we will study how to defend against intended attacks
% from malicious clients and platforms. Furthermore, we plan to
% explore the effective and secure deployment of FedPerGNN in
% real-world personalization systems to serve their users under
% privacy preservation."}

% \wajih{Are some of the above limitations applicable to use? If yes, add them to the threat model.}