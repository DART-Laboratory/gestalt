
\section{Threat Model \& Assumptions}

% \wajih{I thought we assume that the central server could act maliciously. If that is the case, then you need to rewrite the previous sentence because it is giving the wrong impression.}

%\wajih{if we can find more citations that will increase our credibility}

Our threat model assumes that the central server operates with integrity, conducting the federated averaging process without malicious objectives. However, we recognize the risk that the central server could compromise the privacy of client logs if raw data is transmitted to it. Similar to other existing works from the domain of Cryptography and Federated Learning~\cite{roy2020crypte,wu2022federated},  we assume that the utility server is trusted and there is no collusion between the central and utility servers. The integrity of client-side data collection is presumed, along with the absence of malicious clients during the training phase to prevent model poisoning, which is similar to other PIDS works~\cite{cheng2023kairos,flash2024,yangprographer,wang2022threatrace,provdetector2020}

For individual clients (endpoints), we expect that attackers could disguise their harmful activities within benign data, making it difficult to distinguish between legitimate and harmful actions. Our model also considers the threat posed by zero-day vulnerabilities. Despite these challenges, we assume that the activities of attackers will be detectable in the system's records (audit logs). In line with prior studies on data provenance~\cite{nodoze2019, priotracker2018, mzx2016, bates2017transparent,omegalog,rapsheet2020,provthings2018,dossier,inam2023sok,poirot2019}, our approach relies on the provenance collection system's ability to accurately record all system activities and changes. Additionally, we ensure the integrity of audit logs is maintained through the use of established tamper-resistant storage solutions, such as those described by Paccagnella et al.\cite{paccagnella2020custos} and the Hardlog system\cite{hardlog}.

\wajih{In the threat model, I would like to know about how you deal with membership inference, poisoning, gradient attacks or not deal with. In general, it should specify the limitations of privacy of our privacy-preserving techniques. Also, specifying what specific information (host interactions, process names, etc.) the attacker can learn from gradients. More specific you are the better it is.}

% \wajih{Reading Nature's paper they have the following limitations:
% "However, FedPerGNN has the following limitations. First,
% FedPerGNN relies on the assumption that third-party server is
% trusted and does not collude with the recommendation server,
% which is somewhat strong. Second, FedPerGNN may be brittle to
% attackers with a large number of malicious clients. Thus, in our
% future work, we will study how to defend against intended attacks
% from malicious clients and platforms. Furthermore, we plan to
% explore the effective and secure deployment of FedPerGNN in
% real-world personalization systems to serve their users under
% privacy preservation."}

% \wajih{Are some of the above limitations applicable to use? If yes, add them to the threat model.}