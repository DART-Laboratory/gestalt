\section{Background \& Motivation}
\label{sec:motivation}
% Contemporary host intrusion detection systems, exemplified by \unicorn~\cite{han2020unicorn}, \streamspot~\cite{streamspot}, \threatrace~\cite{wang2022threatrace} and \prographer~\cite{yangprographer}, heavily rely on system audit log data to identify malicious entities within a system. Employing advanced deep learning techniques such as \gnn (\gnnshort), these systems strive to enhance the accuracy of threat detection. However, the efficacy of these techniques is contingent upon vast amounts of data to train the underlying models, often reaching terabytes in size. Acquiring such extensive training data from a single user machine is impractical.

% Our investigation, involving the simulation of normal user workloads on test machines, reveals that the audit logs generated from these activities are relatively small in volume. Consequently, they prove insufficient for adequately training these large-scale machine learning models. To address this limitation, it becomes imperative to aggregate data from a diverse array of machines to a centralized storage system for comprehensive model training.

% Nevertheless, the consolidation of audit logs from various sources poses a significant challenge due to the inherent risk of privacy leakage. These logs contain vital information about the diverse activities carried out by different users, encompassing details about utilized applications, browsing history, and sensitive data like email content, phone numbers, as well as financial and medical information. These privacy concerns are underscored in a report by Datadog~\cite{datadog}, a prominent provider of system monitoring services. Consequently, utilizing existing systems for detecting system threats introduces a potential compromise of user privacy.

% Moreover, the centralized aggregation of all data elevates the risk of data leakage and compromises the efficiency of these systems in terms of both memory utilization and runtime efficiency. As a result, there is a pressing need for innovative solutions that balance the imperative of robust threat detection with the paramount importance of safeguarding user privacy and system efficiency.

% Federated learning (FL) is an establish technique for privacy preserving machine learning. In federated learning the individual client data does not leave the system. Instead each client machine trains a local machine learning model on its local data and then these clients sends the trained model to a central server where the federated averaging is utilized to combine the information from these models to get a unified global model. However our experimentation with existing systems reveal that applying federated learning to them yields poor results because these systems are not designed to work in this fashion.

\wajih{make a macro for Provenance-based IDS which is PIDS and use PIDS everywhere where you say intrusion detection. It saves space and also narrows the scope of our paper to PIDS. Also, make a table for the limitations section so that I can point to it in the introduction section. Use macros for words like word2vec, etc. because of letter capitalization. Also convert the words like Graph Neural Network into GNN to save space.}


In this section, we examine existing host intrusion detection systems and underscore their design flaws, particularly regarding the preservation of user log privacy. Flash~\cite{flash2024}, a node-level anomaly detection system, utilizes a \gnn to learn standard system behavior from provenance graphs. This system adopts an advanced featurization method, using a temporal ordering-aware word2vec model to capture both structural and semantic characteristics in audit logs. Moreover, Flash enhances inference speed via an \gnnshort embedding store. In contrast, \threatrace~\cite{wang2022threatrace}, another node-level detection system, also employs a \gnn for anomaly detection. However, it stands apart from Flash due to its scalability issues and reliance on basic features, which limits its effectiveness in fully leveraging the information contained in provenance graphs. Kairos~\cite{cheng2023kairos}, an anomaly-based detector, segments logs into time window queues and uses temporal graph neural networks for anomaly detection. Despite the progress made by existing intrusion detection systems, they fail to provide any privacy safeguards for sensitive information that may be contained in the audit logs they utilize. This deficiency can impede the deployment of these systems in domains where privacy is paramount. We expand upon these points in the following discussion: 

\PP{Data Privacy} The existing intrusion detection systems that have been discussed above predominantly rely on system audit log data to identify malicious entities within a system. These systems employ advanced deep learning techniques, such as \gnn, to achieve impressive detection results. However, a notable downside of these models is their substantial requirement for data to accurately learn benign system behavior. This volume of data is impracticable to generate from a single user's machine. Consequently, in an enterprise environment, it becomes essential to collect data from a multitude of machines in order to compile a sufficiently large dataset for training these detection systems. \wajih{We have some experiments for this as well to show why it is necessary to do get data from some machines for better models. Put numbers or figure related to those experiments here.}


However, this approach introduces a significant risk to user privacy. When a central entity analyzes these logs, it gains insights into the users' system activities. This can range from identifying the applications they use, to the websites they browse, and even potentially inferring their location from network IPs. These privacy concerns have been underscored in a report by Datadog~\cite{datadog}, a prominent provider of system monitoring services. Thus, while existing intrusion detection systems are effective in identifying system threats, they simultaneously pose a potential compromise to user privacy.

\PP{High Network and Disk Overhead} The current systems function under the premise of a centralized infrastructure designed for aggregating user logs to train models. In this setup, user machines periodically transmit their logs to a central server, which then consolidates them in a centralized database. However, the volume of these log data can reach gigabytes, resulting in significant network expenses for both users and the organization. Additionally, it demands substantial disk storage to accommodate all the centralized logs. Such extensive network costs disproportionately affect users with limited bandwidth, as they may be unable to contribute their data for model training. This exclusion hampers the model's ability to learn a comprehensive representation of benign behavior, potentially leading to a higher rate of false alarms for those not participating. Moreover, the necessity for extensive disk storage can restrict organizations from incorporating data from all users, which might introduce data bias and degrade the model's performance in practical scenarios. \wajih{For this point, we discuss that we will do some calculations that how much network overhead it will incur to send one day worth of OpTC dataset and Cadets to the central server for anomaly detection. Add those numbers here.}

\PP{Centralized Learning \& Scalability} The existing systems train their deep neural models using logs stored centrally. This approach significantly prolongs the training time of the models. The challenge escalates when frequent retraining is required to address the issue of concept drift\Fix{~\cite{}}. Additionally, these systems lack inherent mechanisms for parallelizing the training process, leading to an excessively lengthy sequential training period\Fix{~\cite{}}. Furthermore, managing all the data in a single location poses challenges in efficiently utilizing system resources, such as CPU and memory.

Moreover, existing systems, such as Flash, have implemented techniques to achieve efficient runtime performance. However, in an enterprise context, the centralized mode of operation faces scalability limitations. It can only accommodate a fixed number of hosts before the central server, running the intrusion detection system, becomes a bottleneck. In high-throughput environments, this leads to the detector suffering from log congestion.

\begin{table}[h!]
    \centering
    \scriptsize
      \caption{Limitations of Existing Systems.}
      \setlength{\tabcolsep}{4.8pt}
        \begin{tabular}{ | c | c | c | c | c |}
          \hline
               & \bf Privacy & \bf Network  & \bf Disk  & \bf Scalability \\
               & \bf  Preserving & \bf  Cost & \bf Cost &  \\
          \hline
          \Sys & YES                & LOW          & LOW       & HIGH        \\
          \hline
          FLASH      & NO                 & HIGH         & HIGH      & MEDIUM      \\
          \hline
          KAIROS     & NO                 & HIGH         & HIGH      & LOW         \\
          \hline
          THREATRACE & NO                 & HIGH         & HIGH      & LOW        \\
          \hline
        \end{tabular}
        \label{limitations}
    \end{table}

\subsection{How can federated learning help to solve above challenges?}


\subsection{Why is it challenging to apply FL to PIDS?}

Our analysis indicates that applying federated learning to existing systems would produce suboptimal results. This is primarily because these systems were not originally designed to function under a federated learning framework.

\PP{Randomized Esemble Models} \threatrace employs an ensemble of \gnnshort models to comprehensively learn the distribution of benign system activities. In this ensemble, each subsequent model is trained on entities that were misclassified by the preceding model. However, since these models are trained on random segments of the graph, applying the federated averaging algorithm~\cite{mcmahan2017communication} is problematic. Naive combination of these models in a federated context could result in a loss of critical information in the global model, due to their distinct training data subsets.

\PP{Feature Space Heterogeneity} Flash utilizes a temporally-aware Word2Vec model to encode the semantic attributes of entities within the provenance graph. In a federated learning environment, each client machine independently trains its Word2Vec model on its feature set. However, inherent randomness in the Word2Vec algorithm means that identical tokens may be encoded into different vectors by each client. Consequently, when federated averaging is applied to \gnn models that use these features, the performance is notably poor.

\PP{Temporal Misalignment} Kairos leverages temporal graph neural networks to understand the evolution of a system's provenance graph over time. However, the application of federated learning to capture these temporal dependencies faces significant challenges. The fragmentation of data across various clients results in a lack of temporal alignment, which is crucial for the effectiveness of federated averaging in these network types. This misalignment impedes the ability to effectively integrate the learnings from individual client models into a cohesive global model.

\subsection{Overview of \Sys}
The aforementioned challenges substantially hinder the integration of the federated learning paradigm into existing systems. We have developed \Sys to address these obstacles. By merging federated learning with graph representation learning, \Sys provides a privacy-preserving detection system. In this framework, each client trains a \gnnshort model on their local log data. Subsequently, these models engage in a federated averaging process at a central server. To address the issue of heterogeneous feature vectors across different clients, \Sys incorporates a novel vector harmonization module.
