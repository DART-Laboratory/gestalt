\section{Background \& Motivation}
\label{sec:motivation}
% Contemporary host intrusion detection systems, exemplified by \unicorn~\cite{han2020unicorn}, \streamspot~\cite{streamspot}, \threatrace~\cite{wang2022threatrace} and \prographer~\cite{yangprographer}, heavily rely on system audit log data to identify malicious entities within a system. Employing advanced deep learning techniques such as \gnn (\gnnshort), these systems strive to enhance the accuracy of threat detection. However, the efficacy of these techniques is contingent upon vast amounts of data to train the underlying models, often reaching terabytes in size. Acquiring such extensive training data from a single user machine is impractical.

% Our investigation, involving the simulation of normal user workloads on test machines, reveals that the audit logs generated from these activities are relatively small in volume. Consequently, they prove insufficient for adequately training these large-scale machine learning models. To address this limitation, it becomes imperative to aggregate data from a diverse array of machines to a centralized storage system for comprehensive model training.

% Nevertheless, the consolidation of audit logs from various sources poses a significant challenge due to the inherent risk of privacy leakage. These logs contain vital information about the diverse activities carried out by different users, encompassing details about utilized applications, browsing history, and sensitive data like email content, phone numbers, as well as financial and medical information. These privacy concerns are underscored in a report by Datadog~\cite{datadog}, a prominent provider of system monitoring services. Consequently, utilizing existing systems for detecting system threats introduces a potential compromise of user privacy.

% Moreover, the centralized aggregation of all data elevates the risk of data leakage and compromises the efficiency of these systems in terms of both memory utilization and runtime efficiency. As a result, there is a pressing need for innovative solutions that balance the imperative of robust threat detection with the paramount importance of safeguarding user privacy and system efficiency.

% Federated learning (FL) is an establish technique for privacy preserving machine learning. In federated learning the individual client data does not leave the system. Instead each client machine trains a local machine learning model on its local data and then these clients sends the trained model to a central server where the federated averaging is utilized to combine the information from these models to get a unified global model. However our experimentation with existing systems reveal that applying federated learning to them yields poor results because these systems are not designed to work in this fashion.

In this section, we examine existing host intrusion detection systems and underscore their design flaws, particularly regarding the preservation of user log privacy. Our system, \Sys, has been developed as a node-level intrusion detection system. As a result, we concentrate on comparing it with other systems that have similar detection granularity to ensure an equitable evaluation. Flash~\cite{flash2024}, a node-level anomaly detection system, utilizes a \gnn to learn standard system behavior from provenance graphs. This system adopts an advanced featurization method, using a temporal ordering-aware word2vec model to capture both structural and semantic characteristics in audit logs. Moreover, Flash enhances inference speed via an \gnnshort embedding store. In contrast, \threatrace~\cite{wang2022threatrace}, another node-level detection system, also employs a \gnn for anomaly detection. However, it stands apart from Flash due to its scalability issues and reliance on basic features, which limits its effectiveness in fully leveraging the information contained in provenance graphs. \shadewatcher, an edge-level detection system, takes inspiration from recommendation systems, modeling potential interactions within provenance graphs to pinpoint anomalous ones. Additionally, Kairos~\cite{cheng2023kairos}, an anomaly-based detector, segments logs into time window queues and uses temporal graph neural networks for anomaly detection. Despite the progress made by existing intrusion detection systems, they fail to provide any privacy safeguards for sensitive information that may be contained in the audit logs they utilize. This deficiency can impede the deployment of these systems in domains where privacy is paramount. We expand upon these points in the following discussion: 

\PP{Data Privacy} The existing intrusion detection systems that have been discussed above predominantly rely on system audit log data to identify malicious entities within a system. These systems employ advanced deep learning techniques, such as \gnn, to achieve impressive detection results. However, a notable downside of these models is their substantial requirement for data to accurately learn benign system behavior. This volume of data is impracticable to generate from a single user's machine. Consequently, in an enterprise environment, it becomes essential to collect data from a multitude of machines in order to compile a sufficiently large dataset for training these detection systems. However, this approach introduces a significant risk to user privacy. When a central entity analyzes these logs, it gains insights into the users' system activities. This can range from identifying the applications they use, to the websites they browse, and even potentially inferring their location from network IPs. These privacy concerns have been underscored in a report by Datadog~\cite{datadog}, a prominent provider of system monitoring services. Thus, while existing intrusion detection systems are effective in identifying system threats, they simultaneously pose a potential compromise to user privacy.

\PP{High Network and Disk Overhead} The current systems function under the premise of a centralized infrastructure designed for aggregating user logs to train models. In this setup, user machines periodically transmit their logs to a central server, which then consolidates them in a centralized database. However, the volume of these log data can reach gigabytes, resulting in significant network expenses for both users and the organization. Additionally, it demands substantial disk storage to accommodate all the centralized logs. Such extensive network costs disproportionately affect users with limited bandwidth, as they may be unable to contribute their data for model training. This exclusion hampers the model's ability to learn a comprehensive representation of benign behavior, potentially leading to a higher rate of false alarms for those not participating. Moreover, the necessity for extensive disk storage can restrict organizations from incorporating data from all users, which might introduce data bias and degrade the model's performance in practical scenarios.

\PP{Centralized Learning} The existing systems train their deep neural models using logs stored centrally. This approach significantly prolongs the training time of the models. The challenge escalates when frequent retraining is required to address the issue of concept drift. Additionally, these systems lack inherent mechanisms for parallelizing the training process, leading to an excessively lengthy sequential training period. Furthermore, managing all the data in a single location poses challenges in efficiently utilizing system resources, such as CPU and memory.

\PP{Scalability} Existing systems, such as Flash, have implemented techniques to achieve efficient runtime performance. However, in an enterprise context, the centralized mode of operation faces scalability limitations. It can only accommodate a fixed number of hosts before the central server, running the intrusion detection system, becomes a bottleneck. In high-throughput environments, this leads to the detector suffering from log congestion.
