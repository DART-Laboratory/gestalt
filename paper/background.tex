\section{Background}
\label{sec:motivation}

% \wajih{Add two three sentences here saying that in this section, first we give background on blah blah and then we describe why it is challenging blah blah. }

% In this section, we will provide background on data provenance graphs constructed from \logs. Additionally, we will delve into the details of federated learning and how it facilitates the privacy-preserving training of machine learning models. We will also discuss the challenges associated with applying federated learning to \pids.

% Contemporary host PIDS, exemplified by \unicorn~\cite{han2020unicorn}, \streamspot~\cite{streamspot}, \threatrace~\cite{wang2022threatrace} and \prographer~\cite{yangprographer}, heavily rely on system audit log data to identify malicious entities within a system. Employing advanced deep learning techniques such as \gnn (\gnnshort), these systems strive to enhance the accuracy of threat detection. However, the efficacy of these techniques is contingent upon vast amounts of data to train the underlying models, often reaching terabytes in size. Acquiring such extensive training data from a single user machine is impractical.

% Our investigation, involving the simulation of normal user workloads on test machines, reveals that the audit logs generated from these activities are relatively small in volume. Consequently, they prove insufficient for adequately training these large-scale machine learning models. To address this limitation, it becomes imperative to aggregate data from a diverse array of machines to a centralized storage system for comprehensive model training.

% Nevertheless, the consolidation of audit logs from various sources poses a significant challenge due to the inherent risk of privacy leakage. These logs contain vital information about the diverse activities carried out by different users, encompassing details about utilized applications, browsing history, and sensitive data like email content, phone numbers, as well as financial and medical information. These privacy concerns are underscored in a report by Datadog~\cite{datadog}, a prominent provider of system monitoring services. Consequently, utilizing existing systems for detecting system threats introduces a potential compromise of user privacy.

% Moreover, the centralized aggregation of all data elevates the risk of data leakage and compromises the efficiency of these systems in terms of both memory utilization and runtime efficiency. As a result, there is a pressing need for innovative solutions that balance the imperative of robust threat detection with the paramount importance of safeguarding user privacy and system efficiency.

% Federated learning (FL) is an establish technique for privacy preserving machine learning. In federated learning the individual client data does not leave the system. Instead each client machine trains a local machine learning model on its local data and then these clients sends the trained model to a central server where the federated averaging is utilized to combine the information from these models to get a unified global model. However our experimentation with existing systems reveal that applying federated learning to them yields poor results because these systems are not designed to work in this fashion.

%\wajih{make a macro for Provenance-based IDS which is PIDS and use PIDS everywhere where you say intrusion detection. It saves space and also narrows the scope of our paper to PIDS. Use macros for words like \wordvec, etc. because of letter capitalization. Also convert the words like Graph Neural Network into GNN to save space.}

\PP{Data Provenance Graphs}  \Logs are essential for recording system activities, capturing interactions among entities such as processes, files, and sockets. Operating systems like Windows, Linux, and FreeBSD provide built-in tools for log collection, including Event Tracing for Windows (ETW)\cite{windowsaudit}, the Linux Audit system\cite{linuxaudit}, and DTrace~\cite{dtrace}. Each log entry represents a system event, specifying subject and object entities along with contextual attributes like process names, file names, and socket IP addresses. Data provenance leverages these logs to construct a dependency graph that models causal links among system entities. In this graph, nodes represent entities (e.g., processes, files, sockets), and edges denote system syscalls, capturing event relationships. The provenance graph is critical for analyzing system activities and detecting anomalous or malicious entities. Figure~\ref{provexp} shows an example where \textit{Explorer.exe} executes processes like \textit{Notepad.exe} and \textit{Firefox.exe}, which then interact with file and socket nodes.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.4\textwidth]{fig/provexp.pdf}
  \caption{Data provenance graph example.}
  \label{provexp}
  \vspace{-3ex}
\end{figure}
%\wajih{Since we do not have any provenance graph in the paper, we need to provide a short example of a provenance graph.}


% \subsection{Provenance-based IDS (PIDS)}
% Provenance-based Intrusion Detection Systems can be broadly classified into two main types: rule-based and learning-based approaches. The rule-based approaches~\cite{holmes2019,rapsheet2020,poirot2019} leverage knowledge of known attack behaviors to identify attack activities in the provenance graph. However, developing these rules demands significant effort from analysts, and attacks with unseen patterns could go undetected. Conversely, learning-based approaches~\cite{wang2022threatrace,flash2024,cheng2023kairos,yangprographer,shadewatcher} focus on detecting zero-day attacks without requiring prior knowledge of past attacks or specific rules. These anomaly-based methods establish a baseline of benign system activities and identify any significant deviations from this baseline behavior as anomalies.

% \subsection{Limitations of Existing PIDS}
% Here we examine existing PIDS and underscore their design flaws, particularly regarding the preservation of user log privacy and scalability. \flash~\cite{flash2024}, a node-level anomaly detection system, utilizes Graph Neural Networks to learn standard system behavior from provenance graphs. This system adopts an advanced featurization method, using a temporal ordering-aware \wordvec model to capture both structural and semantic characteristics in \logs. Moreover, \flash enhances inference speed via a \gnnshort embedding store. In contrast, \threatrace~\cite{wang2022threatrace}, another node-level detection system, also employs Graph Neural Networks for anomaly detection. However, it stands apart from \flash due to its scalability issues and reliance on basic features, which limits its effectiveness in fully leveraging the information contained in provenance graphs. \kairos~\cite{cheng2023kairos}, an anomaly-based detector, segments logs into time window queues and uses temporal graph neural networks for anomaly detection. Despite the progress made by existing PIDS, they fail to provide any privacy safeguards for sensitive information that may be contained in the \logs they utilize. This deficiency can impede the deployment of these systems in real world settings.

% We provide three limitations of existing systems in detail in Section~\ref{s:intro}, namely: lack of privacy preservation, excessive network overhead, and limited scalability. Table~\ref{tab:limitations} provides an overview of the limitations of existing systems.


% \PP{Lack of Privacy Preservation} The existing PIDSes that have been discussed above predominantly rely on system log data to identify malicious entities within a system. These systems employ advanced deep learning techniques, such as \gnn, to achieve impressive detection results. However, a notable downside of these models is their substantial requirement for data to accurately learn benign system behavior. This volume of data is impracticable to generate from a single user's machine. Consequently, in an enterprise environment, it becomes essential to collect data from a multitude of machines in order to compile a sufficiently large dataset for training these detection systems.  We conducted experiments with the \flash system by training it on data from a single host from \optc and compared the results with those reported in their paper, which were generated using data from multiple hosts. We observed a significant decrease in detection performance when using data from a single host, with the F-score experiencing a 40\% reduction.

% The centralized approach introduces a significant risk to user privacy. When a central entity analyzes these logs, it gains insights into the users system activities. This can range from identifying the applications they use, to the websites they browse, and even potentially inferring their location from network IPs. Thus, while existing PIDS are effective in identifying system threats, they simultaneously pose a potential compromise to user privacy.

% \PP{Excessive Network Overhead} The current systems function under the premise of a centralized infrastructure designed for aggregating user logs to train models. In this setup, user machines periodically transmit their logs to a central server, which then consolidates them in a centralized database. However, the volume of these log data can reach gigabytes, resulting in significant network expenses for both users and the organization. Such extensive network costs disproportionately affect users with limited bandwidth, as they may be unable to contribute their data for model training. This exclusion hampers the model's ability to learn a comprehensive representation of benign behavior, potentially leading to a higher rate of false alarms for those not participating. Our analysis of the \optc dataset reveals that each host generates approximately 1 GB of log data daily. In a real-world enterprise with hundreds of users, this translates to several hundred gigabytes of data being transmitted to the server each day for threat detection. Such volumes pose significant infrastructure and scalability challenges for centralized detection systems.

% \PP{Limited Scalability} The existing systems train their deep neural models using logs stored centrally. This approach significantly prolongs the training time of the models. The challenge escalates when frequent retraining is required to address the issue of concept drift~\cite{lu2018learning}. Additionally, these systems lack inherent mechanisms for parallelizing the training process. Moreover, since all logs need to be aggregated in a centralized storage, the disk overhead costs for the organization increase. The necessity for extensive disk storage can restrict organizations from incorporating data from all users, which might introduce data bias and degrade the model's performance in practical scenarios for these clients.

% Although existing systems, such as \flash, have implemented techniques to achieve efficient runtime performance. However, in an enterprise context, the centralized mode of operation faces scalability limitations. It can only accommodate a fixed number of hosts before the central server, running the intrusion detection system, becomes a bottleneck leading to log congestion. In high-throughput environments, this leads to the detector suffering from log congestion.

% \begin{table}[t!]
%     \centering
%     \scriptsize
%       %\caption{Limitations of existing PIDS. \wajih{Add in caption that which PIDS are not specified in the table and why.}}
%       \caption{Existing PIDS limitations: \flash and \kairos outperform other existing PIDS systems ~\cite{wang2022threatrace,han2020unicorn,streamspot,yangprographer,shadewatcher,provdetector2020}. Therefore, we have excluded these PIDS from the table.}
%       \setlength{\tabcolsep}{4.8pt}
%         \begin{tabular}{ | c | c | c | c | c |}
%           \hline
%                & \bf Privacy & \bf Network  & \bf Disk  & \bf Scalability \\
%                & \bf  Preserving & \bf  Cost & \bf Cost &  \\
%           \hline
%           \Sys  & YES                & LOW          & LOW       & HIGH        \\
%           \hline
%           \disdet~\cite{dong2023distdet} & NO                & LOW         & LOW      & HIGH       \\
%           \hline
%           \flash~\cite{flash2024}     & NO            & HIGH         & HIGH      & MEDIUM      \\
%           \hline
%           \kairos~\cite{cheng2023kairos}     & NO            & HIGH         & HIGH      & LOW         \\
%           \hline
%         \end{tabular}
%         \label{limitations}
%     \end{table}

\PP{Federated Learning}. Federated learning (FL) trains a global model across decentralized clients without sharing raw data. The global model, parameterized by \(\theta\), minimizes a loss function \(L(\theta)\) across all clients. Each client \(i\) computes a local update \(\Delta \theta_i = -\eta \nabla L_i(\theta)\), where \(L_i(\theta)\) is the local loss and \(\eta\) is the learning rate. The central server aggregates updates as \(\Delta \theta_{global} = \frac{1}{N} \sum_{i=1}^{N} \Delta \theta_i\) and updates the global model: \(\theta_{new} = \theta + \Delta \theta_{global}\). This iterative process preserves data privacy and reduces bandwidth usage while training the model. \wajih{make sure that the notations match what you have in the design section.}


% \subsection{Why is it challenging to apply FL to \pids?}

% Applying Federated Learning (FL) to PIDS~\cite{streamspot,provdetector2020,wang2022threatrace,shadewatcher,yangprographer,han2020unicorn,jia2023magic,flash2024,cheng2023kairos,sigl} presents significant challenges due to the inherent characteristics of federated learning and the nature of data in PIDS environments. The key challenges are outlined below:

% \PP{Data Imbalance \& Heterogeneity Among Clients} In federated learning scenarios, the heterogeneity of data across devices often leads to skewed model updates and performance variances, a phenomenon referred to as non-IID~\cite{zhao2018federated} data. This variability in data distribution and volume can result in some clients having extensive data representing a broad spectrum of benign patterns, while others have limited or skewed datasets. Such imbalances complicate the training of a global model \( GNN_{Global} \) that performs uniformly well across all clients. Specifically, if \( GNN_{Global} \) is optimized based on $\mathcal{D} = \bigcup_{i=1}^{N} \mathcal{D}_i$, where $\mathcal{D}_i$ denotes the local dataset of client $i$, the model may become biased toward clients with more comprehensive data collections, potentially overlooking unique patterns in less-represented clients. Existing PIDS lack mechanisms to effectively address this challenge.

% This imbalance complicates the training of a global model \( GNN_{Global} \) that performs uniformly well across all clients, which is critical for PIDS. If the global model is biased towards certain clients, it may produce high false alarms for behaviors present in less-represented clients, reducing the overall detection accuracy and increasing the alert fatigue.

% \PP{Feature Space Heterogeneity} PIDS, such as \flash~\cite{cheng2023kairos}, employ a \wordvec model to encode the semantic attributes of entities within a provenance graph. In a federated learning environment, each client machine independently trains its \wordvec model \( word2vec_{i} \) on its local feature set $\mathcal{F}_i$. The inherent randomness of the \wordvec algorithm means that identical tokens $t$ can be encoded into different vectors $v_i(t)$ by each client $i$. As a result, when each client trains its \wordvec model, identical features are projected into disparate embedding spaces. This discrepancy hinders the convergence of \gnn models \( GNN_{i} \) trained on these features, leading to poor performance when federated averaging is applied.

% This discrepancy hinders the performance of the global model \( GNN_{\text{Global}} \), which is essential for accurately detecting anomalies in provenance graphs. Feature space heterogeneity can make it difficult for the global model to converge, leading to false positives or missed detections in anomaly detection systems.

% \PP{Temporal Misalignment} Some PIDS, such as Kairos~\cite{cheng2023kairos}, leverage temporal graph neural networks (TGN) to analyze the evolution of a system's provenance graph over time. However, the application of federated learning to capture these temporal dependencies presents significant challenges. Data fragmentation across clients results in a lack of temporal alignment, which is crucial for the effective implementation of federated averaging in such networks. Specifically, when models trained on temporally diverse datasets from individual clients are aggregated, they may not align properly, leading to a loss of temporal information. This misalignment impedes the ability to integrate individual learnings into a cohesive global model \( GNN_{Global} \).

% Temporal misalignment can significantly impair the ability of the global model to accurately detect anomalies in PIDS, as the sequence and timing of events is cruical for TGNs to work. PIDS depending on the temporal correlation of events can lead to ineffective threat detection.
