\begin{appendices}

\section{Provenance Graph Constructor}
\label{sub:provconstruct}

\Sys utilizes \logs to construct a system provenance graph. It operates on each client machine, using their local system logs to build the graph. Major operating systems, including Linux and Windows, come equipped with built-in mechanisms for log collection -- specifically, the Linux Audit system~\cite{linuxaudit} and Windows Event Tracing~\cite{windowsaudit}. These logs provide detailed insights into the interactions among various system entities, capturing activities, such as process executions, file operations, and network connections. Using this data, \Sys forms a graph where nodes represent system entities including processes, files, and sockets. The edges of this graph denote events, identified by syscalls, that occur between these entities. Moreover, \Sys enhances each node with comprehensive attributes, including process names, command lines, file names, and network IP addresses. As demonstrated by prior works~\cite{flash2024,cheng2023kairos}, these contextual attributes enable the model to develop a robust understanding of system behavior.

\section{Dataset Description}
\label{sec:dataset:description}

The E3 dataset contains two kinds of threat actors: a nation-state actor and a common threat actor. The goal of the nation-state attacker is to steal proprietary information from the targeted company. Initially, the attacker intends to exploit the webserver hosted on FreeBSD, inject into the SSHD process, and then wait. At this point, the attacker monitors connections and network activity while residing on the FreeBSD host. Subsequently, the attacker targets and exploits the discovered hosts to exfiltrate proprietary data. The common threat attacker aims to steal Personal Identifiable Information (PII) for financial gain by deceiving targeted users into providing access to the network using spear phishing.

The \darpa E5 dataset is more advanced than E3 in terms of attack complexity and data volume. The attackers' capabilities span the entire MITRE ATT\&CK~\cite{xiong2022cyber} adversarial lifecycle, from backdoors to exploit shellcode, from download to the execution of APT simulacra in-memory of the exploited process’s memory, from fingerprinting and surveying the target to exfiltrating sensitive data. Multiple variations of APTs and attack capabilities were delivered for Windows, Ubuntu, FreeBSD, and Android. The logs from E3 and E5 are documented under various scenario names, including Cadets, Trace, Theia, and ClearScope. The attacks in E3 were conducted on a single host per scenario, whereas E5 features three hosts per scenario, presenting a multi-host threat environment.

The \optc dataset, another open-source resource from \darpa, encompasses a comprehensive collection of audit logs from an enterprise environment with 1,000 hosts. This dataset includes six days of benign system logs. Subsequently, attack logs span three days of system activities, featuring red team tactics such as initial compromises, privilege escalations, malicious software installations, and data exfiltration. Each attack day targeted different host machines.

\section{Resource consumption and Processing Times}
\label{sec:resource_consumption}

\PP{Resource Consumption} We conducted experiments to analyze the resource consumption of the central, utility server, and client-side modules of \Sys. We modeled the resource utilization on a client machine using different batches of audit events of varying sizes. For the central and utility servers, we studied resource consumption by varying the number of clients to understand the demands of federated averaging and semantic vector harmonization. The results, depicted in Figure~\ref{fig:resource}, indicate that \Sys's resource consumption is moderate. Specifically, \Sys can process up to 100,000 audit events simultaneously while consuming less than 900 MB of memory and utilizing less than 20\% of CPU resources. This performance suggests that \Sys does not significantly burden the client machine, especially considering the typically low event throughput on such machines. Additionally, our analysis of the host data in the \optc dataset shows that, on average, each client generates approximately 100,000 audit log events within a three-hour period. For the central and utility servers, the resource usage is minimal, demonstrating that our architecture is scalable and suitable for large organizations with many clients.

 \begin{figure}[!t]
  \centering
  \subfloat[CPU utilization client side.]{\includegraphics[width=0.20\textwidth]{fig/cpu.pdf}\label{cpu_client}}
  \hfill
  \subfloat[RAM utilization client side]{\includegraphics[width=0.20\textwidth]{fig/ram.pdf}\label{ram_client}}
  \hfill
  \subfloat[CPU utilization central server.]{\includegraphics[width=0.20\textwidth]{fig/cpu_central.pdf}\label{cpu_central}}
  \hfill
  \subfloat[RAM utilization central server]{\includegraphics[width=0.20\textwidth]{fig/ram_central.pdf}\label{ram_central}}
  \hfill
  \subfloat[CPU utilization utility server.]{\includegraphics[width=0.20\textwidth]{fig/cpu_utility.pdf}\label{cpu_utility}}
  \hfill
  \subfloat[RAM utilization utility server]{\includegraphics[width=0.20\textwidth]{fig/ram_utility.pdf}\label{ram_utility}}
  \caption{Resource consumption of various components of \Sys.}
  \label{fig:resource}
  \vspace{-2ex}
\end{figure}

\PP{Processing Time Analysis}We conducted experiments to study the end-to-end processing time of our system for a client machine. For this, we used batches of audit events of various sizes, conducting end-to-end inference with \Sys to measure the time taken to process these events on a client machine. The results, illustrated in Figure~\ref{sizevstime}, demonstrate that \Sys processes events with notable efficiency. For example, it requires approximately 23 seconds to process a batch of 100,000 events. Given our previous analysis of host logs in the \optc dataset, which indicated that each host generates an average of 100,000 events in three hours, \Sys can process 24 hours worth of log data on a client in merely 3 minutes. This level of efficiency ensures that our system is highly effective, preventing any potential log congestion.

\begin{figure}[!t]
 \centering
 \includegraphics[width=0.25\textwidth]{fig/sizevstime.pdf}
 \caption{Processing time for various audit event sizes evaluated using \optc dataset.}
 \label{sizevstime}
 \vspace{-2ex}
\end{figure}


\section{Ablation study}
\label{app:ablation}

In this ablation study, we analyze the impact of key parameters within \Sys. Specifically, we examine the effects of number of federated averaging rounds, the number of \gnnshort categorized models, the anomaly threshold and differential privacy on accuracy. The effects of these parameters are discussed below:

% \PP{Hosts vs Detection Performance} We utilized the \optc dataset for this experiment, randomly selecting a variable number of hosts to participate in training our models through federated learning. The trained global models were then applied to perform threat detection. Figure~\ref{scoresvshosts} presents these results, illustrating that performance improvement is observed up to a certain number of hosts. This plateau is attributed to the \optc dataset's finite set of benign patterns that can be learned. Beyond this threshold, additional hosts do not contribute new information beneficial to the model, thus halting further performance gains and increasing the risk of overfitting. To mitigate this, client machines can monitor the loss of the global model on their local datasets to determine the optimal point to opt out from the learning process.

% \begin{figure}[!t]
%   \centering
%   \includegraphics[width=0.3\textwidth]{fig/scoresvshosts.pdf}
%   \caption{Effect of number of hosts vs detection metrics using \optc dataset.}
%   \label{scoresvshosts}
%   \vspace{-2ex}
% \end{figure}

\PP{Effect of Federated Averaging Rounds} We employed the \darpa E3 dataset to examine the impact of federated averaging rounds on detection performance. Our methodology involved training the model over a range of federated averaging rounds and subsequently evaluating the model's detection capabilities. The outcomes are depicted in Figure~\ref{roundsvsscore}, which shows that detection performance improves up to a certain number of rounds before declining due to overfitting. Notably, this inflection point is also characterized by a minimal decrease in training loss, suggesting that the model has reached its learning capacity. This observation proves to be a valuable metric for determining the optimal moment to stop training, thereby preventing overfitting and ensuring optimal model performance.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.3\textwidth]{fig/roundsvsscore.pdf}
  \caption{Federated averaging rounds vs detection performance using E3 dataset. }
  \label{roundsvsscore}
  \vspace{-2ex}
\end{figure}

\PP{Effect number of categories vs Detection} We studied the impact of varying the number of categories ($k$) on the detection performance. Within our system entity-level personalized \gnnshort learning framework, $k$ controls the creation of distinct standardized bins. These bins categorize processes across client machines. Subsequently, we employ an ensemble approach, deploying a separate \gnnshort model for each bin. Thus, the total number of \gnnshort models corresponds to the number of categories. The results depicted in Figure~\ref{catgvsscore} indicate that an increase in $k$ enhances precision while maintaining recall levels. This outcome arises because a single model suffices for achieving high recall by effectively distinguishing between benign and malicious patterns. Nevertheless, a single model falls short in fully generalizing across the diverse benign patterns unique to each client due to the potential blending of individual patterns during the process of federated averaging. Employing a specialized \gnnshort strategy across various categories addresses this challenge by minimizing false alarms.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.3\textwidth]{fig/kvsscore.pdf}
  \caption{Effect of number of categories vs detection performance using E3 dataset.}
  \label{catgvsscore}
  \vspace{-2ex}
\end{figure}

%\subsection{Detection Performance Under Different Methods}

%\wajih{In the Nature's paper they apply different federated learning and GNN approaches to show that the one they propose is best. I remember you tried different FL approaches as well. Also tried if you can replace GNN with something else. In our Flash paper we have something similar where we switched ML methods to show the performance. I would like to see similar experimentation here as well. The idea is to prove that the method that we chose is the best.}

% \PP{Effect of number of clients on scalability} \Sys operates in a decentralized manner, ensuring that its scalability post-deployment is independent of the client machines count. After the \fpgl phase concludes, each client runs \Sys locally. However, the number of clients may affect scalability during the training phase, potentially impacting the central and utility servers. The primary function of the central server is to conduct federated averaging on model weights from clients—a straightforward matrix mean operation that requires minimal resources, even with thousands of matrices. Similarly, the utility server's role is to sum and average vectors for overlapping semantic attributes, a task that modern machines are optimized to perform for a large number of samples. Importantly, these operations occur solely during the training phase and do not impact the scalability of the deployed system.

\PP{Anomaly Threshold} This hyperparameter controls the number of alerts generated by \Sys. It determines the trade-off between precision and recall. We use the E5 dataset to investigate the effects of different anomaly thresholds. Figure~\ref{thresh} illustrates the results.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.3\textwidth]{fig/thresh.pdf}
  \caption{Anomaly threshold effect.}
  \label{thresh}
  \vspace{-2ex}
\end{figure}


\PP{Effect of differential privacy on accuracy} Differential privacy is a method that can be combined with federated learning to offer protection against inference attacks~\cite{lyu2020threats,nasr2019comprehensive,zari2021efficient} at the cost of detection accuracy. We have analyzed the robustness of our system against these attacks in Section~\ref{sec:privacy}. Here we will examine the impact of differential privacy noise levels on the detection performance of \Sys. Differential privacy achieves model privacy by adding a controlled amount of Gaussian noise to the model parameters, which helps in concealing the influence of any individual data point. In our implementation, Gaussian noise is used due to its beneficial properties in terms of meeting the differential privacy criteria under the Gaussian mechanism, which involves adjusting the noise scale according to a privacy budget defined by $\epsilon$. This noise is added to the local \gnnshort model updates before they are aggregated at the central server for federated averaging, as detailed in Section~\ref{sec:methodology}.

The parameter $\epsilon$ is crucial; it is inversely related to the amount of noise added -- lower values of $\epsilon$ result in higher noise levels, thereby increasing privacy but potentially degrading the utility of the model. Conversely, a higher $\epsilon$ indicates less noise, which may improve the model's detection capabilities but reduce privacy protection. By adjusting $\epsilon$ during the training phase, we assess the trade-off between privacy and detection performance in the globally trained models across various $\epsilon$ settings. Figure~\ref{epsvsscore} shows that increasing noise strength degrades model utility offering more privacy at the expense of reduced accuracy.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.3\textwidth]{fig/epsvsscore.pdf}
  \caption{Effect of differential privacy noise on detection using E3 dataset. Note that we observed similar results on the other datasets.}
  \label{epsvsscore}
  \vspace{-2ex}
\end{figure}

% \section{Word2vec Semantic Featurization}
% \label{app:Word2vec:Semantic:Featurization}



\end{appendices}

% \label{sub:hyper}

% In  federated learning, where data originates from diverse sources such as user machines running various processes, quantifying this diversity becomes crucial for model development. The Shannon Diversity Index (SDI) is a metric adapted from ecology to measure the heterogeneity of datasets. This report elaborates on the application of SDI in federated learning, focusing on comparing the diversity of sub-models in an ensemble approach to a single model scheme.

% \section*{The Shannon Diversity Index (SDI)}

% SDI quantifies a dataset's diversity by considering both the variety of items (richness) and the distribution frequency of each item (evenness). It is given by:

% \[
% H' = -\sum_{i=1}^{R} p_i \ln(p_i)
% \]

% where:
% \begin{itemize}
%     \item $H'$ is the Shannon Diversity Index,
%     \item $R$ denotes the number of unique items,
%     \item $p_i$ represents the proportion of the dataset made up by the $i$th item.
% \end{itemize}

% In federated learning, SDI can reveal the heterogeneity within the data, guiding the strategy for model training and data segmentation.

% \section*{Application of SDI in Federated Learning}

% \subsection*{Scenario Overview}

% A federated learning setup involves analyzing process data from user machines. This scenario examines the diversity of processes, considering the significant overlap among users.

% \subsection*{Example Dataset}

% The dataset features processes like Chrome, Excel, Spotify, and others, from four user machines. Each machine's processes exhibit a 50\% overlap with others, presenting a realistic view of data distribution.

% \subsection*{Process Distribution Across Users}

% \begin{itemize}
%     \item \textbf{User 1}: \{Chrome, Excel, Spotify, Zoom, Slack\}
%     \item \textbf{User 2}: \{Word, PowerPoint, Teams, Slack, Spotify\}
%     \item \textbf{User 3}: \{Chrome, Zoom, Teams, Excel, Word\}
%     \item \textbf{User 4}: \{PowerPoint, Spotify, Slack, Chrome, Word\}
%     \item \textbf{User 5}: \{Chrome, Zoom, Teams\}

% \end{itemize}

% Processes are then randomly divided into three groups.

% \subsection*{Computing SDI for Each Group and single  Scheme}

% SDI calculations are based on the occurrence frequencies of processes, assessing diversity within each group and comparing it to a single  model approach where all processes are considered together.

% \subsubsection*{SDI Results}

% \begin{itemize}
%     \item \textbf{Category 1}: {Chrome, Zoom, and Teams}.
%     \item \textbf{Category 2}: {Word, Excel, and PowerPoint}.
%     \item \textbf{Category 3}: {Spotify and Slack}.
% \end{itemize}

% \subsection*{Analysis and Implications}

% \begin{itemize}
%     \item \textbf{Category 1 SDI}: $H' = 1.08$
%     \item \textbf{Category 2 SDI}: $H' = 1.10$
%     \item \textbf{Category 3 SDI}: $H' = 0.67$
%     \item The single  model, considering all processes together, yielded an SDI of $H' = 2.06$.
% \end{itemize}

% \subsubsection*{Diversity Comparison}

% Compared to the single  model's SDI, each sub-model reflects a specific aspect of the overall diversity but with reduced complexity, suggesting that sub-models focus on more homogeneous subsets of data.

% \subsubsection*{Influence on Client Fairness}

% By dividing the overall task into sub-tasks (sub-models) and assigning them to different subsets of processes (or data features), the influence of clients with large datasets is diversified across multiple models. This can prevent any single client from disproportionately skewing the outcome of the entire federated learning system.

% \subsubsection*{Balanced Representation}

% Each sub-model specializes in a different aspect of the data, potentially leading to a scenario where clients contribute more evenly across the ensemble. Clients with large datasets still contribute significantly, but their impact is more evenly distributed, enhancing fairness.

