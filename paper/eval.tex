 \section{Evaluation}
 \label{sec:eval}

 In our comprehensive assessment of \Sys, we explore several key research questions. Our testing environment comprised a machine with 8 Intel vCPUs, 80 GB RAM, an NVIDIA RTX2080 GPU, and ran on Ubuntu 18.04.6 LTS.
\wajih{you need add results related to resource overheads, hyperparameters tuning, ablation study, etc. Almost all the experiments that you have in your Flash paper are also applicable here.}
\begin{itemize}[leftmargin=*]
\item \textbf{RQ1.} What is the comparative detection accuracy of \Sys against current systems?
\item \textbf{RQ2.} What is the effectiveness of word2vec harmonization in a utility server setting?
\end{itemize}

\PP{Implementation} We developed \Sys using Python and incorporated the Torch Geometric library for our \gnnshort model. This model employs Sage Convolutions, a graph convolution layer that aggregates features from adjacent nodes to generate new node features. To enhance generalization, the model integrates dropout and ReLU activation functions across layers. For our \wordvec model, we utilized the Gensim library.

\PP{Datasets} \Sys underwent testing with two open-source datasets: \darpa E3~\cite{darpae3} and \darpa \optc~\cite{darpatc}. The E3 dataset, originating from \darpa's third red-vs-blue team exercise, includes audit logs depicting both normal and malicious activities. The \darpa \optc dataset offers a comprehensive view of benign and malevolent audit records from around 1000 hosts. It features six days of normal activities for training, followed by three days showcasing different types of attacks for evaluation purposes. The first day simulates a PowerShell Empire staging scenario, including the initial breach, lateral movements, and privilege escalations. The second day focuses on data exfiltration events, while the third day involves the deployment of malicious software.

\PP{Detectors for Comparison} For benchmarking \Sys, we employed the \threatrace \cite{wang2022threatrace} system, adopting the same detection metrics for comparison. \threatrace, a provenance-based IDS, leverages a \gnn for node-level anomaly detection. It learns the structural patterns of nodes in benign datasets and detects anomalies based on deviations from these patterns. Unlike \Sys, however, \threatrace does not ensure the privacy of user logs and faces scalability issues.

{\renewcommand{\arraystretch}{1.2}% for the vertical padding
  \begin{table*}[t!]
    \centering
    \scriptsize
    \caption{Performance of \Sys against \threatrace. \wajih{we decided to compare this against Flash as well. }}
    \setlength{\tabcolsep}{8pt}
    \begin{tabular}{ccccccccccccc}
      \toprule

    \multirow{2}{*}{\textbf{Datasets}}
    & \multicolumn{4}{c }{\Norothead{ \bf \threatrace}}
    & \multicolumn{4}{c }{\Norothead{ \bf \Sys}}
    \\ \cmidrule(r{\tbspace}){2-5} \cmidrule(r{\tbspace}){6-9} \cmidrule(r{\tbspace}){10-13}

      & {\bf Precision} &  {\bf Recall} & {\bf \fscore} & {\bf TP} / {\bf FP}  & {\bf Precision.}  & {\bf Recall} & {\bf \fscore} & {\bf TP} / {\bf FP}   \\

    \midrule

    Cadets (E3) &  \TCP  & \TCR & \TCF & \TCTP/ \TCFP & \FCP & \FCR & \FCF & \FCTP/ \FCFP  \\
    Trace (E3) &  \TTP  & \TTR & \TTF & \TTTP/ \TTFP & \FTP  & \FTR & \FTF & \FTTP / \FTFP   \\
    Theia (E3) &  \TTHP  & \TTHR & \TTHF & \TTHTP/ \TTHFP & \FTHP  & \FTHR & \FTHF & \FTHTP / \FTHFP  \\
    
    \optc & \TOAP  & \TOAR & \TOAF & \TOATP/ \TOAFP & \FOAP  & \FOAR & \FOAF & \FOATP/ \FOAFP \\
    \bottomrule
    \end{tabular}
  \label{summary:benchmarks:large}
  \end{table*}}

 \subsection*{RQ1. Detection Performance}
 Table~\ref{summary:benchmarks:large} presents a comparative analysis of the detection performance between \Sys and \threatrace. It reveals that \Sys achieves a detection performance comparable to that of \threatrace. The advantage of \Sys lies in its ability to preserve user log privacy via federated learning. Additionally, its scalability is enhanced as each client independently conducts anomaly detection on their logs. This decentralization of learning and detection tasks, enabled by federated learning, significantly enhances the scalability of our system.

 \subsection*{RQ2. Efficacy of Word2vec harmonization}
 To evaluate the efficacy of our method, we performed experiments comparing the utility server-based harmonized model with a local word2vec model on each client. These experiments utilized the \optc dataset. The findings, detailed in Table~\ref{local:wordvec}, demonstrate that our strategy significantly enhances the performance of \Sys. This improvement is attributed to our approach's ability to minimize noise in disparate word2vec vectors, thereby facilitating a more accurate interpretation of various activity patterns in the provenance graph by the \gnnshort model.

\begin{table}[h!]
    \centering
    \scriptsize
      \caption{Effectiveness of word2vec vectors harmonization.}
        \begin{tabular}{ | c | c | c | c | c | c |}
          \hline
            \bf Word2vec Type & \bf Precision & \bf Recall & \bf \fscore & \bf TP & \bf FP \\
          \hline
           Vanilla & 0.66  & 0.97 & 0.79 & 636 & 325 \\
           Harmonized & \FOAP  & \FOAR & \FOAF & \FOATP & \FOAFP \\
          \hline
        \end{tabular}
        \label{local:wordvec}
    \end{table}


%   \begin{table}[h!]
%     \centering
%     \scriptsize
%       \caption{Effect of logs quantity on existing PIDS.}
%         \begin{tabular}{ |c | c | c | c | c |}
%           \hline
%             \bf IDS & Hosts & \bf Precision & \bf Recall & \bf \fscore \\
%             \hline
%             \threatrace & 1 & 0.52  & 0.83 & 0.64  \\
%             \hline
%             \threatrace & 3 & 0.85  & 0.86 & 0.85  \\
%             \hline
%             Flash & 1 & 0.63  & 0.85 & 0.72  \\
%             \hline
%             Flash & 3 & 0.92  & 0.93 & 0.92  \\
%             \hline
%         \end{tabular}
%     \end{table}


% \begin{table}[h!]
%     \centering
%     \scriptsize
%       \caption{Federated Learning Using Central Language Model.}
%         \begin{tabular}{ | c | c | c | c | c | c |}
%           \hline
%             \bf Precision & \bf Recall & \bf \fscore & \bf TP & \bf FP  & \bf TN\\
%           \hline
%            0.89  & 0.96 & 0.92 & 624 & 71 & 26 \\
%           \hline
%         \end{tabular}
%     \end{table}

% \begin{table}[h!]
%     \centering
%     \scriptsize
%       \caption{Federated Learning Using Space Aligned Language Model.}
%         \begin{tabular}{ | c | c | c | c | c | c |}
%           \hline
%             \bf Precision & \bf Recall & \bf \fscore & \bf TP & \bf FP  & \bf TN\\
%           \hline
%            0.64  & 0.96 & 0.77 & 625 & 360 & 25 \\
%           \hline
%         \end{tabular}
%     \end{table}


%   \PP{False alarms analysis for E3 dataset}
%   The performance of \Sys on the Darpa E3 dataset is notably poor. A more comprehensive analysis of the distribution of false alarms provides the following insights:\\

%   \textbf{Cadets:} The performance on the Cadets dataset stands out positively compared to other datasets. This can be attributed to the dataset's relative simplicity, characterized by a smaller variety of node types. Notably, the majority of false alarms on Cadets were generated for nodes FILE\_OBJECT\_DIR, FILE\_OBJECT\_CHAR, and FILE\_OBJECT\_LINK, collectively accounting for 92\% of false alarms.\\

%   \textbf{Trace:} On the Trace dataset, \Sys exhibited a rather average performance. Among the node types, MemoryObject and FILE\_OBJECT\_NAMED\_PIPE contributed to 80\% of the false alarms.\\

%   \textbf{Theia:} When applied to the Theia dataset, the most problematic nodes were FILE\_OBJECT\_DIR and FILE\_OBJECT\_CHAR, making up 77\% of the false alarms.\\

%   \textbf{Fivedirections:} In the case of the Fivedirections dataset, the nodes FILE\_OBJECT\_CHAR, PRINCIPAL\_REMOTE, and SRCSINK\_DATABASE accounted for the majority of false alarms, constituting approximately 90\% of them.\\

%   The analysis above reveals a highly skewed distribution of false alarms, primarily attributed to the intricate nature of node-level classification, which becomes even more challenging when combined with Federated Learning. This complexity results in the model's inability to comprehensively understand every node type, leading to gaps in its comprehension.

%   To address this issue, we need a design change in our anomaly detector, shifting from node-level to graph-level classification. By prioritizing the learning of high-level behaviors within provenance graphs rather than focusing on individual nodes, we can improve the model's overall performance.

%   Furthermore, the \threatrace approach to anomaly detection lacks intuitiveness, causing ongoing confusion. To enhance clarity and alignment with established practices, we should advocate for transitioning to a more conventional and widely-accepted unsupervised learning approach following~\cite{yangprographer,shadewatcher,sigl}.
