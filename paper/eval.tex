 \section{Evaluation}
 \label{sec:eval}

 {\renewcommand{\arraystretch}{1.2}
 \begin{table*}[t!]
   \centering
   \scriptsize
   \caption{Comparison of \Sys against FLASH and KAIROS. Prec.: Precision; Rec.: Recall;}
   \setlength{\tabcolsep}{0.7pt}
   \begin{tabular}{ccccccccccccc}
     \toprule
 
   \multirow{2}{*}{\textbf{Datasets}}
   & \multicolumn{4}{c }{\Norothead{ \bf \Sys}}
   & \multicolumn{4}{c }{\Norothead{ \bf FlASH}}
   & \multicolumn{4}{c }{\Norothead{ \bf KAIROS}}
   \\ \cmidrule(r{\tbspace}){2-5} \cmidrule(r{\tbspace}){6-9} \cmidrule(r{\tbspace}){10-13}
 
     & {\bf Prec.} &  {\bf Rec.} & {\bf \fscore} & {\bf TP}/ {\bf FP}/ {\bf FN}/ {\bf TN} & {\bf Prec.}  & {\bf Rec.} & {\bf \fscore} & {\bf TP}/ {\bf FP}/ {\bf FN}/ {\bf TN} & {\bf Prec.}  & {\bf Rec.} & {\bf \fscore} & {\bf TP}/ {\bf FP}/ {\bf FN}/ {\bf TN} \\
 
   \midrule
 
   E3-CADETS &  0.90 & 0.99 & 0.95 & 12846/ 1408/ 6/ 705,558 & 0.94 & 0.99 & 0.96 & 12851/ 818/ 1/ 706,148 & 0.80 & 1.00 & 0.89 & 4/ 1/ 0/ 174 \\
   E3-TRACE &  0.94 & 0.99 & 0.96 & 67357/ 3951/ 26/ 2,412,056 & 0.95 & 0.99 & 0.97 &  67382/ 3477/ 1/ 2,412,530 & - & - & - & - \\
   E3-THEIA &  0.89 & 0.99 & 0.94 & 25311/ 3155/ 51/ 3,502,171 & 0.92 & 0.99 & 0.95 & 25318/ 2282/ 44/ 3,503,044 & 0.91 & 1.00 & 0.95 & 10/ 1/ 0/ 216 \\  
   OpTC & 0.88 & 0.93 & 0.90 & 605/ 80/ 45/ 1,287,275 & 0.93 & 0.92 & 0.93 & 600/ 43/ 50/ 1,287,312 & 0.84 & 1.00 & 0.91 & 32/ 6/ 0/ 1210 \\
   \bottomrule
   \end{tabular}
 \label{summary:benchmarks:large}
 \end{table*}}

\Sys is developed in Python and leverages the PyTorch and Torch Geometric libraries to implement the federated graph learning framework. For developing the word2vec and vector harmonization module, we employ the Gensim library. Secure communication between clients and the utility server is ensured through Python's Cryptography module. All other functionalities of Sys are encapsulated in Python functions, designed for easy customization and deployment across different system environments.

We evaluate Sys using the open-source datasets Darpa E3 and OptC, which comprise system audit logs that simulate enterprise environments. These logs are collected from both Windows and Linux operating systems. Our evaluation experiments are conducted on a machine running Ubuntu 18.04.6 LTS, equipped with an 8-core Intel CPU and 100 GB of memory. Additionally, the machine features an NVIDIA RTX2080 GPU, utilized for operating our graph learning framework. Our evaluation aims to address the following research questions.

\begin{itemize}[leftmargin=*]
\item \textbf{Q1.} How does \Sys compares to existing systems in terms of detection performance?
\item \textbf{Q2.} What is the effectiveness of word2vec harmonization in a utility server setting?
\item \textbf{Q3.} What is the resource consumption of \Sys running on a client machine?
\item \textbf{Q4.} What is the end to end processing time of \Sys on a client machine?
\item \textbf{Q5.} What is the effect of varying differential privacy noise on detection performance?
\item \textbf{Q6.} Ablation study of various \Sys components.
\end{itemize}

\subsection*{Datasets}
We have employed the Darpa E3 and \optc datasets for our evaluation. The E3 dataset originates from Darpa's third engagement exercise involving red and blue teams. In this exercise, the red team aimed to exploit vulnerabilities in an enterprise's services while masking their attacks with benign system activities. The logs captured from these exercises were documented under various scenario names, including Cadets, Trace, and Theia.

\optc is another open-source dataset from Darpa, encompasses a vast collection of audit logs from an enterprise environment with 1000 hosts. This dataset includes six days of benign system logs, which serve as training data for our system to understand normal behavior. Following the benign logs, there are attack logs covering three days of system activities, featuring red team tactics such as initial compromises, privilege escalations, malicious software installation, and data exfiltration.

Both datasets are accompanied by ground truth documents that help distinguish between benign and malicious events. We utilize attack labels from existing systems, such as \threatrace and FLASH, for our evaluation, cross-referencing these labels with the ground truth to ensure accuracy.

\subsection*{Detectors for Comparison}

To benchmark our system, we conduct comparisons against existing state-of-the-art Provenance-based Intrusion Detection Systems (PIDS). \threatrace, a node-level system, employs graph representation learning to identify anomalous nodes within a provenance graph. In contrast, FLASH, another node-level system, surpasses \threatrace in detection efficiency and effectiveness by leveraging semantic feature vectors and an embeddings database. Given FLASH's superiority over \threatrace, our comparison focuses primarily on FLASH. Additionally, we consider KAIROS, which utilizes temporal graph networks to capture the evolution of a system's provenance graph over time. KAIROS's detection granularity is limited to specific time windows; it aggregates logs within predetermined intervals, subsequently executing anomaly detection on these collected datasets. Consequently, we extend our comparison to include \Sys versus KAIROS, focusing on their respective detection performance.

 \subsection*{Detection Performance}

In this section, we discuss how \Sys compares with other systems in terms of detection performance. Initially, we outline our methodology for deploying \Sys on the DARPA E3 and \optc datasets. The E3 dataset comprises various scenarios, including Cadets, Theia, and Trace, each representing logs generated by a single host machine. To adapt \Sys for this structure, we treat each scenario as an individual host. Consequently, in our federated learning approach, we trained local \gnnshort models on each scenario individually. These local models then participated in federated averaging, a process repeated across ten rounds. Upon completing the training, we evaluated the global \gnnshort model against the attack logs from these E3 scenarios. For the \optc dataset, which is naturally divided among various hosts, we sampled three hosts for inclusion in our federated graph learning experiment. Additionally, we conducted experiments with a varying number of randomly selected hosts, the results of which are detailed in our ablation study.

Table~\ref{summary:benchmarks:large} reveals that \Sys's performance on these datasets is comparable to that of FLASH, despite the diverse log patterns and information contained within each E3 and \optc client. This underscores \Sys's capability to maintain robust detection performance amidst such heterogeneity. KAIROS's evaluation, based on a coarser time-window granularity compared to the node-level granularity of FLASH and \Sys, poses a challenge for direct comparison. Nevertheless, our results remain competitive with KAIROS.

Beyond detection performance, we also highlight \Sys's qualitative advantages, including its privacy-preserving features and distributed, scalable operation. These aspects underscore the value of \Sys in contrast to more centralized systems, emphasizing its utility in a broader range of applications.

 \subsection*{Efficacy of Word2vec harmonization}

 In this section, we evaluate the effectiveness of our Word2Vec vector harmonization scheme through two experiments using the \optc dataset. In the first experiment, each client utilized its locally trained Word2Vec model to encode semantic features during the training process. In the second experiment, we synchronized the individually trained models into a central Word2Vec model using the utility server architecture, as explained in Section~\ref{sec:methodology}. Then each client used this centralized model for generating semantic features. Table~\ref{local:wordvec} presents the experimental results. By employing the harmonized models, we achieved significantly better detection outcomes. This improvement is attributable to the inherent randomness of the Word2Vec algorithm, which causes different clients to produce divergent vectors for identical attributes. Such variability leads to heterogeneity in the feature vectors for the \gnnshort model, impairing the model's ability to generalize and converge effectively during the federated learning process, thereby yielding suboptimal results. However, through our novel, privacy-preserving aggregation of these semantic models, we have addressed this issue.

 \begin{table}[h!]
  \centering
  \scriptsize
    \caption{Effectiveness of word2vec vectors harmonization.}
      \begin{tabular}{ | c | c | c | c | c | c |}
        \hline
          \bf Word2vec Type & \bf Precision & \bf Recall & \bf \fscore & \bf TP & \bf FP \\
        \hline
         Local & 0.66  & 0.97 & 0.79 & 636 & 325 \\
         \hline
         Harmonized & 0.88 & 0.93 & 0.90 & 605 & 80 \\
        \hline
      \end{tabular}
      \label{local:wordvec}
  \end{table}

 \subsection*{CPU and Resource Consumption}

 \subsection*{Processing Time Analysis}

 \subsection*{Effect of Differential Privacy}

 \subsection*{Ablation Study}

\PP{Effect of Hosts vs Detection Performance}

\PP{Effect of Federated Learning Rounds}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.40\textwidth]{fig/scoresvshosts.pdf}
  \caption{Effect of number of hosts vs detection metrics.}
  \label{scoresvshosts}
  \vspace{-2ex}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.40\textwidth]{fig/cpu.pdf}
  \caption{CPU Usage}
  \label{cpu}
  \vspace{-2ex}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.40\textwidth]{fig/ram.pdf}
  \caption{RAM usage}
  \label{ram}
  \vspace{-2ex}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.40\textwidth]{fig/roundsvsscore.pdf}
  \caption{Federated Averaging Rounds vs Detection Performance.}
  \label{roundsvsscore}
  \vspace{-2ex}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.40\textwidth]{fig/sizevstime.pdf}
  \caption{Processing Time for various Audit Event Sizes}
  \label{sizevstime}
  \vspace{-2ex}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.40\textwidth]{fig/epsvsscore.pdf}
  \caption{Effect of Different LDP Noise Level on Detection Performance.}
  \label{epsvsscore}
  \vspace{-2ex}
\end{figure}

