\section{Privacy Preservation Analysis}
\label{sec:privacy}

Within \Sys, potential privacy compromises arise if either the central server, utility server or a malicious client can infer specific details about individual clients' logs, such as the applications in use or particular attributes like filenames and IP addresses. For each of the three components we present the proofs with strong privacy guarantees below:

\renewcommand{\thetheorem}{\arabic{theorem}}

\begin{theorem}[Central Server Privacy]
    For any two provenance graphs \(PGClient_1\), \(PGClient_2\) that differ only in the inclusion of a single node \(x\) and its \(\ell\)-hop attributed neighborhood (i.e., the local subgraph centered at \(x\), including all nodes in \(\mathcal{N}_\ell(x)\), edges, and associated attributes), the probability that an adversary \(\mathcal{A}\), observing only the GNN model updates received by the central server, can determine which graph was used deviates from random guessing by at most a negligible function \(\epsilon(n)\), where \(n\) is the security parameter representing the randomness complexity of the client-side training process.
    \end{theorem}
    
    \begin{proof}
    Let \(x \in \mathcal{V}_i\) denote a node in the client provenance graph \(PGClient_i = (\mathcal{V}_i, \mathcal{E}_i)\). Define its \(\ell\)-hop attributed neighborhood as the induced subgraph that includes:
    - the node \(x\),
    - all nodes within \(\ell\) hops (i.e., \(\mathcal{N}_\ell(x)\)),
    - all edges between them, and
    - their associated node and edge attributes.
    
    Let \(PGClient_1\) and \(PGClient_2\) be two graphs that are identical except that \(PGClient_1\) includes node \(x\) and its \(\ell\)-hop attributed neighborhood, while \(PGClient_2\) does not.
    
    Each client transforms the contextual attributes using the harmonized embedding model \(M_{\text{w2v-harm}}\), which produces vector representations for both nodes and edges. These embeddings are input to an ensemble of category-specific GNNs \(\{GNN_1, \ldots, GNN_{K_{\text{cat}}}\}\), each trained independently on its category. The resulting local gradient updates are sent to the central server, where they are aggregated into global model weights \(w_j^{(r)}\) for each category \(j\).
    
    Let \(GNN_j(PGClient_i; r)\) denote the client-side training of submodel \(j\) on provenance graph \(PGClient_i\) using randomness seed \(r\), which controls embedding initialization, mini-batch sampling, and optimizer dynamics. Even for the same graph, different runs may yield different model updates due to this local randomness.
    
    Let \(P_1\) and \(P_2\) denote the distributions over model updates resulting from training on \(PGClient_1\) and \(PGClient_2\), respectively. Their statistical distance is defined as:
    \begin{align*}
    \Delta(P_1, P_2) = \sup_{S} \big|\, 
    & \Pr\big[GNN_j(PGClient_1) \in S\big] \\
    & - \Pr\big[GNN_j(PGClient_2) \in S\big] \,\big|,
    \end{align*}
    where the supremum is taken over all measurable subsets \(S\) of the update space.
    
    Even though the only change between the graphs is node \(x\), the message-passing nature of GNNs causes its \(\ell\)-hop neighbors to also influence and be influenced by the presence of \(x\). Since embeddings propagate through the graph, the resulting update signal is not isolated to \(x\), but distributed across the receptive field. This non-locality, coupled with random initialization and category-specific GNNs, makes the change statistically hard to detect.
    
    Additionally, the number of possible attributed neighborhoods grows exponentially. Given node attribute vocabulary \(\mathcal{A}_v\) and edge attribute vocabulary \(\mathcal{A}_e\), the number of candidate neighborhoods centered at \(x\) satisfies:
    \[
    |\mathcal{S}(x)| \in \mathcal{O}\left(|\mathcal{A}_v|^{|\mathcal{N}_\ell(x)|} \cdot |\mathcal{A}_e|^{|\mathcal{N}_\ell(x)|}\right).
    \]
    Thus, an adversary attempting to infer the presence of \(x\) would need to simulate an exponential number of structurally and semantically plausible neighborhoods, without access to the embedding function \(M_{\text{w2v-harm}}\) or the client’s randomness seed.
    
    Consequently, the adversary's distinguishing advantage is bounded by:
    \[
    \left| \Pr[\mathcal{A}(w_j^{(r)} \text{ from } PGClient_1) = 1] - \tfrac{1}{2} \right| \leq \epsilon(n),
    \]
    where \(\epsilon(n)\) is negligible in the security parameter \(n\). That is, for every polynomial \(p(n)\), there exists \(n_0\) such that for all \(n > n_0\), \(\epsilon(n) < \frac{1}{p(n)}\).
    
    Therefore, \Sys ensures indistinguishability-based privacy under server-side observation of GNN model updates, even when client graphs differ by a single node and its attributed neighborhood.
    \end{proof}
    
\begin{theorem}[Utility Server Privacy]
Let \(M_{\text{w2v-harm}}: \mathcal{P}_{\text{global}} \to \mathbb{R}^d\) denote the client-side embedding function that maps input tokens \(p \in \mathcal{P}_{\text{global}}\) to high-dimensional vectors \(e \in \mathbb{R}^d\). In \Sys, each client independently applies \(M_{\text{w2v-harm}}\) to locally generate semantic embeddings and encrypts all tokens before transmission. The utility server receives only encrypted tokens and their corresponding embeddings. Then, for any embedding \(e\) observed by the utility server, the probability that an adversary \(\mathcal{A}\) can correctly identify the original token \(p\) such that \(e = M_{\text{w2v-harm}}(p)\) is at most \(\frac{1}{|\mathcal{P}_{\text{global}}|} + \epsilon(d)\), where \(\epsilon(d)\) is negligible in the embedding dimension \(d\).
\end{theorem}

\begin{proof}
Each client applies the local embedding function \(M_{\text{w2v-harm}}\) to convert input tokens \(p \in \mathcal{P}_{\text{global}}\) into semantic vectors \(e = M_{\text{w2v-harm}}(p) \in \mathbb{R}^d\), and encrypts the original tokens before sending them to the utility server. Thus, the utility server receives only encrypted identifiers and their associated semantic embeddings, with no access to the plaintext tokens or the internals of \(M_{\text{w2v-harm}}\).

The embedding function \(M_{\text{w2v-harm}}\), trained using a distributional objective (e.g., skip-gram with negative sampling), is inherently non-injective. Its goal is to preserve contextual similarity, not one-to-one invertibility. Formally, there exist distinct tokens \(p_1, p_2 \in \mathcal{P}_{\text{global}}\), with \(p_1 \neq p_2\), such that \(M_{\text{w2v-harm}}(p_1) \approx M_{\text{w2v-harm}}(p_2)\). Consequently, the preimage set of a given embedding \(e\) is:
\[
M_{\text{w2v-harm}}^{-1}(e) = \{p \in \mathcal{P}_{\text{global}} \mid M_{\text{w2v-harm}}(p) \approx e\},
\]
which may contain multiple semantically related tokens. This ambiguity arises because \(M_{\text{w2v-harm}}\) typically involves non-linear transformations and dimensionality reduction.

Let \(\text{Rev}(M_{\text{w2v-harm}}, e)\) denote a hypothetical reverse function attempting to infer \(p\) from \(e\). Since the utility server has no access to the original training data or encryption keys, its best strategy is effectively bounded by random guessing:
\[
\Pr[\text{Rev}(M_{\text{w2v-harm}}, e) = p] \leq \frac{1}{|\mathcal{P}_{\text{global}}|} + \epsilon(d),
\]
where \(\epsilon(d)\) becomes negligible as \(d\) increases. That is, for every polynomial \(p(d)\), there exists \(d_0\) such that for all \(d > d_0\), \(\epsilon(d) < \frac{1}{p(d)}\).

Therefore, \Sys guarantees that the probability of reverse-engineering semantic embeddings at the utility server is negligible, ensuring strong privacy protection.

\end{proof}

  
\begin{theorem}[Client-Level Privacy]
  In \Sys, let \(w_j^{(r)}\) denote the global model weights for category \(j\) after round \(r\), obtained via aggregation of updates from clients \(\mathcal{C} = \{C_1, \ldots, C_N\}\). Suppose a malicious client \(C_{\text{adv}} \in \mathcal{C}\) attempts to determine whether a specific process entity \(p \in \mathcal{P}_{\text{global}}\) was used by another client. Then, under \Sys’s federated aggregation design, \(C_{\text{adv}}\) cannot attribute the presence of \(p\) to any specific client, and client-level privacy is preserved.
  \end{theorem}
  
  \begin{proof}
  \Sys uses standard federated averaging to compute global model weights \(w_j^{(r)}\) by aggregating local model updates from all clients in \(\mathcal{C}\). These local updates are never shared directly among clients; only their aggregated result is transmitted to participating parties.
  
  As a result, a malicious client \(C_{\text{adv}}\) receives only the final aggregated model and has no visibility into individual contributions. The influence of any specific process entity \(p \in \mathcal{P}_{\text{global}}\) on \(w_j^{(r)}\) is diluted across all participating updates. Without access to isolated gradients or update traces, \(C_{\text{adv}}\) cannot determine whether \(p\) originated from any specific client.
  
  This anonymity-by-aggregation ensures that model contributions cannot be disaggregated or linked to specific sources. Therefore, \Sys preserves client-level privacy against attribution and token inference attacks under the federated aggregation setting.
  \end{proof}  

Therefore, the architecture of \Sys robustly defends against inference attacks from both servers and malicious clients. Its privacy guarantees rely on decentralized learning, semantic embedding ambiguity, and the assumption of non-colluding servers, as upheld by prior works~\cite{roy2020crypte,wu2022federated} and established in our threat model~\ref{sec:threat}.

