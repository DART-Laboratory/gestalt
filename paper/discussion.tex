\section{Discussion \& Future Work}
\label{sec:discussion}

% \wajih{Explain here why we chose GraphSage over GCN and TGN.}

% \wajih{Explain why Utility Server's Random Split is okay from privacy standpoint and accuracy standpoint and will not affect anything else. Also you can refer for more details in the design section.}



\PP{Combining FL with \gnnshort}
APTs involve causally linked steps across system entities, best modeled by provenance graphs~\cite{inam2023sok}. Traditional log-level systems~\cite{deeplog2017,liu2019log2vec,xia2019loggan} fail to capture these relationships, whereas graph learning techniques effectively detect such patterns~\cite{flash2024,cheng2023kairos,jia2023magic}. By integrating FL with \gnnshort, \Sys enables decentralized, privacy-preserving intrusion detection without sacrificing accuracy.

\PP{Unseen Tokens in Semantic Encoder} \Sys uses \wordvec models to encode semantic attributes, but these models only generate embeddings for previously seen tokens, degrading feature quality for new nodes with unseen tokens. Node representations in \Sys combine semantic attributes with neighboring system calls, but for unseen tokens, system calls dominate, as in \threatrace~\cite{wang2022threatrace}. More frequent \wordvec retraining or adopting subword-level models like fastText~\cite{joulin2016bag}, which build embeddings from subword units, can improve coverage. Tokenization methods like Byte-Pair Encoding~\cite{araabi2022effective} further aid by breaking unknown words into smaller components.

\PP{Scalability Bottlenecks}
As \Sys scales to large enterprise networks, two key bottlenecks emerge: (i) the utility server may face throughput limits due to processing encrypted tokens from all clients, and (ii) aggregating frequent model updates from many clients risks state explosion, increasing bandwidth and computational costs. \Sys mitigates these via lightweight models, ensemble learning over categorized system activities, and submodel aggregation. To further scale, we plan to explore hierarchical federation~\cite{zhong2022flee}, model compression~\cite{choudhary2020comprehensive}, selective update aggregation~\cite{ye2020federated}, and robust communication protocols~\cite{laclau2020robust}.

\PP{GraphSage Over Others} We adopt GraphSAGE as our base \gnnshort\ due to its inductive capability and efficient minibatch training, both well-suited for federated settings. Compared to GCN, which requires full-graph access, and TGN, which imposes a temporal constraint that complicates federated learning, GraphSAGE offers a better balance of scalability, flexibility, and performance.


\PP{Alert Investigation} Validating alerts in systems like \Sys is critical for reliability and avoiding alert fatigue~\cite{nodoze2019}. Traditionally, security analysts manually review alerts based on activities within local provenance graphs, but this process is time-consuming, error-prone, and lacks scalability and privacy. Privacy-preserving techniques such as Secure Multi-party Computation (SMC)~\cite{goldreich1998secure}, Homomorphic Encryption (HE)~\cite{yi2014homomorphic}, and Zero-Knowledge Proofs (ZKP)~\cite{fiege1987zero} can address these challenges. SMC enables collaborative alert validation while preserving input privacy, HE ensures confidentiality during encrypted data analysis, and ZKP allows one party to prove an alert's validity without revealing additional information. We identify privacy-preserving alert verification as a promising research direction. We leave it to future work to develop methods for privately sharing alert data with a central server, enabling analysts to perform attack analysis.

\PP{Explainability} Deep learning models in \pids, including \Sys, act as black boxes, making their decisions hard to interpret. Like other \pids~\cite{flash2024,cheng2023kairos,yangprographer}, \Sys struggles with this, limiting adoption versus rule-based systems. Existing explainability methods~\cite{antwarg2021explaining,brown2018recurrent,ardito2021revisiting,hwang2021sfd} mainly assess feature importance. But modern \pids use complex pipelines e.g., \flash uses \wordvec for textual features and \gnnshort for anomaly detection reducing existing methods' effectiveness. Explainable \pids remain a key research direction, with recent LLM advances~\cite{chang2024survey} offering promising avenues.

\PP{Log Retention} Effective retention~\cite{wilbert2012log,rapsheet2020} is essential for investigating long-running APTs. In decentralized systems like \Sys and \disdet, local storage constraints limit log history. To mitigate this, \Sys can integrate secure, tamper-resistant cloud storage~\cite{kumar2018secure,hardlog} for backups. During investigations, logs linked to alerts can be retrieved and sanitized~\cite{portillo2019towards} to preserve privacy while enabling detailed analysis.



\PP{Concept Drift} Concept drift occurs when the data distribution of the system evolves over time, potentially invalidating the patterns learned by \Sys during training. Emerging system activities can result in new benign behaviors being misclassified as anomalies. To address this, periodic retraining with recent data is essential to update the models. Strategies from recent works~\cite{lu2018learning, barbero2022transcending,jordaney2017transcend} provide effective approaches for mitigating concept drift.

% \PP{State Explosion in Massive Networks} Scaling FL to larger enterprise networks introduces potential state explosion issues, primarily due to the increasing complexity in managing and integrating updates from a growing number of clients. As more clients participate, each contributing their local model updates, the volume of data to be processed and aggregated can increase significantly. This escalation can strain communication channels, leading to higher bandwidth requirements and increased latency. Furthermore, the aggregation process itself becomes more computationally demanding as the number of updates grows. \Sys addresses these challenges by utilizing an ensemble learning approach and categorizing system activities, which simplifies the aggregation by processing more uniform data segments. Moreover, our system uses models with small network overhead to prevent bandwidth challenges. To further enhance efficiency, additional techniques such as model compression~\cite{choudhary2020comprehensive}, selective update aggregation~\cite{ye2020federated} and the implementation of more robust communication protocols~\cite{laclau2020robust} can significantly improve handling large volumes of updates, thus preventing state explosion and maintaining system performance in large-scale federated learning environments.




% \wajih{add citations for different techniques (SMC, HE, etc.) below}

%\PP{Retraining Frequency} \wajih{Discuss here how often we need to retrain models on client machines. You can say that it depends on enterprise settings, in our experiments we found that XXX times per day gave us good results.}

%\wajih{Investigation using \Sys. We need to add one paragraph saying something about how to do an investigation using \Sys after detection. It does not have to be detailed. We just need to show that we thought about this problem. At the end of the paragraph say that we leave this for future work.}

%\wajih{Talk about why we did not consider other privacy-preserving techniques beyond the federation, such as secure multi-party computation or homomorphic encryption.}