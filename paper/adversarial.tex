\subsection{Adversarial Attacks Analysis}
\label{sec:adversarial}

We discussed robustness to mimicry attacks in Section~\ref{sec:mimicry}. Here, we address vulnerabilities to other types of adversarial attacks, including gradient-based attacks, model poisoning, and inference attacks (discussed in Section~\ref{sec:privacy}). Addressing these attacks is beyond the scope of this paper, as we focus on establishing an end-to-end framework for privacy-aware PIDS. We plan to integrate advancements in adversarial defense mechanisms against such attacks into \Sys to enhance its robustness in future work. These integrations are discussed below.

\PP{Gradient Attacks} Gradient-based adversarial attacks~\cite{chakraborty2021survey} typically require white-box access to the target machine learning model, including its parameters. This necessity often renders them impractical for real-world applications. In contrast, black-box attacks, which utilize iterative, query-based techniques, tend to be more detectable and complex to implement due to their conspicuous nature. Such attacks are feasible if an attacker manages to compromise a client machine. However, during the operational phase, a compromised client cannot affect other clients because they are working independently. Several existing defenses can be employed during model training to enhance the system's resilience against these attacks. Adversarial training~\cite{tramer2019adversarial} is one effective strategy, wherein the model is trained with perturbed input data to increase its robustness to such attacks.

\PP{Poisoning Attacks} During the training phase, poisoning attacks executed by malicious actors may introduce corrupt weights to compromise the global model~\cite{jagielski2018manipulating}. To improve \Sys's resilience against such threats, several defensive methods can be used. Among these, advanced model aggregation methods, such as Multi-Krum~\cite{munoz2019byzantine}, can be particularly effective. This method employs clustering techniques on the central server to identify anomalous updates during model aggregation. Consequently, outlier updates are removed, enhancing the system's robustness against poisoning.

% \PP{Mimcry Attacks} These attacks aim to evade detection of PIDS by making the behavior of malicious nodes resemble that of benign ones, thereby deceiving the detector. Goyal et al.~\cite{goyal2023sometimes} proposed a mimicry attack designed to circumvent graph-based detection systems such as Streamspot~\cite{streamspot} and Unicorn~\cite{han2020unicorn}, ensuring that the nodes in the attack graph have embeddings similar to those of nodes engaged in benign activities. Another mimicry attack, ProvNinja~\cite{mukherjee2023evading}, crafts adversarial vectors that statistically impersonate system programs. \flash has demonstrated the ineffectiveness of Goyal et al.'s approach when faced with node-level detection granularity, as employed by \Sys. Constructing robust attack vectors for systems like \Sys, which utilize a range of semantic attributes from audit logs to generate contextually rich feature vectors, presents significant challenges for the ProvNinja attack.