{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Ingest E5 data into an elastic search cluster and enter your credentials below\n",
    "'''\n",
    "username = '*'\n",
    "password = '*'\n",
    "host = '*'\n",
    "index_name = \"*\"\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch([host], http_auth=(username, password))\n",
    "\n",
    "# Test connection and index existence\n",
    "if not es.ping():\n",
    "    print(\"Elasticsearch cluster is not accessible!\")\n",
    "else:\n",
    "    print(\"Connected to Elasticsearch.\")\n",
    "if not es.indices.exists(index=index_name):\n",
    "    print(f\"Index {index_name} does not exist.\")\n",
    "else:\n",
    "    print(f\"Index {index_name} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'elastic'\n",
    "password = 'stimulus5affect-roof'\n",
    "host = 'http://128.143.69.88:9200'\n",
    "index_name = \"theia*\"\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch([host], http_auth=(username, password))\n",
    "\n",
    "# Test connection and index existence\n",
    "if not es.ping():\n",
    "    print(\"Elasticsearch cluster is not accessible!\")\n",
    "else:\n",
    "    print(\"Connected to Elasticsearch.\")\n",
    "if not es.indices.exists(index=index_name):\n",
    "    print(f\"Index {index_name} does not exist.\")\n",
    "else:\n",
    "    print(f\"Index {index_name} exists.\")\n",
    "\n",
    "# Define the query\n",
    "query = {\"query\": {\"match_all\": {}}}\n",
    "\n",
    "# Get total count of documents to process\n",
    "total_docs = es.count(index=index_name, body=query)['count']\n",
    "print(f\"Total documents to process: {total_docs}\")\n",
    "\n",
    "edge_types = set([\n",
    "    'EVENT_CLOSE',\n",
    "    'EVENT_OPEN',\n",
    "    'EVENT_READ',\n",
    "    'EVENT_WRITE',\n",
    "    'EVENT_EXECUTE',\n",
    "    'EVENT_RECVFROM',\n",
    "    'EVENT_RECVMSG',\n",
    "    'EVENT_SENDMSG',\n",
    "    'EVENT_SENDTO',\n",
    "])\n",
    "\n",
    "# Define file paths\n",
    "id_to_type_file = 'e5_data/theia_id_to_type.json'\n",
    "net2prop_file = 'e5_data/theia_net2prop.json'\n",
    "info_file = 'e5_data/theia_info.json'\n",
    "\n",
    "# Initialize buffers\n",
    "net2prop_buffer = []\n",
    "id_to_type_buffer = []\n",
    "info_buffer = []\n",
    "buffer_size = 100000  # Change this number to adjust the buffer size\n",
    "\n",
    "# Function to append data to a file\n",
    "def append_to_file(file_path, data):\n",
    "    with open(file_path, 'a') as file:\n",
    "        for item in data:\n",
    "            file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Function to check buffer and flush if necessary\n",
    "def check_and_flush_buffer():\n",
    "    global net2prop_buffer, id_to_type_buffer, info_buffer\n",
    "    if len(net2prop_buffer) >= buffer_size:\n",
    "        append_to_file(net2prop_file, net2prop_buffer)\n",
    "        net2prop_buffer = []\n",
    "    if len(id_to_type_buffer) >= buffer_size:\n",
    "        append_to_file(id_to_type_file, id_to_type_buffer)\n",
    "        id_to_type_buffer = []\n",
    "    if len(info_buffer) >= buffer_size:\n",
    "        append_to_file(info_file, info_buffer)\n",
    "        info_buffer = []\n",
    "        \n",
    "# Start processing documents\n",
    "with tqdm(total=total_docs, desc=\"Processing Documents\") as pbar:\n",
    "    for doc in helpers.scan(es, query=query, index=index_name, size=1000):\n",
    "        pbar.update(1)\n",
    "        \n",
    "        line = doc['_source']\n",
    "        str_line = json.dumps(line)\n",
    "        \n",
    "        if \"avro.cdm20.NetFlowObject\" in str_line:\n",
    "            net_flow_object = line['datum']['com.bbn.tc.schema.avro.cdm20.NetFlowObject']\n",
    "            try:\n",
    "                nodeid = net_flow_object['uuid']\n",
    "                srcaddr = net_flow_object['localAddress'].get('string')\n",
    "                srcport = net_flow_object['localPort'].get('int')\n",
    "                dstaddr = net_flow_object['remoteAddress'].get('string')\n",
    "                dstport = net_flow_object['remotePort'].get('int')\n",
    "\n",
    "                net2prop_data = {nodeid: [srcaddr, srcport, dstaddr, dstport]}\n",
    "                id_to_type_data = {nodeid: 'NETFLOW'}\n",
    "                net2prop_buffer.append(net2prop_data)\n",
    "                id_to_type_buffer.append(id_to_type_data)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        if \"schema.avro.cdm20.Subject\" in str_line:\n",
    "            subject = line['datum']['com.bbn.tc.schema.avro.cdm20.Subject']\n",
    "            uuid = subject['uuid']\n",
    "            record_type = subject['type'] \n",
    "            id_to_type_data = {uuid: record_type}\n",
    "            id_to_type_buffer.append(id_to_type_data)\n",
    "\n",
    "        if \"schema.avro.cdm20.FileObject\" in str_line:\n",
    "            file_object = line['datum']['com.bbn.tc.schema.avro.cdm20.FileObject']\n",
    "            uuid = file_object['uuid']\n",
    "            file_type = file_object['type']\n",
    "            id_to_type_data = {uuid: file_type}\n",
    "            id_to_type_buffer.append(id_to_type_data)\n",
    "            \n",
    "        x = line            \n",
    "\n",
    "        try:\n",
    "            action = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['type']\n",
    "        except:\n",
    "            action = ''\n",
    "\n",
    "        try:\n",
    "            actor = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['subject']['com.bbn.tc.schema.avro.cdm20.UUID']\n",
    "        except:\n",
    "            actor = ''\n",
    "\n",
    "        try:\n",
    "            obj = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm20.UUID']\n",
    "        except:\n",
    "            obj = ''\n",
    "\n",
    "        try:\n",
    "            cmd = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['properties']['map']['exec']\n",
    "        except:\n",
    "            cmd = ''\n",
    "\n",
    "        try:\n",
    "            path = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['predicateObjectPath']['string']\n",
    "        except:\n",
    "            path = ''\n",
    "\n",
    "        try:\n",
    "            timestampnano = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['timestampNanos']\n",
    "            timestamp = x['@timestamp']\n",
    "        except:\n",
    "            timestamp = ''\n",
    "            timestampnano = ''\n",
    "\n",
    "        if action in edge_types:\n",
    "            info_data = {'actorID': actor, 'objectID': obj, 'action': action, 'timestampNanos': timestampnano, 'timestamp': timestamp, 'exec': cmd, 'path': path, 'hostid': x['hostId']}\n",
    "            info_buffer.append(info_data)\n",
    "        \n",
    "        check_and_flush_buffer()\n",
    "        \n",
    "append_to_file(net2prop_file, net2prop_buffer)\n",
    "append_to_file(id_to_type_file, id_to_type_buffer)\n",
    "append_to_file(info_file, info_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_jsonl(file_path):\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            result.update(data)\n",
    "    return result\n",
    "\n",
    "def load_list_from_jsonl(file_path):\n",
    "    result = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            result.append(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_to_type_file = 'e5_data/clearscope_id_to_type.json'\n",
    "net2prop_file = 'e5_data/clearscope_net2prop.json'\n",
    "info_file = 'e5_data/clearscope_info.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch():\n",
    "    \n",
    "    global id_to_type_file, net2prop_file, info_file\n",
    "    \n",
    "    id_to_type = load_dict_from_jsonl(id_to_type_file)\n",
    "    net2prop = load_dict_from_jsonl(net2prop_file)\n",
    "    info = load_list_from_jsonl(info_file)\n",
    "    \n",
    "    for i in range(len(info)):\n",
    "        try:\n",
    "            typ = id_to_type[info[i]['objectID']]\n",
    "            info[i]['object'] = typ\n",
    "            info[i]['actor_type'] = id_to_type[info[i]['actorID']]\n",
    "            if typ == 'NETFLOW':\n",
    "                attr = net2prop[info[i]['objectID']]\n",
    "                info[i]['path'] = attr[0]+' '+attr[1]+' '+attr[2]+' '+attr[3]\n",
    "        except:\n",
    "            info[i]['object'] = None\n",
    "            info[i]['actor_type'] = None\n",
    "            \n",
    "    df = pd.DataFrame.from_records(info)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stitch()\n",
    "df.to_parquet(\"e5_data/clearscope_df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def load_dict_from_jsonl(file_path):\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            result.update(data)\n",
    "    return result\n",
    "\n",
    "def process_info_file(info_file_path, id_to_type, net2prop, parquet_file_path):\n",
    "    # Define the schema with all fields as string types, except timestampNanos as int64\n",
    "    schema = pa.schema([\n",
    "        ('actorID', pa.string()),\n",
    "        ('objectID', pa.string()),\n",
    "        ('actor_type', pa.string()),\n",
    "        ('object', pa.string()),\n",
    "        ('action', pa.string()),\n",
    "        ('timestampNanos', pa.int64()),\n",
    "        ('timestamp', pa.string()),\n",
    "        ('exec', pa.string()),\n",
    "        ('path', pa.string()),\n",
    "        ('hostid', pa.string()),\n",
    "    ])\n",
    "    \n",
    "    writer = None\n",
    "    \n",
    "    with open(info_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            try:\n",
    "                typ = id_to_type[data['objectID']]\n",
    "                data['object'] = typ\n",
    "                data['actor_type'] = id_to_type[data['actorID']]\n",
    "                if typ == 'NETFLOW':\n",
    "                    attr = net2prop[data['objectID']]\n",
    "                    data['path'] = ' '.join(str(item) for item in attr)\n",
    "            except KeyError:\n",
    "                data['object'] = None\n",
    "                data['actor_type'] = None\n",
    "            \n",
    "            # Convert fields to their respective types\n",
    "            record = {key: data.get(key, '') for key in schema.names}\n",
    "            record['timestampNanos'] = int(data.get('timestampNanos', 0)) if data.get('timestampNanos') is not None else 0\n",
    "            \n",
    "            # Convert this record into a PyArrow RecordBatch\n",
    "            batch = pa.RecordBatch.from_pandas(pd.DataFrame([record]), schema=schema, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(parquet_file_path, schema, compression='snappy')\n",
    "            writer.write_batch(batch)\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "def stitch():\n",
    "    global id_to_type_file, net2prop_file, info_file\n",
    "    \n",
    "    id_to_type = load_dict_from_jsonl(id_to_type_file)\n",
    "    net2prop = load_dict_from_jsonl(net2prop_file)\n",
    "    \n",
    "    process_info_file(info_file, id_to_type, net2prop, \"e5_data/clearscope_df.parquet\")\n",
    "    \n",
    "stitch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dict_from_jsonl(file_path):\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            result.update(data)\n",
    "    return result\n",
    "\n",
    "def is_timestamp_in_range(timestamp_str):\n",
    "    # Define your time ranges\n",
    "    range1_start = datetime.strptime(\"2019-05-08 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    range1_end = datetime.strptime(\"2019-05-09 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    range2_start = datetime.strptime(\"2019-05-15 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    range2_end = datetime.strptime(\"2019-05-16 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    range3_start = datetime.strptime(\"2019-05-17 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    range3_end = datetime.strptime(\"2019-05-18 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Adjust the parsing format to match the timestamp in your data, ignoring milliseconds and timezone\n",
    "    # First, remove the timezone information if it exists\n",
    "    if '+' in timestamp_str:\n",
    "        timestamp_str = timestamp_str.split('+')[0]\n",
    "    elif '-' in timestamp_str and timestamp_str.rfind('-') > timestamp_str.find('T'):\n",
    "        timestamp_str = timestamp_str.rsplit('-', 1)[0]\n",
    "    \n",
    "    # Next, remove the milliseconds\n",
    "    timestamp_str = timestamp_str.split('.')[0]\n",
    "    \n",
    "    # Finally, parse the timestamp\n",
    "    timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Check if the timestamp falls within any of the defined ranges\n",
    "    if timestamp < range1_start or (range2_start <= timestamp < range2_end) or (range3_start <= timestamp < range3_end):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def process_info_file(info_file_path, id_to_type, net2prop, parquet_file_path):\n",
    "    schema = pa.schema([\n",
    "        ('actorID', pa.string()),\n",
    "        ('objectID', pa.string()),\n",
    "        ('actor_type', pa.string()),\n",
    "        ('object', pa.string()),\n",
    "        ('action', pa.string()),\n",
    "        ('timestampNanos', pa.int64()),\n",
    "        ('timestamp', pa.string()),\n",
    "        ('exec', pa.string()),\n",
    "        ('path', pa.string()),\n",
    "        ('hostid', pa.string()),\n",
    "    ])\n",
    "    \n",
    "    writer = None\n",
    "    \n",
    "    with open(info_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            if not is_timestamp_in_range(data.get('timestamp', '')):\n",
    "                continue  # Skip records not within the specified time ranges\n",
    "            \n",
    "            try:\n",
    "                typ = id_to_type[data['objectID']]\n",
    "                data['object'] = typ\n",
    "                data['actor_type'] = id_to_type[data['actorID']]\n",
    "                if typ == 'NETFLOW':\n",
    "                    attr = net2prop[data['objectID']]\n",
    "                    data['path'] = ' '.join(str(item) for item in attr)\n",
    "            except KeyError:\n",
    "                data['object'] = None\n",
    "                data['actor_type'] = None\n",
    "            \n",
    "            record = {key: data.get(key, '') for key in schema.names}\n",
    "            record['timestampNanos'] = int(data.get('timestampNanos', 0)) if data.get('timestampNanos') is not None else 0\n",
    "            \n",
    "            batch = pa.RecordBatch.from_pandas(pd.DataFrame([record]), schema=schema, preserve_index=False)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(parquet_file_path, schema, compression='snappy')\n",
    "            writer.write_batch(batch)\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "def stitch():\n",
    "    global id_to_type_file, net2prop_file, info_file\n",
    "    \n",
    "    id_to_type = load_dict_from_jsonl(id_to_type_file)\n",
    "    net2prop = load_dict_from_jsonl(net2prop_file)\n",
    "    \n",
    "    process_info_file(info_file, id_to_type, net2prop, \"e5_data/clearscope_df.parquet\")\n",
    "    \n",
    "stitch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dict_from_jsonl(file_path):\n",
    "    result = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            result.update(data)\n",
    "    return result\n",
    "\n",
    "def stitch(data_buffer,file_path):\n",
    "    \n",
    "    id_to_type_file = 'e5_data/theia_id_to_type.json'\n",
    "    net2prop_file = 'e5_data/theia_net2prop.json' \n",
    "    \n",
    "    id_to_type = load_dict_from_jsonl(id_to_type_file)\n",
    "    net2prop = load_dict_from_jsonl(net2prop_file)\n",
    "    info = data_buffer\n",
    "    \n",
    "    for i in range(len(info)):\n",
    "        try:\n",
    "            typ = id_to_type[info[i]['objectID']]\n",
    "            info[i]['object'] = typ\n",
    "            info[i]['actor_type'] = id_to_type[info[i]['actorID']]\n",
    "            if typ == 'NETFLOW':\n",
    "                attr = net2prop[info[i]['objectID']]\n",
    "                info[i]['path'] = attr[0]+' '+attr[1]+' '+attr[2]+' '+attr[3]\n",
    "        except:\n",
    "            info[i]['object'] = None\n",
    "            info[i]['actor_type'] = None\n",
    "            \n",
    "    df = pd.DataFrame.from_records(info)\n",
    "    df = df.dropna()\n",
    "    df.to_parquet(f\"e5_data/{file_path}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_elastic(query,file_path):\n",
    "    \n",
    "    username = 'elastic'\n",
    "    password = 'stimulus5affect-roof'\n",
    "    host = 'http://128.143.69.88:9200'\n",
    "    index_name = \"clearscope*\"\n",
    "\n",
    "    # Initialize Elasticsearch client\n",
    "    es = Elasticsearch([host], http_auth=(username, password))\n",
    "\n",
    "    # Test connection and index existence\n",
    "    if not es.ping():\n",
    "        print(\"Elasticsearch cluster is not accessible!\")\n",
    "    else:\n",
    "        print(\"Connected to Elasticsearch.\")\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        print(f\"Index {index_name} does not exist.\")\n",
    "    else:\n",
    "        print(f\"Index {index_name} exists.\")\n",
    "\n",
    "    total_docs = es.count(index=index_name, body=query)['count']\n",
    "\n",
    "    edge_types = set([\n",
    "        'EVENT_CLOSE',\n",
    "        'EVENT_OPEN',\n",
    "        'EVENT_READ',\n",
    "        'EVENT_WRITE',\n",
    "        'EVENT_EXECUTE',\n",
    "        'EVENT_RECVFROM',\n",
    "        'EVENT_RECVMSG',\n",
    "        'EVENT_SENDMSG',\n",
    "        'EVENT_SENDTO',\n",
    "    ])\n",
    "\n",
    "    info_buffer = []\n",
    "\n",
    "    # Start processing documents\n",
    "    with tqdm(total=total_docs, desc=\"Processing Documents\") as pbar:\n",
    "        for doc in helpers.scan(es, query=query, index=index_name, size=5000):\n",
    "            pbar.update(1)\n",
    "\n",
    "            line = doc['_source']\n",
    "            str_line = json.dumps(line)\n",
    "\n",
    "            x = line            \n",
    "\n",
    "            try:\n",
    "                action = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['type']\n",
    "            except:\n",
    "                action = ''\n",
    "\n",
    "            try:\n",
    "                actor = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['subject']['com.bbn.tc.schema.avro.cdm20.UUID']\n",
    "            except:\n",
    "                actor = ''\n",
    "\n",
    "            try:\n",
    "                obj = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm20.UUID']\n",
    "            except:\n",
    "                obj = ''\n",
    "\n",
    "            try:\n",
    "                cmd = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['properties']['map']['exec']\n",
    "            except:\n",
    "                cmd = ''\n",
    "\n",
    "            try:\n",
    "                path = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['predicateObjectPath']['string']\n",
    "            except:\n",
    "                path = ''\n",
    "\n",
    "            try:\n",
    "                timestampnano = x['datum']['com.bbn.tc.schema.avro.cdm20.Event']['timestampNanos']\n",
    "                timestamp = x['@timestamp']\n",
    "            except:\n",
    "                timestamp = ''\n",
    "                timestampnano = ''\n",
    "\n",
    "            if action in edge_types:\n",
    "                info_data = {'actorID': actor, 'objectID': obj, 'action': action, 'timestampNanos': timestampnano, 'timestamp': timestamp, 'exec': cmd, 'path': path, 'hostid': x['hostId']}\n",
    "                info_buffer.append(info_data)\n",
    "    \n",
    "    stitch(info_buffer,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "      \"query\": {\n",
    "        \"bool\": {\n",
    "          \"should\": [\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                  \"gte\": \"2019-05-15T00:00:00.360Z\",\n",
    "                  \"lte\": \"2019-05-16T00:00:00.360Z\",\n",
    "                  \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                  \"gte\": \"2019-05-17T00:00:00.360Z\",\n",
    "                  \"lte\": \"2019-05-18T00:00:00.360Z\",\n",
    "                  \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          ],\n",
    "          \"minimum_should_match\": 1\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "query_elastic(query,\"mal_clearscope1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "tasks = [\n",
    "    ({\"query\": {\"range\": {\"@timestamp\": {\"gte\": \"2019-05-08T00:00:00.360Z\", \"lte\": \"2019-05-09T00:00:00.360Z\", \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"}}}}, \"ben_clearscope1\"),\n",
    "    ({\"query\": {\"bool\": {\"should\": [{\"range\": {\"@timestamp\": {\"gte\": \"2019-05-15T00:00:00.360Z\", \"lte\": \"2019-05-16T00:00:00.360Z\", \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"}},},{\"range\": {\"@timestamp\": {\"gte\": \"2019-05-17T00:00:00.360Z\", \"lte\": \"2019-05-18T00:00:00.360Z\", \"format\": \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"}}}], \"minimum_should_match\": 1}}}, \"mal_clearscope1\"),\n",
    "]\n",
    "\n",
    "# Execute the queries in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Schedule the query_elastic function to be called with the queries\n",
    "    future_to_query = {executor.submit(query_elastic, task[0], task[1]): task for task in tasks}\n",
    "    \n",
    "    # Iterate through the completed futures\n",
    "    for future in concurrent.futures.as_completed(future_to_query):\n",
    "        task = future_to_query[future]\n",
    "        try:\n",
    "            # Get the result of the execution\n",
    "            data = future.result()\n",
    "            # Optionally, process the result here\n",
    "        except Exception as exc:\n",
    "            print(f'{task[1]} generated an exception: {exc}')\n",
    "        else:\n",
    "            print(f'{task[1]} has completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the time ranges\n",
    "time_ranges = [\n",
    "    {\"gte\": \"2019-05-15T00:00:00\", \"lte\": \"2019-05-16T00:00:00\"},\n",
    "    {\"gte\": \"2019-05-17T00:00:00\", \"lte\": \"2019-05-18T00:00:00\"},\n",
    "]\n",
    "\n",
    "# Convert time range strings to datetime objects for comparison\n",
    "for range_ in time_ranges:\n",
    "    range_[\"gte\"] = datetime.strptime(range_[\"gte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    range_[\"lte\"] = datetime.strptime(range_[\"lte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "def record_in_range(record_timestamp, time_ranges):\n",
    "    \"\"\"Check if the record's timestamp falls within any of the specified ranges.\"\"\"\n",
    "    # Remove timezone and microseconds from the timestamp\n",
    "    # This takes up to the seconds component and ignores the rest\n",
    "    timestamp_without_tz_and_microseconds = record_timestamp[:19]\n",
    "    \n",
    "    # Parse the adjusted timestamp\n",
    "    record_dt = datetime.strptime(timestamp_without_tz_and_microseconds, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Check if the datetime object falls within any of the specified ranges\n",
    "    return any(range_[\"gte\"] <= record_dt <= range_[\"lte\"] for range_ in time_ranges)\n",
    "\n",
    "\n",
    "info_buffer = []\n",
    "\n",
    "# Assuming the file path is 'your_file.json'\n",
    "with open('e5_data/clearscope_info.json', 'r') as file:\n",
    "    file.seek(0, 2)  # Move to the end of the file to get the total size\n",
    "    total_size = file.tell()\n",
    "    file.seek(0)  # Reset to the start of the file\n",
    "\n",
    "    processed_bytes = 0  # Initialize processed bytes counter\n",
    "    \n",
    "    with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Processing records\") as pbar:\n",
    "        for line in file:\n",
    "            line_bytes = len(line.encode('utf-8'))  # Get the byte length of the line\n",
    "            processed_bytes += line_bytes  # Update processed bytes\n",
    "            \n",
    "            line = line.replace(\"'\", '\"')  # Ensure proper JSON formatting\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                if record_in_range(record['timestamp'], time_ranges):\n",
    "                    info_buffer.append(record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "            pbar.update(line_bytes)  # Update the progress bar with the byte length of the processed line\n",
    "\n",
    "print(f\"Filtered records count: {len(info_buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the time ranges\n",
    "time_ranges = [\n",
    "    {\"gte\": \"2019-05-15T00:00:00\", \"lte\": \"2019-05-16T00:00:00\"},\n",
    "    {\"gte\": \"2019-05-17T00:00:00\", \"lte\": \"2019-05-18T00:00:00\"},\n",
    "]\n",
    "\n",
    "# Convert time range strings to datetime objects for comparison\n",
    "for range_ in time_ranges:\n",
    "    range_[\"gte\"] = datetime.strptime(range_[\"gte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    range_[\"lte\"] = datetime.strptime(range_[\"lte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "def record_in_range(record_timestamp, time_ranges):\n",
    "    record_dt = datetime.strptime(record_timestamp[:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    return any(range_[\"gte\"] <= record_dt <= range_[\"lte\"] for range_ in time_ranges)\n",
    "\n",
    "info_buffer = []\n",
    "\n",
    "total_records = 152489742  # The total number of records\n",
    "\n",
    "# Assuming the file path is 'your_file.json'\n",
    "with open('e5_data/clearscope_info.json', 'r') as file:\n",
    "    # Initialize the progress bar with the total number of records\n",
    "    with tqdm(total=total_records, unit='records', desc=\"Processing records\") as pbar:\n",
    "        for line in file:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                if record_in_range(record['timestamp'], time_ranges):\n",
    "                    info_buffer.append(record)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "            finally:\n",
    "                # Update the progress bar by one for each record processed\n",
    "                pbar.update(1)\n",
    "\n",
    "print(f\"Filtered records count: {len(info_buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "time_ranges = [\n",
    "    {\"gte\": \"2019-05-15T00:00:00\", \"lte\": \"2019-05-16T00:00:00\"},\n",
    "    {\"gte\": \"2019-05-17T00:00:00\", \"lte\": \"2019-05-18T00:00:00\"},\n",
    "]\n",
    "\n",
    "# Convert the time range strings to datetime objects for comparison\n",
    "for range_ in time_ranges:\n",
    "    range_[\"gte\"] = datetime.strptime(range_[\"gte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    range_[\"lte\"] = datetime.strptime(range_[\"lte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "def record_in_range(record_timestamp, time_ranges):\n",
    "    # If the input is already a datetime object (e.g., Timestamp), use it directly\n",
    "    if isinstance(record_timestamp, datetime):\n",
    "        record_dt = record_timestamp\n",
    "    else:\n",
    "        # Otherwise, assume it's a string and parse it\n",
    "        record_dt = datetime.strptime(record_timestamp[:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Convert to offset-naive UTC datetime for comparison\n",
    "    if record_dt.tzinfo is not None:\n",
    "        record_dt = record_dt.replace(tzinfo=None)\n",
    "    \n",
    "    return any(range_[\"gte\"] <= record_dt <= range_[\"lte\"] for range_ in time_ranges)\n",
    "\n",
    "# Function to apply to each partition of the Dask DataFrame\n",
    "def filter_partition(partition, time_ranges=time_ranges):\n",
    "    return partition[partition['timestamp'].apply(lambda x: record_in_range(x, time_ranges))]\n",
    "\n",
    "# Assuming JSON records are stored one per line in 'your_file.json'\n",
    "ddf = dd.read_json('e5_data/clearscope_info.json', blocksize=\"1GB\")\n",
    "\n",
    "# Apply the filtering function to each partition\n",
    "filtered_ddf = ddf.map_partitions(filter_partition, time_ranges=time_ranges, meta=ddf)\n",
    "\n",
    "# Wrap the compute() call with the ProgressBar context manager\n",
    "with ProgressBar():\n",
    "    filtered_result = filtered_ddf.compute()\n",
    "\n",
    "# Convert the result to a list of dictionaries\n",
    "info_buffer = filtered_result.to_dict(orient='records')\n",
    "\n",
    "print(f\"Filtered records count: {len(info_buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "time_ranges = [\n",
    "    {\"gte\": \"2019-05-15T00:00:00\", \"lte\": \"2019-05-16T00:00:00\"},\n",
    "]\n",
    "\n",
    "# Convert the time range strings to datetime objects for comparison\n",
    "for range_ in time_ranges:\n",
    "    range_[\"gte\"] = datetime.strptime(range_[\"gte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    range_[\"lte\"] = datetime.strptime(range_[\"lte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "def record_in_range(record_timestamp, time_ranges):\n",
    "    # If the input is already a datetime object (e.g., Timestamp), use it directly\n",
    "    if isinstance(record_timestamp, datetime):\n",
    "        record_dt = record_timestamp\n",
    "    else:\n",
    "        # Otherwise, assume it's a string and parse it\n",
    "        record_dt = datetime.strptime(record_timestamp[:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Convert to offset-naive UTC datetime for comparison\n",
    "    if record_dt.tzinfo is not None:\n",
    "        record_dt = record_dt.replace(tzinfo=None)\n",
    "    \n",
    "    return any(range_[\"gte\"] <= record_dt <= range_[\"lte\"] for range_ in time_ranges)\n",
    "\n",
    "# Function to apply to each partition of the Dask DataFrame\n",
    "def filter_partition(partition, time_ranges=time_ranges):\n",
    "    return partition[partition['timestamp'].apply(lambda x: record_in_range(x, time_ranges))]\n",
    "\n",
    "# Assuming JSON records are stored one per line in 'your_file.json'\n",
    "ddf = dd.read_json('e5_data/theia_info.json', blocksize=\"1GB\")\n",
    "\n",
    "# Apply the filtering function to each partition\n",
    "filtered_ddf = ddf.map_partitions(filter_partition, time_ranges=time_ranges, meta=ddf)\n",
    "\n",
    "# Wrap the compute() call with the ProgressBar context manager\n",
    "with ProgressBar():\n",
    "    filtered_result = filtered_ddf.compute()\n",
    "\n",
    "# Convert the result to a list of dictionaries\n",
    "info_buffer = filtered_result.to_dict(orient='records')\n",
    "\n",
    "print(f\"Filtered records count: {len(info_buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch(info_buffer,\"theia_mal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "time_ranges = [\n",
    "    {\"gte\": \"2019-05-8T00:00:00\", \"lte\": \"2019-05-9T00:00:00\"},\n",
    "]\n",
    "\n",
    "# Convert the time range strings to datetime objects for comparison\n",
    "for range_ in time_ranges:\n",
    "    range_[\"gte\"] = datetime.strptime(range_[\"gte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    range_[\"lte\"] = datetime.strptime(range_[\"lte\"], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "def record_in_range(record_timestamp, time_ranges):\n",
    "    # If the input is already a datetime object (e.g., Timestamp), use it directly\n",
    "    if isinstance(record_timestamp, datetime):\n",
    "        record_dt = record_timestamp\n",
    "    else:\n",
    "        # Otherwise, assume it's a string and parse it\n",
    "        record_dt = datetime.strptime(record_timestamp[:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    \n",
    "    # Convert to offset-naive UTC datetime for comparison\n",
    "    if record_dt.tzinfo is not None:\n",
    "        record_dt = record_dt.replace(tzinfo=None)\n",
    "    \n",
    "    return any(range_[\"gte\"] <= record_dt <= range_[\"lte\"] for range_ in time_ranges)\n",
    "\n",
    "# Function to apply to each partition of the Dask DataFrame\n",
    "def filter_partition(partition, time_ranges=time_ranges):\n",
    "    return partition[partition['timestamp'].apply(lambda x: record_in_range(x, time_ranges))]\n",
    "\n",
    "# Assuming JSON records are stored one per line in 'your_file.json'\n",
    "ddf = dd.read_json('e5_data/theia_info.json', blocksize=\"1GB\")\n",
    "\n",
    "# Apply the filtering function to each partition\n",
    "filtered_ddf = ddf.map_partitions(filter_partition, time_ranges=time_ranges, meta=ddf)\n",
    "\n",
    "# Wrap the compute() call with the ProgressBar context manager\n",
    "with ProgressBar():\n",
    "    filtered_result = filtered_ddf.compute()\n",
    "\n",
    "# Convert the result to a list of dictionaries\n",
    "info_buffer = filtered_result.to_dict(orient='records')\n",
    "\n",
    "print(f\"Filtered records count: {len(info_buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch(info_buffer,\"theia_ben\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
