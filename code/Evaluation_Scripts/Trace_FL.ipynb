{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F1op-CbyLuN4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the require libraries here\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import orjson as json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import multiprocessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be/Notebooks_FL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and setting up working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be/content'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "Setting working directory for rivana jupyter notebook\n",
    "'''\n",
    "os.chdir(\"../content\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nM7KaeCbA_mQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing some additional libraries\n",
    "'''\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for loading, cleaning and constructing features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BfjmrhfUr3pK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the main featurizer. It constructs the graph for the cadets dataset.\n",
    "\n",
    "Args:\n",
    "    df (DataFrame): This is the main dataframe containing all the system events from the cadets dataset.\n",
    "\n",
    "return:\n",
    "    features (list): Contains word2vec encoded feature vectors for each node\n",
    "    feat_labels (list): Contains label for each node\n",
    "    edge_index (list): Contains information about edges between nodes in the graph.\n",
    "    mapp (list): contains id of each node\n",
    "'''\n",
    "\n",
    "def prepare_graph(df):\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    edges = []\n",
    "    \n",
    "    dummies = {\"SUBJECT_PROCESS\":0, \"MemoryObject\":1, \"FILE_OBJECT_CHAR\":2, \"FILE_OBJECT_FILE\":3, \"FILE_OBJECT_DIR\":4, \"SUBJECT_UNIT\":5, \"UnnamedPipeObject\":6, \"FILE_OBJECT_UNIX_SOCKET\":7, \"SRCSINK_UNKNOWN\":8, \"FILE_OBJECT_LINK\":9, \"NetFlowObject\":10, \"FILE_OBJECT_BLOCK\":11}\n",
    "    for i in range(len(df)):\n",
    "        x = df.iloc[i]\n",
    "        action = x[\"action\"]\n",
    "        \n",
    "        actorid = x[\"actorID\"]\n",
    "        if not (actorid in nodes):\n",
    "            nodes[actorid] =  []\n",
    "        nodes[actorid].append(x['exec'])\n",
    "        nodes[actorid].append(action)\n",
    "        if x['path'] != '':\n",
    "            nodes[actorid].append(x['path'])\n",
    "        labels[actorid] = dummies[x['actor_type']]\n",
    "\n",
    "        objectid = x[\"objectID\"]\n",
    "        if not (objectid in nodes):\n",
    "            nodes[objectid] =  []\n",
    "        nodes[objectid].append(x['exec'])\n",
    "        nodes[objectid].append(action)\n",
    "        if x['path'] != '':\n",
    "             nodes[objectid].append(x['path'])\n",
    "        labels[objectid] = dummies[x['object']]\n",
    "\n",
    "        edges.append(( actorid, objectid ))\n",
    "\n",
    "    features = []\n",
    "    feat_labels = []\n",
    "    edge_index = [[],[]]\n",
    "    index  = {}\n",
    "    mapp = []\n",
    "\n",
    "    for k,v in nodes.items():\n",
    "      features.append(v)\n",
    "      feat_labels.append(labels[k])\n",
    "      index[k] = len(features) - 1\n",
    "      mapp.append(k)\n",
    "\n",
    "    for x in edges:\n",
    "        src = index[x[0]]\n",
    "        dst = index[x[1]]\n",
    "\n",
    "        edge_index[0].append(src)\n",
    "        edge_index[1].append(dst)\n",
    "\n",
    "    return features,feat_labels,edge_index,mapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fmXWs1dKIzD8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the model. The model consists of two sageconv layers from the paper GraphSage\n",
    "'''\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channel, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, out_channel, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YBuP_tSq94f4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function helps visualize the output of the model.\n",
    "'''\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PCP6SXwZaif"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This saves the word2vec model after each iteration\n",
    "'''\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('word2vec_trace_E3.model')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "P8oBL8LFaeOf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This logs the epoch number to the console\n",
    "'''\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Se7Ei4tAapVj"
   },
   "outputs": [],
   "source": [
    "logger = EpochLogger()\n",
    "saver = EpochSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HvH1OSGqgTe7"
   },
   "outputs": [],
   "source": [
    "f = open(\"darpatc/trace_train.txt\")\n",
    "data = f.read().split('\\n')\n",
    "data = [line.split('\\t') for line in data]\n",
    "df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "df = df.dropna()\n",
    "df.sort_values(by='timestamp', ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding semantic attributes from the raw cadets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attributes(d,p):\n",
    "    \n",
    "    f = open(p)\n",
    "    data = [json.loads(x) for x in f if \"EVENT\" in x]\n",
    "\n",
    "    info = []\n",
    "    for x in data:\n",
    "        try:\n",
    "            action = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['type']\n",
    "        except:\n",
    "            action = ''\n",
    "        try:\n",
    "            actor = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['subject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            actor = ''\n",
    "        try:\n",
    "            obj = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            obj = ''\n",
    "        try:\n",
    "            timestamp = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['timestampNanos']\n",
    "        except:\n",
    "            timestamp = ''\n",
    "        try:\n",
    "            cmd = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['properties']['map']['exec']\n",
    "        except:\n",
    "            cmd = ''\n",
    "        try:\n",
    "            path = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObjectPath']['string']\n",
    "        except:\n",
    "            path = ''\n",
    "        try:\n",
    "            path2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2Path']['string']\n",
    "        except:\n",
    "            path2 = ''\n",
    "        try:\n",
    "            obj2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "            info.append({'actorID':actor,'objectID':obj2,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path2})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        info.append({'actorID':actor,'objectID':obj,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path})\n",
    "\n",
    "    rdf = pd.DataFrame.from_records(info).astype(str)\n",
    "    d = d.astype(str)\n",
    "\n",
    "    return d.merge(rdf,how='inner',on=['actorID','objectID','action','timestamp']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_word2vec_models(models):\n",
    "    # Create an empty unified model\n",
    "    unified_model = Word2Vec(vector_size=models[0].vector_size, window=models[0].window, min_count=models[0].min_count, sg=models[0].sg)\n",
    "\n",
    "    # Initialize the vocabulary with the words from the first model\n",
    "    unified_model.build_vocab([list(models[0].wv.index_to_key)])\n",
    "\n",
    "    # Copy the vectors from the first model to the unified model for the initial vocabulary\n",
    "    for word in unified_model.wv.index_to_key:\n",
    "        unified_model.wv[word] = models[0].wv[word]\n",
    "\n",
    "    # Iterate through the remaining models and add their unique words and vectors\n",
    "    for model in models[1:]:\n",
    "        # Get the set of unique words in the current model's vocabulary\n",
    "        unique_words = set(model.wv.index_to_key) - set(unified_model.wv.index_to_key)\n",
    "\n",
    "        # Add the unique words to the unified model's vocabulary\n",
    "        unified_model.build_vocab([list(unique_words)], update=True)\n",
    "\n",
    "        # Copy the vectors for the unique words from the current model to the unified model\n",
    "        for word in unique_words:\n",
    "            unified_model.wv[word] = model.wv[word]\n",
    "\n",
    "    return unified_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, n):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into n sub-dataframes of approximately equal size.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame to be split\n",
    "    - n: Number of sub-dataframes to create\n",
    "    \n",
    "    Returns:\n",
    "    - A list of n sub-dataframes\n",
    "    \"\"\"\n",
    "    # Calculate the size of each chunk\n",
    "    chunk_size = len(df) // n\n",
    "    \n",
    "    # Initialize an empty list to store the sub-dataframes\n",
    "    sub_dfs = []\n",
    "    \n",
    "    # Loop through the DataFrame and create sub-dataframes\n",
    "    for i in range(n):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size if i != n - 1 else len(df)\n",
    "        sub_dfs.append(df.iloc[start_idx:end_idx])\n",
    "        \n",
    "    return sub_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_attributes(df,\"ta1-trace-e3-official-1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dfs = split_dataframe(df,3)\n",
    "client_out = [prepare_graph(x) for x in client_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_models = []\n",
    "for data in client_out:\n",
    "    phrases,labels,edges,mapp = data\n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=30, window=5, min_count=1, workers=1,epochs=300)\n",
    "    word_models.append(word2vec)\n",
    "\n",
    "global_word = combine_word2vec_models(word_models)\n",
    "global_word.save(\"../Content_FL_Exp/global_word2vec_trace.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrases,labels,edges,mapp = prepare_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3RDmGME5iPb5"
   },
   "outputs": [],
   "source": [
    "#word2vec = Word2Vec(sentences=phrases, vector_size=30, window=5, min_count=1, workers=8,epochs=300,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "p3TAi69zI1bO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the train and test function in this cell \n",
    "'''\n",
    "from sklearn.utils import class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "template = GCN(30,11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Vn_pMyt5Jd-6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding function for running word2vec inference\n",
    "'''\n",
    "from collections import Counter\n",
    "word2vec = Word2Vec.load(\"../Content_FL_Exp/global_word2vec_trace.model\")\n",
    "\n",
    "def infer(doc):\n",
    "  temp = dict(Counter(doc))\n",
    "  emb = np.zeros(30)\n",
    "  count = 0\n",
    "  for k,v in temp.items():\n",
    "    if k in word2vec.wv:\n",
    "      emb = emb + word2vec.wv[k]*v\n",
    "      count = count + 1\n",
    "  emb = emb / count\n",
    "  return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_client_gnn(client_data):\n",
    "    ################################## Training Main Model #####################################\n",
    "    model = copy.deepcopy(template)\n",
    "    if \"trace_global.pth\" in os.listdir(\"../Content_FL_Exp\"):\n",
    "        model.load_state_dict(torch.load(\"../Content_FL_Exp/trace_global.pth\"))\n",
    "        \n",
    "    phrases,labels,edges,mapp = client_data\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)  \n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        out = model(graph.x, graph.edge_index) \n",
    "        loss = criterion(out, graph.y) \n",
    "        loss.backward() \n",
    "        optimizer.step()      \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(client_models):\n",
    "    global_model = copy.deepcopy(template)\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        param_list = [client_models[i].state_dict()[k] for i in range(len(client_models))]\n",
    "        global_dict[k] = torch.stack(param_list, 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    torch.save(global_model.state_dict(), \"../Content_FL_Exp/trace_global.pth\")\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Round 0\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.40928053855896\n",
      "Epoch: 1, Loss: 2.3782448768615723\n",
      "Epoch: 2, Loss: 2.3492908477783203\n",
      "Epoch: 3, Loss: 2.3310773372650146\n",
      "Epoch: 4, Loss: 2.3202807903289795\n",
      "Epoch: 5, Loss: 2.3118338584899902\n",
      "Epoch: 6, Loss: 2.3070855140686035\n",
      "Epoch: 7, Loss: 2.302199125289917\n",
      "Epoch: 8, Loss: 2.2967441082000732\n",
      "Epoch: 9, Loss: 2.2966020107269287\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.401787757873535\n",
      "Epoch: 1, Loss: 2.3750247955322266\n",
      "Epoch: 2, Loss: 2.3530642986297607\n",
      "Epoch: 3, Loss: 2.3310062885284424\n",
      "Epoch: 4, Loss: 2.3283329010009766\n",
      "Epoch: 5, Loss: 2.314653158187866\n",
      "Epoch: 6, Loss: 2.308307647705078\n",
      "Epoch: 7, Loss: 2.3004519939422607\n",
      "Epoch: 8, Loss: 2.304187774658203\n",
      "Epoch: 9, Loss: 2.294304847717285\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.4047152996063232\n",
      "Epoch: 1, Loss: 2.370090961456299\n",
      "Epoch: 2, Loss: 2.3456203937530518\n",
      "Epoch: 3, Loss: 2.3325226306915283\n",
      "Epoch: 4, Loss: 2.323406457901001\n",
      "Epoch: 5, Loss: 2.315194845199585\n",
      "Epoch: 6, Loss: 2.3072454929351807\n",
      "Epoch: 7, Loss: 2.3035857677459717\n",
      "Epoch: 8, Loss: 2.297696828842163\n",
      "Epoch: 9, Loss: 2.294147253036499\n",
      "Learning Round 1\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.293067455291748\n",
      "Epoch: 1, Loss: 2.2899436950683594\n",
      "Epoch: 2, Loss: 2.286897659301758\n",
      "Epoch: 3, Loss: 2.285301923751831\n",
      "Epoch: 4, Loss: 2.284672260284424\n",
      "Epoch: 5, Loss: 2.283524513244629\n",
      "Epoch: 6, Loss: 2.2844696044921875\n",
      "Epoch: 7, Loss: 2.281963586807251\n",
      "Epoch: 8, Loss: 2.2816507816314697\n",
      "Epoch: 9, Loss: 2.280397415161133\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2997801303863525\n",
      "Epoch: 1, Loss: 2.2959694862365723\n",
      "Epoch: 2, Loss: 2.2873096466064453\n",
      "Epoch: 3, Loss: 2.2864410877227783\n",
      "Epoch: 4, Loss: 2.2899184226989746\n",
      "Epoch: 5, Loss: 2.2843546867370605\n",
      "Epoch: 6, Loss: 2.281660556793213\n",
      "Epoch: 7, Loss: 2.2823421955108643\n",
      "Epoch: 8, Loss: 2.2805707454681396\n",
      "Epoch: 9, Loss: 2.2817821502685547\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2934060096740723\n",
      "Epoch: 1, Loss: 2.2888522148132324\n",
      "Epoch: 2, Loss: 2.2880160808563232\n",
      "Epoch: 3, Loss: 2.2847886085510254\n",
      "Epoch: 4, Loss: 2.283583641052246\n",
      "Epoch: 5, Loss: 2.282052516937256\n",
      "Epoch: 6, Loss: 2.2807412147521973\n",
      "Epoch: 7, Loss: 2.2805464267730713\n",
      "Epoch: 8, Loss: 2.27972674369812\n",
      "Epoch: 9, Loss: 2.2793073654174805\n",
      "Learning Round 2\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2801759243011475\n",
      "Epoch: 1, Loss: 2.2800750732421875\n",
      "Epoch: 2, Loss: 2.279611349105835\n",
      "Epoch: 3, Loss: 2.2791128158569336\n",
      "Epoch: 4, Loss: 2.2794926166534424\n",
      "Epoch: 5, Loss: 2.2785654067993164\n",
      "Epoch: 6, Loss: 2.2781388759613037\n",
      "Epoch: 7, Loss: 2.2781336307525635\n",
      "Epoch: 8, Loss: 2.2776548862457275\n",
      "Epoch: 9, Loss: 2.2778050899505615\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.285752534866333\n",
      "Epoch: 1, Loss: 2.2861239910125732\n",
      "Epoch: 2, Loss: 2.283296823501587\n",
      "Epoch: 3, Loss: 2.279670000076294\n",
      "Epoch: 4, Loss: 2.2804555892944336\n",
      "Epoch: 5, Loss: 2.278696298599243\n",
      "Epoch: 6, Loss: 2.2774128913879395\n",
      "Epoch: 7, Loss: 2.2762022018432617\n",
      "Epoch: 8, Loss: 2.2763829231262207\n",
      "Epoch: 9, Loss: 2.2756540775299072\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2803797721862793\n",
      "Epoch: 1, Loss: 2.279904365539551\n",
      "Epoch: 2, Loss: 2.279362916946411\n",
      "Epoch: 3, Loss: 2.278062582015991\n",
      "Epoch: 4, Loss: 2.2779788970947266\n",
      "Epoch: 5, Loss: 2.277708053588867\n",
      "Epoch: 6, Loss: 2.277172565460205\n",
      "Epoch: 7, Loss: 2.2771172523498535\n",
      "Epoch: 8, Loss: 2.277097702026367\n",
      "Epoch: 9, Loss: 2.2768476009368896\n",
      "Learning Round 3\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.277773141860962\n",
      "Epoch: 1, Loss: 2.2782745361328125\n",
      "Epoch: 2, Loss: 2.2781989574432373\n",
      "Epoch: 3, Loss: 2.2775840759277344\n",
      "Epoch: 4, Loss: 2.2776477336883545\n",
      "Epoch: 5, Loss: 2.2773094177246094\n",
      "Epoch: 6, Loss: 2.277223587036133\n",
      "Epoch: 7, Loss: 2.2774498462677\n",
      "Epoch: 8, Loss: 2.276792049407959\n",
      "Epoch: 9, Loss: 2.2767081260681152\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2812514305114746\n",
      "Epoch: 1, Loss: 2.282533645629883\n",
      "Epoch: 2, Loss: 2.2776474952697754\n",
      "Epoch: 3, Loss: 2.278137445449829\n",
      "Epoch: 4, Loss: 2.27719783782959\n",
      "Epoch: 5, Loss: 2.2770917415618896\n",
      "Epoch: 6, Loss: 2.2756166458129883\n",
      "Epoch: 7, Loss: 2.2760326862335205\n",
      "Epoch: 8, Loss: 2.275221824645996\n",
      "Epoch: 9, Loss: 2.2747929096221924\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.277040719985962\n",
      "Epoch: 1, Loss: 2.2790207862854004\n",
      "Epoch: 2, Loss: 2.2770943641662598\n",
      "Epoch: 3, Loss: 2.277480363845825\n",
      "Epoch: 4, Loss: 2.2771072387695312\n",
      "Epoch: 5, Loss: 2.27689266204834\n",
      "Epoch: 6, Loss: 2.2767999172210693\n",
      "Epoch: 7, Loss: 2.27663516998291\n",
      "Epoch: 8, Loss: 2.27660870552063\n",
      "Epoch: 9, Loss: 2.2763760089874268\n",
      "Learning Round 4\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.276778221130371\n",
      "Epoch: 1, Loss: 2.2780377864837646\n",
      "Epoch: 2, Loss: 2.2768378257751465\n",
      "Epoch: 3, Loss: 2.2769289016723633\n",
      "Epoch: 4, Loss: 2.2769339084625244\n",
      "Epoch: 5, Loss: 2.2770261764526367\n",
      "Epoch: 6, Loss: 2.2765612602233887\n",
      "Epoch: 7, Loss: 2.2765324115753174\n",
      "Epoch: 8, Loss: 2.2766313552856445\n",
      "Epoch: 9, Loss: 2.2763400077819824\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.279092788696289\n",
      "Epoch: 1, Loss: 2.2762582302093506\n",
      "Epoch: 2, Loss: 2.2752346992492676\n",
      "Epoch: 3, Loss: 2.275084972381592\n",
      "Epoch: 4, Loss: 2.2752151489257812\n",
      "Epoch: 5, Loss: 2.275001287460327\n",
      "Epoch: 6, Loss: 2.2749507427215576\n",
      "Epoch: 7, Loss: 2.274700403213501\n",
      "Epoch: 8, Loss: 2.274168014526367\n",
      "Epoch: 9, Loss: 2.274163246154785\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.277059316635132\n",
      "Epoch: 1, Loss: 2.2785744667053223\n",
      "Epoch: 2, Loss: 2.2770559787750244\n",
      "Epoch: 3, Loss: 2.2769429683685303\n",
      "Epoch: 4, Loss: 2.2767410278320312\n",
      "Epoch: 5, Loss: 2.2766294479370117\n",
      "Epoch: 6, Loss: 2.276442289352417\n",
      "Epoch: 7, Loss: 2.2762718200683594\n",
      "Epoch: 8, Loss: 2.2761423587799072\n",
      "Epoch: 9, Loss: 2.276217460632324\n",
      "Learning Round 5\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2764194011688232\n",
      "Epoch: 1, Loss: 2.278108835220337\n",
      "Epoch: 2, Loss: 2.2771499156951904\n",
      "Epoch: 3, Loss: 2.2767059803009033\n",
      "Epoch: 4, Loss: 2.276937961578369\n",
      "Epoch: 5, Loss: 2.2766990661621094\n",
      "Epoch: 6, Loss: 2.2768032550811768\n",
      "Epoch: 7, Loss: 2.27654767036438\n",
      "Epoch: 8, Loss: 2.2765731811523438\n",
      "Epoch: 9, Loss: 2.276336431503296\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.274245262145996\n",
      "Epoch: 1, Loss: 2.2747080326080322\n",
      "Epoch: 2, Loss: 2.274799108505249\n",
      "Epoch: 3, Loss: 2.27493953704834\n",
      "Epoch: 4, Loss: 2.2745707035064697\n",
      "Epoch: 5, Loss: 2.274261236190796\n",
      "Epoch: 6, Loss: 2.273982048034668\n",
      "Epoch: 7, Loss: 2.274099111557007\n",
      "Epoch: 8, Loss: 2.274109125137329\n",
      "Epoch: 9, Loss: 2.274010419845581\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2764511108398438\n",
      "Epoch: 1, Loss: 2.2791101932525635\n",
      "Epoch: 2, Loss: 2.276967763900757\n",
      "Epoch: 3, Loss: 2.276726722717285\n",
      "Epoch: 4, Loss: 2.276643991470337\n",
      "Epoch: 5, Loss: 2.2765166759490967\n",
      "Epoch: 6, Loss: 2.2763755321502686\n",
      "Epoch: 7, Loss: 2.276153564453125\n",
      "Epoch: 8, Loss: 2.2760965824127197\n",
      "Epoch: 9, Loss: 2.276017189025879\n",
      "Learning Round 6\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.276301145553589\n",
      "Epoch: 1, Loss: 2.27901029586792\n",
      "Epoch: 2, Loss: 2.277130365371704\n",
      "Epoch: 3, Loss: 2.276979923248291\n",
      "Epoch: 4, Loss: 2.2777891159057617\n",
      "Epoch: 5, Loss: 2.277050733566284\n",
      "Epoch: 6, Loss: 2.2768359184265137\n",
      "Epoch: 7, Loss: 2.2766056060791016\n",
      "Epoch: 8, Loss: 2.276594638824463\n",
      "Epoch: 9, Loss: 2.276520252227783\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2737438678741455\n",
      "Epoch: 1, Loss: 2.278855562210083\n",
      "Epoch: 2, Loss: 2.2753124237060547\n",
      "Epoch: 3, Loss: 2.2753968238830566\n",
      "Epoch: 4, Loss: 2.2748939990997314\n",
      "Epoch: 5, Loss: 2.2742552757263184\n",
      "Epoch: 6, Loss: 2.2742156982421875\n",
      "Epoch: 7, Loss: 2.2742648124694824\n",
      "Epoch: 8, Loss: 2.2742772102355957\n",
      "Epoch: 9, Loss: 2.274085521697998\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2762985229492188\n",
      "Epoch: 1, Loss: 2.2795794010162354\n",
      "Epoch: 2, Loss: 2.277125835418701\n",
      "Epoch: 3, Loss: 2.2773032188415527\n",
      "Epoch: 4, Loss: 2.2772719860076904\n",
      "Epoch: 5, Loss: 2.2768874168395996\n",
      "Epoch: 6, Loss: 2.276503562927246\n",
      "Epoch: 7, Loss: 2.2764692306518555\n",
      "Epoch: 8, Loss: 2.2763803005218506\n",
      "Epoch: 9, Loss: 2.276378631591797\n",
      "Learning Round 7\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2764124870300293\n",
      "Epoch: 1, Loss: 2.278353214263916\n",
      "Epoch: 2, Loss: 2.2768917083740234\n",
      "Epoch: 3, Loss: 2.2767155170440674\n",
      "Epoch: 4, Loss: 2.276981830596924\n",
      "Epoch: 5, Loss: 2.276456117630005\n",
      "Epoch: 6, Loss: 2.276350736618042\n",
      "Epoch: 7, Loss: 2.276428699493408\n",
      "Epoch: 8, Loss: 2.2762067317962646\n",
      "Epoch: 9, Loss: 2.276177167892456\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273885726928711\n",
      "Epoch: 1, Loss: 2.2762749195098877\n",
      "Epoch: 2, Loss: 2.2752254009246826\n",
      "Epoch: 3, Loss: 2.2742340564727783\n",
      "Epoch: 4, Loss: 2.274876594543457\n",
      "Epoch: 5, Loss: 2.274407386779785\n",
      "Epoch: 6, Loss: 2.2739415168762207\n",
      "Epoch: 7, Loss: 2.273974657058716\n",
      "Epoch: 8, Loss: 2.274094343185425\n",
      "Epoch: 9, Loss: 2.2746198177337646\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.276395797729492\n",
      "Epoch: 1, Loss: 2.2791385650634766\n",
      "Epoch: 2, Loss: 2.276905059814453\n",
      "Epoch: 3, Loss: 2.276564121246338\n",
      "Epoch: 4, Loss: 2.276754140853882\n",
      "Epoch: 5, Loss: 2.2764108180999756\n",
      "Epoch: 6, Loss: 2.276263952255249\n",
      "Epoch: 7, Loss: 2.275991916656494\n",
      "Epoch: 8, Loss: 2.275770425796509\n",
      "Epoch: 9, Loss: 2.2756638526916504\n",
      "Learning Round 8\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2761363983154297\n",
      "Epoch: 1, Loss: 2.28007173538208\n",
      "Epoch: 2, Loss: 2.2770094871520996\n",
      "Epoch: 3, Loss: 2.2774102687835693\n",
      "Epoch: 4, Loss: 2.2770650386810303\n",
      "Epoch: 5, Loss: 2.276822328567505\n",
      "Epoch: 6, Loss: 2.2765021324157715\n",
      "Epoch: 7, Loss: 2.2763216495513916\n",
      "Epoch: 8, Loss: 2.2763049602508545\n",
      "Epoch: 9, Loss: 2.2762234210968018\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2740275859832764\n",
      "Epoch: 1, Loss: 2.276543140411377\n",
      "Epoch: 2, Loss: 2.276639938354492\n",
      "Epoch: 3, Loss: 2.2747368812561035\n",
      "Epoch: 4, Loss: 2.2756457328796387\n",
      "Epoch: 5, Loss: 2.2741973400115967\n",
      "Epoch: 6, Loss: 2.2741026878356934\n",
      "Epoch: 7, Loss: 2.2739357948303223\n",
      "Epoch: 8, Loss: 2.2741401195526123\n",
      "Epoch: 9, Loss: 2.275200366973877\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2763214111328125\n",
      "Epoch: 1, Loss: 2.2799582481384277\n",
      "Epoch: 2, Loss: 2.277634382247925\n",
      "Epoch: 3, Loss: 2.276594638824463\n",
      "Epoch: 4, Loss: 2.2764503955841064\n",
      "Epoch: 5, Loss: 2.2762653827667236\n",
      "Epoch: 6, Loss: 2.2758309841156006\n",
      "Epoch: 7, Loss: 2.2756738662719727\n",
      "Epoch: 8, Loss: 2.275606155395508\n",
      "Epoch: 9, Loss: 2.2755000591278076\n",
      "Learning Round 9\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2761597633361816\n",
      "Epoch: 1, Loss: 2.278740882873535\n",
      "Epoch: 2, Loss: 2.276632785797119\n",
      "Epoch: 3, Loss: 2.2765696048736572\n",
      "Epoch: 4, Loss: 2.2763590812683105\n",
      "Epoch: 5, Loss: 2.2761425971984863\n",
      "Epoch: 6, Loss: 2.276149272918701\n",
      "Epoch: 7, Loss: 2.2758898735046387\n",
      "Epoch: 8, Loss: 2.2757797241210938\n",
      "Epoch: 9, Loss: 2.275865316390991\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2738378047943115\n",
      "Epoch: 1, Loss: 2.2774603366851807\n",
      "Epoch: 2, Loss: 2.274061441421509\n",
      "Epoch: 3, Loss: 2.273796319961548\n",
      "Epoch: 4, Loss: 2.2748281955718994\n",
      "Epoch: 5, Loss: 2.2738876342773438\n",
      "Epoch: 6, Loss: 2.2737269401550293\n",
      "Epoch: 7, Loss: 2.274258613586426\n",
      "Epoch: 8, Loss: 2.273890972137451\n",
      "Epoch: 9, Loss: 2.2736997604370117\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2759716510772705\n",
      "Epoch: 1, Loss: 2.279895782470703\n",
      "Epoch: 2, Loss: 2.2765140533447266\n",
      "Epoch: 3, Loss: 2.276226758956909\n",
      "Epoch: 4, Loss: 2.276130437850952\n",
      "Epoch: 5, Loss: 2.2758865356445312\n",
      "Epoch: 6, Loss: 2.2758078575134277\n",
      "Epoch: 7, Loss: 2.2756454944610596\n",
      "Epoch: 8, Loss: 2.275649070739746\n",
      "Epoch: 9, Loss: 2.2755818367004395\n",
      "Learning Round 10\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2759151458740234\n",
      "Epoch: 1, Loss: 2.280762195587158\n",
      "Epoch: 2, Loss: 2.276503562927246\n",
      "Epoch: 3, Loss: 2.2767930030822754\n",
      "Epoch: 4, Loss: 2.2770543098449707\n",
      "Epoch: 5, Loss: 2.27665114402771\n",
      "Epoch: 6, Loss: 2.276712656021118\n",
      "Epoch: 7, Loss: 2.2765302658081055\n",
      "Epoch: 8, Loss: 2.2766776084899902\n",
      "Epoch: 9, Loss: 2.2764618396759033\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273052453994751\n",
      "Epoch: 1, Loss: 2.276273488998413\n",
      "Epoch: 2, Loss: 2.2756881713867188\n",
      "Epoch: 3, Loss: 2.274287462234497\n",
      "Epoch: 4, Loss: 2.273784875869751\n",
      "Epoch: 5, Loss: 2.273986577987671\n",
      "Epoch: 6, Loss: 2.274280309677124\n",
      "Epoch: 7, Loss: 2.2742388248443604\n",
      "Epoch: 8, Loss: 2.273930311203003\n",
      "Epoch: 9, Loss: 2.273709535598755\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2758071422576904\n",
      "Epoch: 1, Loss: 2.2806320190429688\n",
      "Epoch: 2, Loss: 2.2761800289154053\n",
      "Epoch: 3, Loss: 2.276534080505371\n",
      "Epoch: 4, Loss: 2.276153087615967\n",
      "Epoch: 5, Loss: 2.2760603427886963\n",
      "Epoch: 6, Loss: 2.2760400772094727\n",
      "Epoch: 7, Loss: 2.2758541107177734\n",
      "Epoch: 8, Loss: 2.2758219242095947\n",
      "Epoch: 9, Loss: 2.2754952907562256\n",
      "Learning Round 11\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2760448455810547\n",
      "Epoch: 1, Loss: 2.2793219089508057\n",
      "Epoch: 2, Loss: 2.27659010887146\n",
      "Epoch: 3, Loss: 2.276521921157837\n",
      "Epoch: 4, Loss: 2.2764394283294678\n",
      "Epoch: 5, Loss: 2.2762928009033203\n",
      "Epoch: 6, Loss: 2.276102304458618\n",
      "Epoch: 7, Loss: 2.276014804840088\n",
      "Epoch: 8, Loss: 2.275843858718872\n",
      "Epoch: 9, Loss: 2.275723695755005\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2732973098754883\n",
      "Epoch: 1, Loss: 2.2778537273406982\n",
      "Epoch: 2, Loss: 2.273987054824829\n",
      "Epoch: 3, Loss: 2.274144172668457\n",
      "Epoch: 4, Loss: 2.2738749980926514\n",
      "Epoch: 5, Loss: 2.2739064693450928\n",
      "Epoch: 6, Loss: 2.273991584777832\n",
      "Epoch: 7, Loss: 2.2740375995635986\n",
      "Epoch: 8, Loss: 2.273878812789917\n",
      "Epoch: 9, Loss: 2.273815393447876\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.275815725326538\n",
      "Epoch: 1, Loss: 2.280333995819092\n",
      "Epoch: 2, Loss: 2.2759454250335693\n",
      "Epoch: 3, Loss: 2.2759363651275635\n",
      "Epoch: 4, Loss: 2.276015281677246\n",
      "Epoch: 5, Loss: 2.275839328765869\n",
      "Epoch: 6, Loss: 2.2757680416107178\n",
      "Epoch: 7, Loss: 2.2757227420806885\n",
      "Epoch: 8, Loss: 2.275515556335449\n",
      "Epoch: 9, Loss: 2.2753283977508545\n",
      "Learning Round 12\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2760493755340576\n",
      "Epoch: 1, Loss: 2.2793378829956055\n",
      "Epoch: 2, Loss: 2.2771012783050537\n",
      "Epoch: 3, Loss: 2.276667833328247\n",
      "Epoch: 4, Loss: 2.2763187885284424\n",
      "Epoch: 5, Loss: 2.2760980129241943\n",
      "Epoch: 6, Loss: 2.276149272918701\n",
      "Epoch: 7, Loss: 2.2758290767669678\n",
      "Epoch: 8, Loss: 2.2760298252105713\n",
      "Epoch: 9, Loss: 2.275731086730957\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273629665374756\n",
      "Epoch: 1, Loss: 2.278005838394165\n",
      "Epoch: 2, Loss: 2.2743680477142334\n",
      "Epoch: 3, Loss: 2.27390456199646\n",
      "Epoch: 4, Loss: 2.2738335132598877\n",
      "Epoch: 5, Loss: 2.273955821990967\n",
      "Epoch: 6, Loss: 2.2734081745147705\n",
      "Epoch: 7, Loss: 2.2741281986236572\n",
      "Epoch: 8, Loss: 2.273698568344116\n",
      "Epoch: 9, Loss: 2.274036169052124\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2756130695343018\n",
      "Epoch: 1, Loss: 2.2802493572235107\n",
      "Epoch: 2, Loss: 2.2767179012298584\n",
      "Epoch: 3, Loss: 2.276329278945923\n",
      "Epoch: 4, Loss: 2.276151657104492\n",
      "Epoch: 5, Loss: 2.2760603427886963\n",
      "Epoch: 6, Loss: 2.275912284851074\n",
      "Epoch: 7, Loss: 2.27578067779541\n",
      "Epoch: 8, Loss: 2.275604248046875\n",
      "Epoch: 9, Loss: 2.2755465507507324\n",
      "Learning Round 13\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.276010513305664\n",
      "Epoch: 1, Loss: 2.2783076763153076\n",
      "Epoch: 2, Loss: 2.276533842086792\n",
      "Epoch: 3, Loss: 2.2764737606048584\n",
      "Epoch: 4, Loss: 2.276308536529541\n",
      "Epoch: 5, Loss: 2.2762229442596436\n",
      "Epoch: 6, Loss: 2.2759714126586914\n",
      "Epoch: 7, Loss: 2.2759976387023926\n",
      "Epoch: 8, Loss: 2.275852680206299\n",
      "Epoch: 9, Loss: 2.2757010459899902\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2736597061157227\n",
      "Epoch: 1, Loss: 2.2760531902313232\n",
      "Epoch: 2, Loss: 2.273858070373535\n",
      "Epoch: 3, Loss: 2.274569034576416\n",
      "Epoch: 4, Loss: 2.2743594646453857\n",
      "Epoch: 5, Loss: 2.274082660675049\n",
      "Epoch: 6, Loss: 2.27412486076355\n",
      "Epoch: 7, Loss: 2.2740769386291504\n",
      "Epoch: 8, Loss: 2.273986577987671\n",
      "Epoch: 9, Loss: 2.2737081050872803\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2756080627441406\n",
      "Epoch: 1, Loss: 2.2790305614471436\n",
      "Epoch: 2, Loss: 2.2762553691864014\n",
      "Epoch: 3, Loss: 2.2761361598968506\n",
      "Epoch: 4, Loss: 2.2762844562530518\n",
      "Epoch: 5, Loss: 2.2759158611297607\n",
      "Epoch: 6, Loss: 2.2756729125976562\n",
      "Epoch: 7, Loss: 2.2754297256469727\n",
      "Epoch: 8, Loss: 2.2755749225616455\n",
      "Epoch: 9, Loss: 2.2753942012786865\n",
      "Learning Round 14\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2758517265319824\n",
      "Epoch: 1, Loss: 2.278949499130249\n",
      "Epoch: 2, Loss: 2.2773241996765137\n",
      "Epoch: 3, Loss: 2.276993751525879\n",
      "Epoch: 4, Loss: 2.2763922214508057\n",
      "Epoch: 5, Loss: 2.2763993740081787\n",
      "Epoch: 6, Loss: 2.276359796524048\n",
      "Epoch: 7, Loss: 2.2760956287384033\n",
      "Epoch: 8, Loss: 2.2760496139526367\n",
      "Epoch: 9, Loss: 2.276090145111084\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273176670074463\n",
      "Epoch: 1, Loss: 2.2766551971435547\n",
      "Epoch: 2, Loss: 2.2755444049835205\n",
      "Epoch: 3, Loss: 2.275468349456787\n",
      "Epoch: 4, Loss: 2.274606704711914\n",
      "Epoch: 5, Loss: 2.2737700939178467\n",
      "Epoch: 6, Loss: 2.273890495300293\n",
      "Epoch: 7, Loss: 2.2738776206970215\n",
      "Epoch: 8, Loss: 2.2740039825439453\n",
      "Epoch: 9, Loss: 2.273850202560425\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2754244804382324\n",
      "Epoch: 1, Loss: 2.2814903259277344\n",
      "Epoch: 2, Loss: 2.2774851322174072\n",
      "Epoch: 3, Loss: 2.276578664779663\n",
      "Epoch: 4, Loss: 2.276459217071533\n",
      "Epoch: 5, Loss: 2.2761409282684326\n",
      "Epoch: 6, Loss: 2.2760889530181885\n",
      "Epoch: 7, Loss: 2.2759127616882324\n",
      "Epoch: 8, Loss: 2.2757740020751953\n",
      "Epoch: 9, Loss: 2.275660753250122\n",
      "Learning Round 15\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.276158094406128\n",
      "Epoch: 1, Loss: 2.277484178543091\n",
      "Epoch: 2, Loss: 2.2760963439941406\n",
      "Epoch: 3, Loss: 2.2763607501983643\n",
      "Epoch: 4, Loss: 2.2761573791503906\n",
      "Epoch: 5, Loss: 2.2758190631866455\n",
      "Epoch: 6, Loss: 2.2757229804992676\n",
      "Epoch: 7, Loss: 2.2756855487823486\n",
      "Epoch: 8, Loss: 2.2754666805267334\n",
      "Epoch: 9, Loss: 2.2754271030426025\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273346185684204\n",
      "Epoch: 1, Loss: 2.2747695446014404\n",
      "Epoch: 2, Loss: 2.2735865116119385\n",
      "Epoch: 3, Loss: 2.2744081020355225\n",
      "Epoch: 4, Loss: 2.274606466293335\n",
      "Epoch: 5, Loss: 2.2742395401000977\n",
      "Epoch: 6, Loss: 2.2737457752227783\n",
      "Epoch: 7, Loss: 2.2737467288970947\n",
      "Epoch: 8, Loss: 2.2739980220794678\n",
      "Epoch: 9, Loss: 2.273338794708252\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.275782823562622\n",
      "Epoch: 1, Loss: 2.278076648712158\n",
      "Epoch: 2, Loss: 2.275921106338501\n",
      "Epoch: 3, Loss: 2.275702953338623\n",
      "Epoch: 4, Loss: 2.2756600379943848\n",
      "Epoch: 5, Loss: 2.2754404544830322\n",
      "Epoch: 6, Loss: 2.275242805480957\n",
      "Epoch: 7, Loss: 2.275066614151001\n",
      "Epoch: 8, Loss: 2.2750189304351807\n",
      "Epoch: 9, Loss: 2.2749409675598145\n",
      "Learning Round 16\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2757108211517334\n",
      "Epoch: 1, Loss: 2.2816286087036133\n",
      "Epoch: 2, Loss: 2.2773940563201904\n",
      "Epoch: 3, Loss: 2.2771286964416504\n",
      "Epoch: 4, Loss: 2.276416301727295\n",
      "Epoch: 5, Loss: 2.2769174575805664\n",
      "Epoch: 6, Loss: 2.276553153991699\n",
      "Epoch: 7, Loss: 2.276418685913086\n",
      "Epoch: 8, Loss: 2.2768845558166504\n",
      "Epoch: 9, Loss: 2.276369571685791\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273343563079834\n",
      "Epoch: 1, Loss: 2.275914192199707\n",
      "Epoch: 2, Loss: 2.277326822280884\n",
      "Epoch: 3, Loss: 2.2747011184692383\n",
      "Epoch: 4, Loss: 2.2742884159088135\n",
      "Epoch: 5, Loss: 2.2744734287261963\n",
      "Epoch: 6, Loss: 2.2744603157043457\n",
      "Epoch: 7, Loss: 2.2742645740509033\n",
      "Epoch: 8, Loss: 2.2737927436828613\n",
      "Epoch: 9, Loss: 2.273852825164795\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.2752232551574707\n",
      "Epoch: 1, Loss: 2.2817699909210205\n",
      "Epoch: 2, Loss: 2.2768912315368652\n",
      "Epoch: 3, Loss: 2.2761335372924805\n",
      "Epoch: 4, Loss: 2.2761454582214355\n",
      "Epoch: 5, Loss: 2.2759926319122314\n",
      "Epoch: 6, Loss: 2.2759833335876465\n",
      "Epoch: 7, Loss: 2.2759463787078857\n",
      "Epoch: 8, Loss: 2.275724411010742\n",
      "Epoch: 9, Loss: 2.275737762451172\n",
      "Learning Round 17\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.276055335998535\n",
      "Epoch: 1, Loss: 2.2782912254333496\n",
      "Epoch: 2, Loss: 2.277026653289795\n",
      "Epoch: 3, Loss: 2.276711940765381\n",
      "Epoch: 4, Loss: 2.2762062549591064\n",
      "Epoch: 5, Loss: 2.276275873184204\n",
      "Epoch: 6, Loss: 2.276041269302368\n",
      "Epoch: 7, Loss: 2.2760720252990723\n",
      "Epoch: 8, Loss: 2.275904893875122\n",
      "Epoch: 9, Loss: 2.275886297225952\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.274158239364624\n",
      "Epoch: 1, Loss: 2.275285482406616\n",
      "Epoch: 2, Loss: 2.2750308513641357\n",
      "Epoch: 3, Loss: 2.2749414443969727\n",
      "Epoch: 4, Loss: 2.2745373249053955\n",
      "Epoch: 5, Loss: 2.274261474609375\n",
      "Epoch: 6, Loss: 2.2738468647003174\n",
      "Epoch: 7, Loss: 2.2735397815704346\n",
      "Epoch: 8, Loss: 2.2739133834838867\n",
      "Epoch: 9, Loss: 2.274113893508911\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.276002883911133\n",
      "Epoch: 1, Loss: 2.2782087326049805\n",
      "Epoch: 2, Loss: 2.2762694358825684\n",
      "Epoch: 3, Loss: 2.275951862335205\n",
      "Epoch: 4, Loss: 2.2757086753845215\n",
      "Epoch: 5, Loss: 2.2755491733551025\n",
      "Epoch: 6, Loss: 2.2755234241485596\n",
      "Epoch: 7, Loss: 2.2753450870513916\n",
      "Epoch: 8, Loss: 2.27524995803833\n",
      "Epoch: 9, Loss: 2.2750980854034424\n",
      "Learning Round 18\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.2757599353790283\n",
      "Epoch: 1, Loss: 2.2787702083587646\n",
      "Epoch: 2, Loss: 2.277139186859131\n",
      "Epoch: 3, Loss: 2.2761917114257812\n",
      "Epoch: 4, Loss: 2.2762563228607178\n",
      "Epoch: 5, Loss: 2.2762467861175537\n",
      "Epoch: 6, Loss: 2.276139736175537\n",
      "Epoch: 7, Loss: 2.2761738300323486\n",
      "Epoch: 8, Loss: 2.275970458984375\n",
      "Epoch: 9, Loss: 2.2758467197418213\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.2734313011169434\n",
      "Epoch: 1, Loss: 2.2778873443603516\n",
      "Epoch: 2, Loss: 2.2743782997131348\n",
      "Epoch: 3, Loss: 2.2738213539123535\n",
      "Epoch: 4, Loss: 2.273681640625\n",
      "Epoch: 5, Loss: 2.2737162113189697\n",
      "Epoch: 6, Loss: 2.2752249240875244\n",
      "Epoch: 7, Loss: 2.273782968521118\n",
      "Epoch: 8, Loss: 2.2740612030029297\n",
      "Epoch: 9, Loss: 2.274226427078247\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.27536678314209\n",
      "Epoch: 1, Loss: 2.2805824279785156\n",
      "Epoch: 2, Loss: 2.2783362865448\n",
      "Epoch: 3, Loss: 2.276383876800537\n",
      "Epoch: 4, Loss: 2.276931047439575\n",
      "Epoch: 5, Loss: 2.2763779163360596\n",
      "Epoch: 6, Loss: 2.27632999420166\n",
      "Epoch: 7, Loss: 2.276093006134033\n",
      "Epoch: 8, Loss: 2.2758545875549316\n",
      "Epoch: 9, Loss: 2.275679349899292\n",
      "Learning Round 19\n",
      "Training Client Model 0\n",
      "Epoch: 0, Loss: 2.27598237991333\n",
      "Epoch: 1, Loss: 2.2773587703704834\n",
      "Epoch: 2, Loss: 2.276500940322876\n",
      "Epoch: 3, Loss: 2.277211904525757\n",
      "Epoch: 4, Loss: 2.2759976387023926\n",
      "Epoch: 5, Loss: 2.275907039642334\n",
      "Epoch: 6, Loss: 2.275973320007324\n",
      "Epoch: 7, Loss: 2.275862455368042\n",
      "Epoch: 8, Loss: 2.27561616897583\n",
      "Epoch: 9, Loss: 2.2758097648620605\n",
      "Training Client Model 1\n",
      "Epoch: 0, Loss: 2.273960828781128\n",
      "Epoch: 1, Loss: 2.2798328399658203\n",
      "Epoch: 2, Loss: 2.2750203609466553\n",
      "Epoch: 3, Loss: 2.2744617462158203\n",
      "Epoch: 4, Loss: 2.2750065326690674\n",
      "Epoch: 5, Loss: 2.274329662322998\n",
      "Epoch: 6, Loss: 2.2736847400665283\n",
      "Epoch: 7, Loss: 2.2736053466796875\n",
      "Epoch: 8, Loss: 2.2740607261657715\n",
      "Epoch: 9, Loss: 2.2737832069396973\n",
      "Training Client Model 2\n",
      "Epoch: 0, Loss: 2.275634288787842\n",
      "Epoch: 1, Loss: 2.280092239379883\n",
      "Epoch: 2, Loss: 2.2768242359161377\n",
      "Epoch: 3, Loss: 2.2762880325317383\n",
      "Epoch: 4, Loss: 2.2764697074890137\n",
      "Epoch: 5, Loss: 2.276261568069458\n",
      "Epoch: 6, Loss: 2.2759573459625244\n",
      "Epoch: 7, Loss: 2.2758116722106934\n",
      "Epoch: 8, Loss: 2.2755959033966064\n",
      "Epoch: 9, Loss: 2.2755274772644043\n"
     ]
    }
   ],
   "source": [
    "learning_rounds = 10\n",
    "\n",
    "for r in range(learning_rounds):\n",
    "    print(f\"Learning Round {r}\")\n",
    "   \n",
    "    client_models = []\n",
    "    for i in range(len(client_out)):\n",
    "        print(f\"Training Client Model {i}\")\n",
    "        cmodel = train_client_gnn(client_out[i])\n",
    "        client_models.append(cmodel)\n",
    "    \n",
    "    global_model = server_aggregate(client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the trained GNN model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PKJ53Fh5ogvy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used for constructing neighborhood around a given \n",
    "set of nodes for backwards or forward tracking\n",
    "'''\n",
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def construct_neighborhood(ids,mapp,edges,hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    else:\n",
    "        neighbors = set()\n",
    "        for i in range(len(edges[0])):\n",
    "            if mapp[edges[0][i]] in ids:\n",
    "                neighbors.add(mapp[edges[1][i]])\n",
    "            if mapp[edges[1][i]] in ids:\n",
    "                neighbors.add(mapp[edges[0][i]])\n",
    "        return neighbors.union( construct_neighborhood(neighbors,mapp,edges,hops-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vgqyu7E5qPet"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function logs the evaluation metrics.\n",
    "'''\n",
    "\n",
    "def helper(MP,all_pids,GP,edges,mapp):\n",
    "\n",
    "    GN = all_pids - GP\n",
    "    MN = all_pids - MP\n",
    "\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP.intersection(GN)\n",
    "    FN = MN.intersection(GP)\n",
    "    TN = MN.intersection(GN)\n",
    "    \n",
    "    two_hop_gp = construct_neighborhood(GP,mapp,edges,2)\n",
    "    two_hop_tp = construct_neighborhood(TP,mapp,edges,2)\n",
    "    FPL = FP - two_hop_gp\n",
    "    TPL = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "    \n",
    "    alerts = TP.union(FP)\n",
    "\n",
    "    TP,FP,FN,TN = len(TPL),len(FPL),len(FN),len(TN)\n",
    "    \n",
    "    FPR = FP / (FP+TN)\n",
    "    TPR = TP / (TP+FN)\n",
    "\n",
    "    print(f\"Number of True Positives: {TP}\")\n",
    "    print(f\"Number of Fasle Positives: {FP}\")\n",
    "    print(f\"Number of False Negatives: {FN}\")\n",
    "    print(f\"Number of True Negatives: {TN}\\n\")\n",
    "\n",
    "    prec = TP / (TP + FP)\n",
    "    print(f\"Precision: {prec}\")\n",
    "\n",
    "    rec = TP / (TP + FN)\n",
    "    print(f\"Recall: {rec}\")\n",
    "\n",
    "    fscore = (2*prec*rec) / (prec + rec)\n",
    "    print(f\"Fscore: {fscore}\\n\")\n",
    "    \n",
    "    #return alerts\n",
    "    return TPL,FPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OZFrSLVZ29qU"
   },
   "outputs": [],
   "source": [
    "f = open(\"darpatc/trace_test.txt\")\n",
    "data = f.read().split('\\n')\n",
    "data = [line.split('\\t') for line in data]\n",
    "df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "df = df.dropna()\n",
    "df.sort_values(by='timestamp', ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_attributes(df,\"ta1-trace-e3-official-1.json.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = open(\"trace.txt\").read()\n",
    "GT_mal = set(gt.split(\"\\n\"))\n",
    "\n",
    "data = df\n",
    "\n",
    "phrases,labels,edges,mapp = prepare_graph(data)\n",
    "\n",
    "nodes = [infer(x) for x in phrases]\n",
    "nodes = np.array(nodes)  \n",
    "\n",
    "all_ids = list(data['actorID']) + list(data['objectID'])\n",
    "all_ids = set(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(30,11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311255,
     "status": "ok",
     "timestamp": 1673577553815,
     "user": {
      "displayName": "Mati Ur Rehman",
      "userId": "04281203290774044297"
     },
     "user_tz": 300
    },
    "id": "DsLlVS6zpox5",
    "outputId": "e79153e2-2e54-4cbf-c241-1293e0ee2be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives: 67383\n",
      "Number of Fasle Positives: 40014\n",
      "Number of False Negatives: 0\n",
      "Number of True Negatives: 1040723\n",
      "\n",
      "Precision: 0.6274197603285008\n",
      "Recall: 1.0\n",
      "Fscore: 0.7710607621009269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../Content_FL_Exp/trace_global.pth'))\n",
    "model.eval()\n",
    "out = model(graph.x, graph.edge_index)\n",
    "\n",
    "sorted, indices = out.sort(dim=1,descending=True)\n",
    "conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "pred = indices[:,0]\n",
    "cond = (pred == graph.y)# & (conf >= 0.9)\n",
    "flag[cond] = torch.logical_and(flag[cond], torch.tensor([False]*len(flag[cond]), dtype=torch.bool))\n",
    "\n",
    "index = utils.mask_to_index(flag).tolist()\n",
    "ids = set([mapp[x] for x in index])\n",
    "TPL,FPL = helper(set(ids),set(all_ids),GT_mal,edges,mapp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
