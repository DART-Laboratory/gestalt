{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F1op-CbyLuN4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the require libraries here\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import orjson as json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import multiprocessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be/Notebooks_FL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and setting up working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be/content'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "Setting working directory for rivana jupyter notebook\n",
    "'''\n",
    "os.chdir(\"../content\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nM7KaeCbA_mQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing some additional libraries\n",
    "'''\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for loading, cleaning and constructing features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BfjmrhfUr3pK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the main featurizer. It constructs the graph for the cadets dataset.\n",
    "\n",
    "Args:\n",
    "    df (DataFrame): This is the main dataframe containing all the system events from the cadets dataset.\n",
    "\n",
    "return:\n",
    "    features (list): Contains word2vec encoded feature vectors for each node\n",
    "    feat_labels (list): Contains label for each node\n",
    "    edge_index (list): Contains information about edges between nodes in the graph.\n",
    "    mapp (list): contains id of each node\n",
    "'''\n",
    "\n",
    "def prepare_graph(df):\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    edges = []\n",
    "        \n",
    "    dummies = {\n",
    "                 'FILE_OBJECT_BLOCK': 0,\n",
    "                 'FILE_OBJECT_CHAR': 1,\n",
    "                 'FILE_OBJECT_DIR': 2,\n",
    "                 'FILE_OBJECT_FILE': 3,\n",
    "                 'FILE_OBJECT_LINK': 4,\n",
    "                 'FILE_OBJECT_NAMED_PIPE': 5,\n",
    "                 'FILE_OBJECT_PEFILE': 6,\n",
    "                 'FILE_OBJECT_UNIX_SOCKET': 7,\n",
    "                 'MemoryObject': 8,\n",
    "                 'NetFlowObject': 9,\n",
    "                 'PRINCIPAL_LOCAL': 10,\n",
    "                 'PRINCIPAL_REMOTE': 11,\n",
    "                 'SRCSINK_DATABASE': 12,\n",
    "                 'SRCSINK_PROCESS_MANAGEMENT': 13,\n",
    "                 'SRCSINK_UNKNOWN': 14,\n",
    "                 'SUBJECT_PROCESS': 15,\n",
    "                 'SUBJECT_THREAD': 16,\n",
    "                 'SUBJECT_UNIT': 17,\n",
    "                 'UnnamedPipeObject': 18,\n",
    "                 'VALUE_TYPE_SRC': 19\n",
    "                }\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x = df.iloc[i]\n",
    "        action = x[\"action\"]\n",
    "        \n",
    "        actorid = x[\"actorID\"]\n",
    "        if not (actorid in nodes):\n",
    "            nodes[actorid] =  []\n",
    "        nodes[actorid].append(x['exec'])\n",
    "        nodes[actorid].append(action)\n",
    "        if x['path'] != '':\n",
    "            nodes[actorid].append(x['path'])\n",
    "        labels[actorid] = dummies[x['actor_type']]\n",
    "\n",
    "        objectid = x[\"objectID\"]\n",
    "        if not (objectid in nodes):\n",
    "            nodes[objectid] =  []\n",
    "        nodes[objectid].append(x['exec'])\n",
    "        nodes[objectid].append(action)\n",
    "        if x['path'] != '':\n",
    "             nodes[objectid].append(x['path'])\n",
    "        labels[objectid] = dummies[x['object']]\n",
    "\n",
    "        edges.append(( actorid, objectid ))\n",
    "\n",
    "    features = []\n",
    "    feat_labels = []\n",
    "    edge_index = [[],[]]\n",
    "    index  = {}\n",
    "    mapp = []\n",
    "\n",
    "    for k,v in nodes.items():\n",
    "      features.append(v)\n",
    "      feat_labels.append(labels[k])\n",
    "      index[k] = len(features) - 1\n",
    "      mapp.append(k)\n",
    "\n",
    "    for x in edges:\n",
    "        src = index[x[0]]\n",
    "        dst = index[x[1]]\n",
    "\n",
    "        edge_index[0].append(src)\n",
    "        edge_index[1].append(dst)\n",
    "\n",
    "    return features,feat_labels,edge_index,mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fmXWs1dKIzD8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the model. The model consists of two sageconv layers from the paper GraphSage\n",
    "'''\n",
    "#from torch_geometric.nn import SAGEConv, PDNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channel, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, out_channel, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YBuP_tSq94f4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function helps visualize the output of the model.\n",
    "'''\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding semantic attributes from the raw cadets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used for attributing semnatic information like process names, executable paths,\n",
    "file paths etc using the raw cadets data\n",
    "'''\n",
    "\n",
    "def add_attributes(d,p):\n",
    "    \n",
    "    f = open(p)\n",
    "    data = [json.loads(x) for x in f if \"EVENT\" in x]\n",
    "\n",
    "    info = []\n",
    "    for x in data:\n",
    "        try:\n",
    "            action = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['type']\n",
    "        except:\n",
    "            action = ''\n",
    "        try:\n",
    "            actor = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['subject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            actor = ''\n",
    "        try:\n",
    "            obj = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            obj = ''\n",
    "        try:\n",
    "            timestamp = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['timestampNanos']\n",
    "        except:\n",
    "            timestamp = ''\n",
    "        try:\n",
    "            cmd = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['properties']['map']['exec']\n",
    "        except:\n",
    "            cmd = ''\n",
    "        try:\n",
    "            path = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObjectPath']['string']\n",
    "        except:\n",
    "            path = ''\n",
    "        try:\n",
    "            path2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2Path']['string']\n",
    "        except:\n",
    "            path2 = ''\n",
    "        try:\n",
    "            obj2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "            info.append({'actorID':actor,'objectID':obj2,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path2})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        info.append({'actorID':actor,'objectID':obj,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path})\n",
    "\n",
    "    rdf = pd.DataFrame.from_records(info).astype(str)\n",
    "    d = d.astype(str)\n",
    "\n",
    "    return d.merge(rdf,how='inner',on=['actorID','objectID','action','timestamp']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_word2vec_models(models):\n",
    "    # Create an empty unified model\n",
    "    unified_model = Word2Vec(vector_size=models[0].vector_size, window=models[0].window, min_count=models[0].min_count, sg=models[0].sg)\n",
    "\n",
    "    # Initialize the vocabulary with the words from the first model\n",
    "    unified_model.build_vocab([list(models[0].wv.index_to_key)])\n",
    "\n",
    "    # Copy the vectors from the first model to the unified model for the initial vocabulary\n",
    "    for word in unified_model.wv.index_to_key:\n",
    "        unified_model.wv[word] = models[0].wv[word]\n",
    "\n",
    "    # Iterate through the remaining models and add their unique words and vectors\n",
    "    for model in models[1:]:\n",
    "        # Get the set of unique words in the current model's vocabulary\n",
    "        unique_words = set(model.wv.index_to_key) - set(unified_model.wv.index_to_key)\n",
    "\n",
    "        # Add the unique words to the unified model's vocabulary\n",
    "        unified_model.build_vocab([list(unique_words)], update=True)\n",
    "\n",
    "        # Copy the vectors for the unique words from the current model to the unified model\n",
    "        for word in unique_words:\n",
    "            unified_model.wv[word] = model.wv[word]\n",
    "\n",
    "    return unified_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_models = []\n",
    "#for m in ['cadets','theia','trace','five']:\n",
    "#    word2vec = Word2Vec.load(f\"word2vec_{m}_E3.model\")\n",
    "#    word_models.append(word2vec)\n",
    "\n",
    "#global_word = combine_word2vec_models(word_models)\n",
    "#global_word.save(\"../Content_FL_Exp/global_word2vec_E3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrases,labels,edges,mapp = prepare_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3RDmGME5iPb5"
   },
   "outputs": [],
   "source": [
    "#word2vec = Word2Vec(sentences=phrases, vector_size=30, window=5, min_count=1, workers=8,epochs=300,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "p3TAi69zI1bO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the train and test function in this cell \n",
    "'''\n",
    "from sklearn.utils import class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "template = GCN(30,20).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Vn_pMyt5Jd-6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding function for running word2vec inference\n",
    "'''\n",
    "from collections import Counter\n",
    "word2vec = Word2Vec.load(\"../Content_FL_Exp/global_word2vec_E3.model\")\n",
    "\n",
    "def infer(doc):\n",
    "  temp = dict(Counter(doc))\n",
    "  emb = np.zeros(30)\n",
    "  count = 0\n",
    "  for k,v in temp.items():\n",
    "    if k in word2vec.wv:\n",
    "      emb = emb + word2vec.wv[k]*v\n",
    "      count = count + 1\n",
    "  emb = emb / count\n",
    "  return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_client_gnn(data_name):\n",
    "    ################################## Training Main Model #####################################\n",
    "    model = copy.deepcopy(template)\n",
    "    if \"e3_global.pth\" in os.listdir(\"../Content_FL_Exp\"):\n",
    "        model.load_state_dict(torch.load(\"../Content_FL_Exp/e3_global.pth\"))\n",
    "        \n",
    "    with open(f\"../Content_FL_Exp/{data_name}_cached_train.json\", \"r\") as json_file:\n",
    "        client_data = json.load(json_file)\n",
    "        \n",
    "    phrases,labels,edges,mapp = client_data\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)  \n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        out = model(graph.x, graph.edge_index) \n",
    "        loss = criterion(out, graph.y) \n",
    "        loss.backward() \n",
    "        optimizer.step()      \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(client_models):\n",
    "    global_model = copy.deepcopy(template)\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        param_list = [client_models[i].state_dict()[k] for i in range(len(client_models))]\n",
    "        global_dict[k] = torch.stack(param_list, 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    torch.save(global_model.state_dict(), \"../Content_FL_Exp/e3_global.pth\")\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(name):\n",
    "    \n",
    "    train_file = None\n",
    "    attribute_file = None\n",
    "    \n",
    "    if name == 'cadets':\n",
    "        train_file = 'darpatc/cadets_train.txt'\n",
    "        attribute_file = \"ta1-cadets-e3-official.json.1\"\n",
    "        \n",
    "    if name == 'theia':\n",
    "        train_file = \"darpatc/theia_train.txt\"\n",
    "        attribute_file = \"ta1-theia-e3-official-1r.json\"\n",
    "    \n",
    "    if name == 'fivedirections':\n",
    "        train_file = \"darpatc/fivedirections_train.txt\"\n",
    "        attribute_file = \"ta1-fivedirections-e3-official-2.json\"\n",
    "        \n",
    "    if name == 'trace':\n",
    "        train_file = \"darpatc/trace_train.txt\"\n",
    "        attribute_file = \"ta1-trace-e3-official-1.json\"  \n",
    "        \n",
    "    f = open(train_file)\n",
    "    data = f.read().split('\\n')\n",
    "    data = [line.split('\\t') for line in data]\n",
    "    df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "    df = df.dropna()\n",
    "    df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "    \n",
    "    df = add_attributes(df,attribute_file)\n",
    "    \n",
    "    out_data = prepare_graph(df)\n",
    "    \n",
    "    file_path = f\"../Content_FL_Exp/{name}_cached_train.json\"\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(out_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in ['cadets','theia','fivedirections','trace']:\n",
    "#    print(f'Processing {x}')\n",
    "#    save_processed_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Round 0\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.997450351715088\n",
      "Epoch: 1, Loss: 2.97074556350708\n",
      "Epoch: 2, Loss: 2.9803810119628906\n",
      "Epoch: 3, Loss: 2.9546849727630615\n",
      "Epoch: 4, Loss: 2.9504528045654297\n",
      "Epoch: 5, Loss: 2.9424033164978027\n",
      "Epoch: 6, Loss: 2.9457931518554688\n",
      "Epoch: 7, Loss: 2.9458630084991455\n",
      "Epoch: 8, Loss: 2.952678680419922\n",
      "Epoch: 9, Loss: 2.9443485736846924\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9973065853118896\n",
      "Epoch: 1, Loss: 2.982213020324707\n",
      "Epoch: 2, Loss: 2.9687869548797607\n",
      "Epoch: 3, Loss: 2.9582056999206543\n",
      "Epoch: 4, Loss: 2.951622247695923\n",
      "Epoch: 5, Loss: 2.947148561477661\n",
      "Epoch: 6, Loss: 2.944131851196289\n",
      "Epoch: 7, Loss: 2.9420642852783203\n",
      "Epoch: 8, Loss: 2.9401135444641113\n",
      "Epoch: 9, Loss: 2.9382224082946777\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 3.0068767070770264\n",
      "Epoch: 1, Loss: 2.998281240463257\n",
      "Epoch: 2, Loss: 2.9892704486846924\n",
      "Epoch: 3, Loss: 2.981553792953491\n",
      "Epoch: 4, Loss: 2.9752674102783203\n",
      "Epoch: 5, Loss: 2.969343662261963\n",
      "Epoch: 6, Loss: 2.9635367393493652\n",
      "Epoch: 7, Loss: 2.9567081928253174\n",
      "Epoch: 8, Loss: 2.950075626373291\n",
      "Epoch: 9, Loss: 2.944742202758789\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.9989969730377197\n",
      "Epoch: 1, Loss: 2.9827070236206055\n",
      "Epoch: 2, Loss: 2.9718542098999023\n",
      "Epoch: 3, Loss: 2.961879014968872\n",
      "Epoch: 4, Loss: 2.957127571105957\n",
      "Epoch: 5, Loss: 2.950817584991455\n",
      "Epoch: 6, Loss: 2.94608998298645\n",
      "Epoch: 7, Loss: 2.94354248046875\n",
      "Epoch: 8, Loss: 2.9408161640167236\n",
      "Epoch: 9, Loss: 2.939918279647827\n",
      "Learning Round 1\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.9679014682769775\n",
      "Epoch: 1, Loss: 2.9531939029693604\n",
      "Epoch: 2, Loss: 2.9463462829589844\n",
      "Epoch: 3, Loss: 2.9455173015594482\n",
      "Epoch: 4, Loss: 2.9477357864379883\n",
      "Epoch: 5, Loss: 2.94028377532959\n",
      "Epoch: 6, Loss: 2.9353442192077637\n",
      "Epoch: 7, Loss: 2.933650016784668\n",
      "Epoch: 8, Loss: 2.931302309036255\n",
      "Epoch: 9, Loss: 2.933223009109497\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9567928314208984\n",
      "Epoch: 1, Loss: 2.9447245597839355\n",
      "Epoch: 2, Loss: 2.940023899078369\n",
      "Epoch: 3, Loss: 2.9368786811828613\n",
      "Epoch: 4, Loss: 2.934947967529297\n",
      "Epoch: 5, Loss: 2.933527946472168\n",
      "Epoch: 6, Loss: 2.9327950477600098\n",
      "Epoch: 7, Loss: 2.9320993423461914\n",
      "Epoch: 8, Loss: 2.9315483570098877\n",
      "Epoch: 9, Loss: 2.9310381412506104\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.979267120361328\n",
      "Epoch: 1, Loss: 2.9630699157714844\n",
      "Epoch: 2, Loss: 2.952000856399536\n",
      "Epoch: 3, Loss: 2.945387363433838\n",
      "Epoch: 4, Loss: 2.941176176071167\n",
      "Epoch: 5, Loss: 2.9390296936035156\n",
      "Epoch: 6, Loss: 2.9373626708984375\n",
      "Epoch: 7, Loss: 2.935211420059204\n",
      "Epoch: 8, Loss: 2.933690309524536\n",
      "Epoch: 9, Loss: 2.932781934738159\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.9739391803741455\n",
      "Epoch: 1, Loss: 2.9576642513275146\n",
      "Epoch: 2, Loss: 2.9508750438690186\n",
      "Epoch: 3, Loss: 2.944157838821411\n",
      "Epoch: 4, Loss: 2.94008731842041\n",
      "Epoch: 5, Loss: 2.9364027976989746\n",
      "Epoch: 6, Loss: 2.9355309009552\n",
      "Epoch: 7, Loss: 2.933683395385742\n",
      "Epoch: 8, Loss: 2.9310495853424072\n",
      "Epoch: 9, Loss: 2.931046724319458\n",
      "Learning Round 2\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.9485137462615967\n",
      "Epoch: 1, Loss: 2.9414491653442383\n",
      "Epoch: 2, Loss: 2.9348936080932617\n",
      "Epoch: 3, Loss: 2.933544158935547\n",
      "Epoch: 4, Loss: 2.9324872493743896\n",
      "Epoch: 5, Loss: 2.937040090560913\n",
      "Epoch: 6, Loss: 2.932199478149414\n",
      "Epoch: 7, Loss: 2.9277548789978027\n",
      "Epoch: 8, Loss: 2.928346633911133\n",
      "Epoch: 9, Loss: 2.9272849559783936\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.938166379928589\n",
      "Epoch: 1, Loss: 2.9346628189086914\n",
      "Epoch: 2, Loss: 2.933088779449463\n",
      "Epoch: 3, Loss: 2.9318199157714844\n",
      "Epoch: 4, Loss: 2.931165933609009\n",
      "Epoch: 5, Loss: 2.930518865585327\n",
      "Epoch: 6, Loss: 2.9298479557037354\n",
      "Epoch: 7, Loss: 2.929327964782715\n",
      "Epoch: 8, Loss: 2.9288909435272217\n",
      "Epoch: 9, Loss: 2.9285171031951904\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.9619228839874268\n",
      "Epoch: 1, Loss: 2.945359945297241\n",
      "Epoch: 2, Loss: 2.9379518032073975\n",
      "Epoch: 3, Loss: 2.936885118484497\n",
      "Epoch: 4, Loss: 2.93554949760437\n",
      "Epoch: 5, Loss: 2.9338295459747314\n",
      "Epoch: 6, Loss: 2.93216609954834\n",
      "Epoch: 7, Loss: 2.9312222003936768\n",
      "Epoch: 8, Loss: 2.9308218955993652\n",
      "Epoch: 9, Loss: 2.9302475452423096\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.9630370140075684\n",
      "Epoch: 1, Loss: 2.9445858001708984\n",
      "Epoch: 2, Loss: 2.937098503112793\n",
      "Epoch: 3, Loss: 2.935274124145508\n",
      "Epoch: 4, Loss: 2.9340295791625977\n",
      "Epoch: 5, Loss: 2.931920051574707\n",
      "Epoch: 6, Loss: 2.9297502040863037\n",
      "Epoch: 7, Loss: 2.9292800426483154\n",
      "Epoch: 8, Loss: 2.928629159927368\n",
      "Epoch: 9, Loss: 2.9274637699127197\n",
      "Learning Round 3\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.9411017894744873\n",
      "Epoch: 1, Loss: 2.935426950454712\n",
      "Epoch: 2, Loss: 2.9324984550476074\n",
      "Epoch: 3, Loss: 2.931713819503784\n",
      "Epoch: 4, Loss: 2.9293243885040283\n",
      "Epoch: 5, Loss: 2.9275991916656494\n",
      "Epoch: 6, Loss: 2.9273173809051514\n",
      "Epoch: 7, Loss: 2.927867889404297\n",
      "Epoch: 8, Loss: 2.9261128902435303\n",
      "Epoch: 9, Loss: 2.925384998321533\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9327938556671143\n",
      "Epoch: 1, Loss: 2.931589126586914\n",
      "Epoch: 2, Loss: 2.929941177368164\n",
      "Epoch: 3, Loss: 2.9283552169799805\n",
      "Epoch: 4, Loss: 2.9279067516326904\n",
      "Epoch: 5, Loss: 2.9274344444274902\n",
      "Epoch: 6, Loss: 2.926945447921753\n",
      "Epoch: 7, Loss: 2.9264302253723145\n",
      "Epoch: 8, Loss: 2.9262301921844482\n",
      "Epoch: 9, Loss: 2.925821304321289\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.9531545639038086\n",
      "Epoch: 1, Loss: 2.939725399017334\n",
      "Epoch: 2, Loss: 2.935506820678711\n",
      "Epoch: 3, Loss: 2.9342832565307617\n",
      "Epoch: 4, Loss: 2.933232307434082\n",
      "Epoch: 5, Loss: 2.9314494132995605\n",
      "Epoch: 6, Loss: 2.930582046508789\n",
      "Epoch: 7, Loss: 2.929875373840332\n",
      "Epoch: 8, Loss: 2.930077075958252\n",
      "Epoch: 9, Loss: 2.929814338684082\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.9562153816223145\n",
      "Epoch: 1, Loss: 2.944415807723999\n",
      "Epoch: 2, Loss: 2.9361870288848877\n",
      "Epoch: 3, Loss: 2.934703826904297\n",
      "Epoch: 4, Loss: 2.9320006370544434\n",
      "Epoch: 5, Loss: 2.9301254749298096\n",
      "Epoch: 6, Loss: 2.9278948307037354\n",
      "Epoch: 7, Loss: 2.92712140083313\n",
      "Epoch: 8, Loss: 2.9265098571777344\n",
      "Epoch: 9, Loss: 2.925856113433838\n",
      "Learning Round 4\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.9362668991088867\n",
      "Epoch: 1, Loss: 2.9341094493865967\n",
      "Epoch: 2, Loss: 2.9279885292053223\n",
      "Epoch: 3, Loss: 2.928098678588867\n",
      "Epoch: 4, Loss: 2.9282796382904053\n",
      "Epoch: 5, Loss: 2.9263916015625\n",
      "Epoch: 6, Loss: 2.925992012023926\n",
      "Epoch: 7, Loss: 2.926663875579834\n",
      "Epoch: 8, Loss: 2.92692232131958\n",
      "Epoch: 9, Loss: 2.924593210220337\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9310128688812256\n",
      "Epoch: 1, Loss: 2.929955005645752\n",
      "Epoch: 2, Loss: 2.927990198135376\n",
      "Epoch: 3, Loss: 2.9271912574768066\n",
      "Epoch: 4, Loss: 2.927044153213501\n",
      "Epoch: 5, Loss: 2.9256434440612793\n",
      "Epoch: 6, Loss: 2.9253971576690674\n",
      "Epoch: 7, Loss: 2.9251041412353516\n",
      "Epoch: 8, Loss: 2.9250004291534424\n",
      "Epoch: 9, Loss: 2.9248900413513184\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.956573486328125\n",
      "Epoch: 1, Loss: 2.9439351558685303\n",
      "Epoch: 2, Loss: 2.935781240463257\n",
      "Epoch: 3, Loss: 2.9358131885528564\n",
      "Epoch: 4, Loss: 2.9351091384887695\n",
      "Epoch: 5, Loss: 2.9322149753570557\n",
      "Epoch: 6, Loss: 2.9301211833953857\n",
      "Epoch: 7, Loss: 2.9296863079071045\n",
      "Epoch: 8, Loss: 2.9299168586730957\n",
      "Epoch: 9, Loss: 2.930140256881714\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.9604525566101074\n",
      "Epoch: 1, Loss: 2.942335367202759\n",
      "Epoch: 2, Loss: 2.938950300216675\n",
      "Epoch: 3, Loss: 2.936685800552368\n",
      "Epoch: 4, Loss: 2.932239532470703\n",
      "Epoch: 5, Loss: 2.9292478561401367\n",
      "Epoch: 6, Loss: 2.9277596473693848\n",
      "Epoch: 7, Loss: 2.926600933074951\n",
      "Epoch: 8, Loss: 2.9260385036468506\n",
      "Epoch: 9, Loss: 2.9261128902435303\n",
      "Learning Round 5\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.932321310043335\n",
      "Epoch: 1, Loss: 2.933210611343384\n",
      "Epoch: 2, Loss: 2.927030563354492\n",
      "Epoch: 3, Loss: 2.9272751808166504\n",
      "Epoch: 4, Loss: 2.927225351333618\n",
      "Epoch: 5, Loss: 2.9256391525268555\n",
      "Epoch: 6, Loss: 2.924983501434326\n",
      "Epoch: 7, Loss: 2.9260189533233643\n",
      "Epoch: 8, Loss: 2.923996925354004\n",
      "Epoch: 9, Loss: 2.924576759338379\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.929987907409668\n",
      "Epoch: 1, Loss: 2.9296274185180664\n",
      "Epoch: 2, Loss: 2.927241802215576\n",
      "Epoch: 3, Loss: 2.9262919425964355\n",
      "Epoch: 4, Loss: 2.925765037536621\n",
      "Epoch: 5, Loss: 2.925199031829834\n",
      "Epoch: 6, Loss: 2.9249048233032227\n",
      "Epoch: 7, Loss: 2.924835681915283\n",
      "Epoch: 8, Loss: 2.924624443054199\n",
      "Epoch: 9, Loss: 2.9245572090148926\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.9594013690948486\n",
      "Epoch: 1, Loss: 2.9423699378967285\n",
      "Epoch: 2, Loss: 2.9359853267669678\n",
      "Epoch: 3, Loss: 2.935380458831787\n",
      "Epoch: 4, Loss: 2.9341299533843994\n",
      "Epoch: 5, Loss: 2.9324991703033447\n",
      "Epoch: 6, Loss: 2.930771827697754\n",
      "Epoch: 7, Loss: 2.9297823905944824\n",
      "Epoch: 8, Loss: 2.9294373989105225\n",
      "Epoch: 9, Loss: 2.929734706878662\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.960667371749878\n",
      "Epoch: 1, Loss: 2.947988748550415\n",
      "Epoch: 2, Loss: 2.9341866970062256\n",
      "Epoch: 3, Loss: 2.9355108737945557\n",
      "Epoch: 4, Loss: 2.9353182315826416\n",
      "Epoch: 5, Loss: 2.930964231491089\n",
      "Epoch: 6, Loss: 2.929374933242798\n",
      "Epoch: 7, Loss: 2.927182912826538\n",
      "Epoch: 8, Loss: 2.9270262718200684\n",
      "Epoch: 9, Loss: 2.9267213344573975\n",
      "Learning Round 6\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.9293875694274902\n",
      "Epoch: 1, Loss: 2.9302597045898438\n",
      "Epoch: 2, Loss: 2.925795316696167\n",
      "Epoch: 3, Loss: 2.925663471221924\n",
      "Epoch: 4, Loss: 2.925001382827759\n",
      "Epoch: 5, Loss: 2.9258110523223877\n",
      "Epoch: 6, Loss: 2.923999309539795\n",
      "Epoch: 7, Loss: 2.9232213497161865\n",
      "Epoch: 8, Loss: 2.9231865406036377\n",
      "Epoch: 9, Loss: 2.923396587371826\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9289896488189697\n",
      "Epoch: 1, Loss: 2.9290807247161865\n",
      "Epoch: 2, Loss: 2.926084041595459\n",
      "Epoch: 3, Loss: 2.9258525371551514\n",
      "Epoch: 4, Loss: 2.92561411857605\n",
      "Epoch: 5, Loss: 2.9250922203063965\n",
      "Epoch: 6, Loss: 2.924712896347046\n",
      "Epoch: 7, Loss: 2.924605131149292\n",
      "Epoch: 8, Loss: 2.924484968185425\n",
      "Epoch: 9, Loss: 2.9244046211242676\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.961459159851074\n",
      "Epoch: 1, Loss: 2.9457833766937256\n",
      "Epoch: 2, Loss: 2.936455726623535\n",
      "Epoch: 3, Loss: 2.9357614517211914\n",
      "Epoch: 4, Loss: 2.9351718425750732\n",
      "Epoch: 5, Loss: 2.9325358867645264\n",
      "Epoch: 6, Loss: 2.930300235748291\n",
      "Epoch: 7, Loss: 2.9295098781585693\n",
      "Epoch: 8, Loss: 2.9295239448547363\n",
      "Epoch: 9, Loss: 2.9297924041748047\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.964379072189331\n",
      "Epoch: 1, Loss: 2.9478683471679688\n",
      "Epoch: 2, Loss: 2.938596725463867\n",
      "Epoch: 3, Loss: 2.9352569580078125\n",
      "Epoch: 4, Loss: 2.9336936473846436\n",
      "Epoch: 5, Loss: 2.9328155517578125\n",
      "Epoch: 6, Loss: 2.927351713180542\n",
      "Epoch: 7, Loss: 2.9270198345184326\n",
      "Epoch: 8, Loss: 2.9266602993011475\n",
      "Epoch: 9, Loss: 2.9269838333129883\n",
      "Learning Round 7\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.9305338859558105\n",
      "Epoch: 1, Loss: 2.9291937351226807\n",
      "Epoch: 2, Loss: 2.9270968437194824\n",
      "Epoch: 3, Loss: 2.925253391265869\n",
      "Epoch: 4, Loss: 2.9253101348876953\n",
      "Epoch: 5, Loss: 2.9249727725982666\n",
      "Epoch: 6, Loss: 2.9236385822296143\n",
      "Epoch: 7, Loss: 2.923875331878662\n",
      "Epoch: 8, Loss: 2.9232141971588135\n",
      "Epoch: 9, Loss: 2.9231345653533936\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9290995597839355\n",
      "Epoch: 1, Loss: 2.9282305240631104\n",
      "Epoch: 2, Loss: 2.9255359172821045\n",
      "Epoch: 3, Loss: 2.925154685974121\n",
      "Epoch: 4, Loss: 2.925307035446167\n",
      "Epoch: 5, Loss: 2.9248311519622803\n",
      "Epoch: 6, Loss: 2.9244916439056396\n",
      "Epoch: 7, Loss: 2.9243946075439453\n",
      "Epoch: 8, Loss: 2.924365997314453\n",
      "Epoch: 9, Loss: 2.9241833686828613\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.962282657623291\n",
      "Epoch: 1, Loss: 2.9466300010681152\n",
      "Epoch: 2, Loss: 2.935943841934204\n",
      "Epoch: 3, Loss: 2.9352526664733887\n",
      "Epoch: 4, Loss: 2.934811592102051\n",
      "Epoch: 5, Loss: 2.932966709136963\n",
      "Epoch: 6, Loss: 2.9308576583862305\n",
      "Epoch: 7, Loss: 2.929894208908081\n",
      "Epoch: 8, Loss: 2.9291465282440186\n",
      "Epoch: 9, Loss: 2.9292593002319336\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.9670889377593994\n",
      "Epoch: 1, Loss: 2.9504075050354004\n",
      "Epoch: 2, Loss: 2.9369845390319824\n",
      "Epoch: 3, Loss: 2.934776782989502\n",
      "Epoch: 4, Loss: 2.9321556091308594\n",
      "Epoch: 5, Loss: 2.9297332763671875\n",
      "Epoch: 6, Loss: 2.928208351135254\n",
      "Epoch: 7, Loss: 2.926022529602051\n",
      "Epoch: 8, Loss: 2.926215887069702\n",
      "Epoch: 9, Loss: 2.925889492034912\n",
      "Learning Round 8\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.931051015853882\n",
      "Epoch: 1, Loss: 2.927096128463745\n",
      "Epoch: 2, Loss: 2.925562858581543\n",
      "Epoch: 3, Loss: 2.926107406616211\n",
      "Epoch: 4, Loss: 2.92527437210083\n",
      "Epoch: 5, Loss: 2.924243450164795\n",
      "Epoch: 6, Loss: 2.9233927726745605\n",
      "Epoch: 7, Loss: 2.923081398010254\n",
      "Epoch: 8, Loss: 2.9232118129730225\n",
      "Epoch: 9, Loss: 2.9229800701141357\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.928831100463867\n",
      "Epoch: 1, Loss: 2.9283993244171143\n",
      "Epoch: 2, Loss: 2.925710678100586\n",
      "Epoch: 3, Loss: 2.925524950027466\n",
      "Epoch: 4, Loss: 2.9250802993774414\n",
      "Epoch: 5, Loss: 2.9246060848236084\n",
      "Epoch: 6, Loss: 2.924478769302368\n",
      "Epoch: 7, Loss: 2.9243597984313965\n",
      "Epoch: 8, Loss: 2.9243032932281494\n",
      "Epoch: 9, Loss: 2.9240224361419678\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.9612820148468018\n",
      "Epoch: 1, Loss: 2.9476025104522705\n",
      "Epoch: 2, Loss: 2.9366872310638428\n",
      "Epoch: 3, Loss: 2.935072422027588\n",
      "Epoch: 4, Loss: 2.934823751449585\n",
      "Epoch: 5, Loss: 2.9328160285949707\n",
      "Epoch: 6, Loss: 2.930478096008301\n",
      "Epoch: 7, Loss: 2.929063081741333\n",
      "Epoch: 8, Loss: 2.9286839962005615\n",
      "Epoch: 9, Loss: 2.9288485050201416\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.965294122695923\n",
      "Epoch: 1, Loss: 2.9483180046081543\n",
      "Epoch: 2, Loss: 2.9353232383728027\n",
      "Epoch: 3, Loss: 2.93314790725708\n",
      "Epoch: 4, Loss: 2.933244228363037\n",
      "Epoch: 5, Loss: 2.9297051429748535\n",
      "Epoch: 6, Loss: 2.9277334213256836\n",
      "Epoch: 7, Loss: 2.926908016204834\n",
      "Epoch: 8, Loss: 2.9260001182556152\n",
      "Epoch: 9, Loss: 2.9254202842712402\n",
      "Learning Round 9\n",
      "Training Client Model: cadets\n",
      "Epoch: 0, Loss: 2.928849458694458\n",
      "Epoch: 1, Loss: 2.928839921951294\n",
      "Epoch: 2, Loss: 2.9256269931793213\n",
      "Epoch: 3, Loss: 2.9246435165405273\n",
      "Epoch: 4, Loss: 2.925391912460327\n",
      "Epoch: 5, Loss: 2.923952102661133\n",
      "Epoch: 6, Loss: 2.92307186126709\n",
      "Epoch: 7, Loss: 2.922781467437744\n",
      "Epoch: 8, Loss: 2.9231834411621094\n",
      "Epoch: 9, Loss: 2.9226925373077393\n",
      "Training Client Model: theia\n",
      "Epoch: 0, Loss: 2.9286646842956543\n",
      "Epoch: 1, Loss: 2.928601026535034\n",
      "Epoch: 2, Loss: 2.9254727363586426\n",
      "Epoch: 3, Loss: 2.924983024597168\n",
      "Epoch: 4, Loss: 2.9249179363250732\n",
      "Epoch: 5, Loss: 2.924732208251953\n",
      "Epoch: 6, Loss: 2.924532413482666\n",
      "Epoch: 7, Loss: 2.924262523651123\n",
      "Epoch: 8, Loss: 2.9241597652435303\n",
      "Epoch: 9, Loss: 2.924159049987793\n",
      "Training Client Model: fivedirections\n",
      "Epoch: 0, Loss: 2.9631471633911133\n",
      "Epoch: 1, Loss: 2.9471161365509033\n",
      "Epoch: 2, Loss: 2.936502456665039\n",
      "Epoch: 3, Loss: 2.9351377487182617\n",
      "Epoch: 4, Loss: 2.9344980716705322\n",
      "Epoch: 5, Loss: 2.931994915008545\n",
      "Epoch: 6, Loss: 2.929471254348755\n",
      "Epoch: 7, Loss: 2.9288828372955322\n",
      "Epoch: 8, Loss: 2.9294722080230713\n",
      "Epoch: 9, Loss: 2.929280996322632\n",
      "Training Client Model: trace\n",
      "Epoch: 0, Loss: 2.965596914291382\n",
      "Epoch: 1, Loss: 2.949570417404175\n",
      "Epoch: 2, Loss: 2.9352617263793945\n",
      "Epoch: 3, Loss: 2.9328296184539795\n",
      "Epoch: 4, Loss: 2.9307029247283936\n",
      "Epoch: 5, Loss: 2.929943084716797\n",
      "Epoch: 6, Loss: 2.927694797515869\n",
      "Epoch: 7, Loss: 2.9256513118743896\n",
      "Epoch: 8, Loss: 2.9251818656921387\n",
      "Epoch: 9, Loss: 2.9263908863067627\n"
     ]
    }
   ],
   "source": [
    "learning_rounds = 10\n",
    "\n",
    "for r in range(learning_rounds):\n",
    "    print(f\"Learning Round {r}\")\n",
    "   \n",
    "    client_models = []\n",
    "    for x in ['cadets','theia','fivedirections','trace']:\n",
    "        print(f\"Training Client Model: {x}\")\n",
    "        cmodel = train_client_gnn(x)\n",
    "        client_models.append(cmodel)\n",
    "    \n",
    "    global_model = server_aggregate(client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the trained GNN model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PKJ53Fh5ogvy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used for constructing neighborhood around a given \n",
    "set of nodes for backwards or forward tracking\n",
    "'''\n",
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def construct_neighborhood(ids,mapp,edges,hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    else:\n",
    "        neighbors = set()\n",
    "        for i in range(len(edges[0])):\n",
    "            if mapp[edges[0][i]] in ids:\n",
    "                neighbors.add(mapp[edges[1][i]])\n",
    "            if mapp[edges[1][i]] in ids:\n",
    "                neighbors.add(mapp[edges[0][i]])\n",
    "        return neighbors.union( construct_neighborhood(neighbors,mapp,edges,hops-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vgqyu7E5qPet"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function logs the evaluation metrics.\n",
    "'''\n",
    "\n",
    "def helper(MP,all_pids,GP,edges,mapp):\n",
    "\n",
    "    GN = all_pids - GP\n",
    "    MN = all_pids - MP\n",
    "\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP.intersection(GN)\n",
    "    FN = MN.intersection(GP)\n",
    "    TN = MN.intersection(GN)\n",
    "    \n",
    "    two_hop_gp = construct_neighborhood(GP,mapp,edges,2)\n",
    "    two_hop_tp = construct_neighborhood(TP,mapp,edges,2)\n",
    "    FPL = FP - two_hop_gp\n",
    "    TPL = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "    \n",
    "    alerts = TP.union(FP)\n",
    "\n",
    "    TP,FP,FN,TN = len(TPL),len(FPL),len(FN),len(TN)\n",
    "    \n",
    "    FPR = FP / (FP+TN)\n",
    "    TPR = TP / (TP+FN)\n",
    "\n",
    "    print(f\"Number of True Positives: {TP}\")\n",
    "    print(f\"Number of Fasle Positives: {FP}\")\n",
    "    print(f\"Number of False Negatives: {FN}\")\n",
    "    print(f\"Number of True Negatives: {TN}\\n\")\n",
    "\n",
    "    prec = TP / (TP + FP)\n",
    "    print(f\"Precision: {prec}\")\n",
    "\n",
    "    rec = TP / (TP + FN)\n",
    "    print(f\"Recall: {rec}\")\n",
    "\n",
    "    fscore = (2*prec*rec) / (prec + rec)\n",
    "    print(f\"Fscore: {fscore}\\n\")\n",
    "    \n",
    "    #return alerts\n",
    "    return TPL,FPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data_test(name):\n",
    "    \n",
    "    test_file = None\n",
    "    attribute_file = None\n",
    "    \n",
    "    if name == 'cadets':\n",
    "        test_file = 'darpatc/cadets_test.txt'\n",
    "        attribute_file = \"ta1-cadets-e3-official-2.json\"\n",
    "        \n",
    "    if name == 'theia':\n",
    "        test_file = \"darpatc/theia_test.txt\"\n",
    "        attribute_file = \"ta1-theia-e3-official-6r.json.8\"\n",
    "    \n",
    "    if name == 'fivedirections':\n",
    "        test_file = \"darpatc/fivedirections_test.txt\"\n",
    "        attribute_file = \"ta1-fivedirections-e3-official-2.json.23\"\n",
    "        \n",
    "    if name == 'trace':\n",
    "        test_file = \"darpatc/trace_test.txt\"\n",
    "        attribute_file = \"ta1-trace-e3-official-1.json.4\"\n",
    "        \n",
    "    f = open(test_file)\n",
    "    data = f.read().split('\\n')\n",
    "    data = [line.split('\\t') for line in data]\n",
    "    df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "    df = df.dropna()\n",
    "    df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "    \n",
    "    df = add_attributes(df,attribute_file)\n",
    "    \n",
    "    out_data = prepare_graph(df)\n",
    "    \n",
    "    file_path = f\"../Content_FL_Exp/{name}_cached_test.json\"\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(out_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in ['cadets','theia','fivedirections','trace']:\n",
    "#    print(f'Processing {x}')\n",
    "#    save_processed_data_test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(data_name,is_fl):    \n",
    "    global word2vec\n",
    "    \n",
    "    with open(f\"../Content_FL_Exp/{data_name}_cached_test.json\", \"r\") as json_file:\n",
    "        client_data = json.load(json_file)\n",
    "        \n",
    "    gt = open(f\"{data_name}.txt\").read()\n",
    "    GT_mal = set(gt.split(\"\\n\"))\n",
    "\n",
    "    phrases,labels,edges,mapp = client_data\n",
    "    \n",
    "    if is_fl:\n",
    "        model = GCN(30,20).to(device)\n",
    "        word2vec = Word2Vec.load(\"../Content_FL_Exp/global_word2vec_E3.model\")\n",
    "        model.load_state_dict(torch.load(f'../Content_FL_Exp/e3_global.pth'))\n",
    "    else:\n",
    "        if data_name == 'fivedirections':\n",
    "            model = GCN(30,13).to(device)\n",
    "            word2vec = Word2Vec.load(\"word2vec_five_E3.model\")\n",
    "            model.load_state_dict(torch.load(f'word2vec_gnn_five13_E3.pth'))\n",
    "            \n",
    "        if data_name == 'cadets':\n",
    "            model = GCN(30,6).to(device)\n",
    "            word2vec = Word2Vec.load(\"word2vec_cadets_E3.model\")\n",
    "            model.load_state_dict(torch.load(f'word2vec_gnn_cadets0_E3.pth'))\n",
    "            \n",
    "        if data_name == 'theia':\n",
    "            model = GCN(30,5).to(device)\n",
    "            word2vec = Word2Vec.load(\"word2vec_theia_E3.model\")\n",
    "            model.load_state_dict(torch.load(f'word2vec_gnn_theia0_E3.pth'))\n",
    "            \n",
    "        if data_name == 'trace':\n",
    "            model = GCN(30,11).to(device)\n",
    "            word2vec = Word2Vec.load(\"word2vec_trace_E3.model\")\n",
    "            model.load_state_dict(torch.load(f'word2vec_gnn_trace0_E3.pth'))\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)  \n",
    "\n",
    "    all_ids = set(mapp)\n",
    "        \n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    model.eval()\n",
    "    out = model(graph.x, graph.edge_index)\n",
    "\n",
    "    sorted, indices = out.sort(dim=1,descending=True)\n",
    "    conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "    conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "    pred = indices[:,0]\n",
    "    cond = (pred == graph.y)# & (conf >= 0.9)\n",
    "    flag[cond] = torch.logical_and(flag[cond], torch.tensor([False]*len(flag[cond]), dtype=torch.bool))\n",
    "\n",
    "    index = utils.mask_to_index(flag).tolist()\n",
    "    ids = set([mapp[x] for x in index])\n",
    "    TPL,FPL = helper(set(ids),set(all_ids),GT_mal,edges,mapp)\n",
    "    mapp_to_labels = {x:y for x,y in zip(mapp,labels)}\n",
    "    return TPL,FPL,mapp_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fl = []\n",
    "for x in ['cadets','trace','fivedirections','theia']:\n",
    "    print(f\"Running Evaluation For {x}\")\n",
    "    temp = run_evaluation(x,True)\n",
    "    data_fl.append(temp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
