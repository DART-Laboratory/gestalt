{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import multiprocessing\n",
    "import random\n",
    "import xxhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.manual_seed(42)\\nnp.random.seed(42)\\nrandom.seed(42)\\nif torch.cuda.is_available():\\n    torch.cuda.manual_seed(42)\\n    torch.cuda.manual_seed_all(42)\\ntorch.backends.cudnn.deterministic = True\\ntorch.backends.cudnn.benchmark = False\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding():\n",
    "\n",
    "    def __init__(self, d_model ,max_len = 100000):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        self.pe[:,0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "encoder = PositionalEncoding(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def infer(doc,word2vec):  \n",
    "    word_emb = []\n",
    "    for word in doc:\n",
    "        if word in word2vec.wv:\n",
    "            word_emb.append(word2vec.wv[word])\n",
    "  \n",
    "    if len(word_emb) == 0:\n",
    "        return np.zeros(20)\n",
    "\n",
    "    out_emb = torch.tensor(word_emb,dtype=torch.float)\n",
    "    if len(doc) < 100000:\n",
    "        out_emb = encoder.embed(out_emb)\n",
    "    out_emb = out_emb.detach().cpu().numpy()\n",
    "    out_emb = np.mean(out_emb,axis=0)\n",
    "    return out_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    new_data = {}\n",
    "    for x in data:\n",
    "        check1 = x['object'] in ['PROCESS','FILE','FLOW','MODULE']\n",
    "        check2 = not (x['action'] in ['START','TERMINATE'])\n",
    "        check3 = x['actorID'] != x['objectID']\n",
    "        key = (x['action'],x['actorID'],x['objectID'],x['object'],x['pid'],x['ppid'])\n",
    "        if check1 and check2 and check3:\n",
    "            new_data[key] = x\n",
    "    return list(new_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    action = x[\"action\"]\n",
    "    props = x['properties']\n",
    "    typ = x['object']\n",
    "\n",
    "    phrase = ''\n",
    "    try:\n",
    "        if typ == 'PROCESS':\n",
    "            phrase = f\"{props['parent_image_path']} {action} {props['image_path']} {props['command_line']}\"    \n",
    "\n",
    "        elif typ == 'FILE':\n",
    "            phrase = f\"{props['image_path']} {action} {props['file_path']}\"    \n",
    "\n",
    "        elif typ == 'FLOW':\n",
    "            phrase = f\"{props['image_path']} {action}  {props['dest_ip']} {props['dest_port']} {props['direction']}\"    \n",
    "\n",
    "        else:\n",
    "            phrase = f\"{props['image_path']} {action} {props['module_path']}\"\n",
    "    except:\n",
    "        phrase = ''\n",
    "  \n",
    "    return phrase.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    data = preprocess(text)\n",
    "\n",
    "    temp = [describe(x) for x in data]\n",
    "    temp = [x for x in temp if len(x) != 0]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data[i]['phrase'] = temp[i]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df['timestamp'] = df['timestamp'].str[:-6]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],infer_datetime_format=True)\n",
    "    df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(dataset_id):\n",
    "    f = open(f\"content/data/hosts/{dataset_id}\")\n",
    "    content = [json.loads(line) for line in f]\n",
    "    return prepare_graph(transform(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph(df):\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    edges = []\n",
    "\n",
    "    dummies = {'PROCESS':0,'FLOW':1,'FILE':2,'MODULE':3}\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        x = df.iloc[i]\n",
    "\n",
    "        actorid = x['actorID']\n",
    "        if not (actorid in nodes):\n",
    "            nodes[actorid] = []\n",
    "        nodes[actorid] += x['phrase']\n",
    "        labels[actorid] = dummies['PROCESS']\n",
    "\n",
    "        objectid = x[\"objectID\"]\n",
    "        if not (objectid in nodes):\n",
    "            nodes[objectid] = []\n",
    "        nodes[objectid] += x['phrase']\n",
    "        labels[objectid] = dummies[x['object']]\n",
    "\n",
    "        if x['object'] == 'FLOW':\n",
    "            edges.append(( actorid, objectid, x['properties']['direction'] ))\n",
    "        else:\n",
    "            edges.append(( actorid, objectid, x['action'] ))\n",
    "\n",
    "    features = []\n",
    "    feat_labels = []\n",
    "    edge_index = [[],[]]\n",
    "    index  = {}\n",
    "    mapp = []\n",
    "              \n",
    "    for k,v in nodes.items():\n",
    "        features.append(v)\n",
    "        feat_labels.append(labels[k])\n",
    "        index[k] = len(features) - 1\n",
    "        mapp.append(k)\n",
    "\n",
    "    for x in edges:\n",
    "        src = index[x[0]]\n",
    "        dst = index[x[1]]\n",
    "    \n",
    "        if x[2] in ['READ','inbound']:\n",
    "            edge_index[0].append(dst)\n",
    "            edge_index[1].append(src)    \n",
    "        else:\n",
    "            edge_index[0].append(src)\n",
    "            edge_index[1].append(dst)    \n",
    "    \n",
    "    return features,feat_labels,edge_index,mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "Defining the model. The model consists mainly of graph sage and graph attention layers\n",
    "'''\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(20, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, 20, normalize=True)\n",
    "        self.linear = nn.Linear(in_features=20,out_features=4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.linear(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def freeze_conv_layers(self):\n",
    "        for param in self.conv1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.conv2.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self,client_id):\n",
    "        self.epoch = 0\n",
    "        self.cid = client_id\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save(f\"Content_FL_Exp/{self.cid}.model\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        pass\n",
    "        #print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        #print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "        \n",
    "def train_word2vec_func(docs,client_id):\n",
    "    logger = EpochLogger()\n",
    "    saver = EpochSaver(client_id)\n",
    "    word2vec = Word2Vec(sentences=docs, vector_size=20, window=5, min_count=1,workers=5,epochs=100,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.utils import class_weight\n",
    "import copy\n",
    "\n",
    "template = GCN().to(device)\n",
    "\n",
    "def train_gnn_func(nodes,labels,edges,tune=False):\n",
    "    \n",
    "    model = copy.deepcopy(template)\n",
    "    if \"global.pth\" in os.listdir(\"Content_FL_Exp\"):\n",
    "        model.load_state_dict(torch.load(\"Content_FL_Exp/global.pth\"))\n",
    "    \n",
    "    if tune:\n",
    "        model.freeze_conv_layers()\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        out = model(graph.x, graph.edge_index) \n",
    "        loss = criterion(out, graph.y) \n",
    "        loss.backward() \n",
    "        optimizer.step()      \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ben_proc.json', \"r\") as json_file:\n",
    "    data_cache = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_handling_loop(client_id):\n",
    "    docs,labels,edges,mapp = [None]*4\n",
    "    \n",
    "    if client_id in data_cache:\n",
    "        docs,labels,edges,mapp = data_cache[client_id]\n",
    "    else:\n",
    "        docs,labels,edges,mapp = load_data(client_id)\n",
    "        data_cache[client_id] = [docs,labels,edges,mapp]\n",
    "        \n",
    "    #if not f\"{client_id}.model\" in os.listdir(\"Content_FL_Exp\"):\n",
    "    #    train_word2vec_func(docs,client_id)\n",
    "    \n",
    "    nodes_feat = []\n",
    "    word2vec = Word2Vec.load(f\"Content_FL_Exp/word2vec4.model\")\n",
    "    for x in docs:\n",
    "        nodes_feat.append( infer(x,word2vec) ) \n",
    "        \n",
    "    trained_client_model = train_gnn_func(nodes_feat,labels,edges)\n",
    "    torch.save(trained_client_model.state_dict(), f\"Content_FL_Exp/{client_id}.pth\")\n",
    "    return trained_client_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_cache = {}\\nfor client_id in [\\'201.txt\\',\\'501.txt\\',\\'051.txt\\']:\\n        \\n    docs,labels,edges,mapp = load_data(client_id)\\n    data_cache[client_id] = [docs,labels,edges,mapp]\\n\\nfile_path = \"ben_proc.json\"\\n\\n# Write the dictionary to the JSON file\\nwith open(file_path, \"w\") as json_file:\\n    json.dump(data_cache, json_file)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data_cache = {}\n",
    "for client_id in ['201.txt','501.txt','051.txt']:\n",
    "        \n",
    "    docs,labels,edges,mapp = load_data(client_id)\n",
    "    data_cache[client_id] = [docs,labels,edges,mapp]\n",
    "\n",
    "file_path = \"ben_proc.json\"\n",
    "\n",
    "# Write the dictionary to the JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data_cache, json_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def perform_federated_learning(n_clients):\n",
    "    client_models = []\n",
    "    for c in n_clients:\n",
    "        gnn = client_handling_loop(c)\n",
    "        client_models.append(gnn)\n",
    "    return client_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(client_models):\n",
    "    global_model = copy.deepcopy(template)\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        param_list = [client_models[i].state_dict()[k] for i in range(len(client_models))]\n",
    "        global_dict[k] = torch.stack(param_list, 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    torch.save(global_model.state_dict(), \"Content_FL_Exp/global.pth\")\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'Content_FL_Exp/*.pth': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm Content_FL_Exp/*.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(client_id):\n",
    "    docs,labels,edges,mapp = data_cache[client_id]\n",
    "        \n",
    "    #if not f\"{client_id}.model\" in os.listdir(\"Content_FL_Exp\"):\n",
    "    #    train_word2vec_func(docs,client_id)\n",
    "    \n",
    "    nodes_feat = []\n",
    "    word2vec = Word2Vec.load(f\"Content_FL_Exp/word2vec4.model\")\n",
    "    for x in docs:\n",
    "        nodes_feat.append( infer(x,word2vec) ) \n",
    "        \n",
    "    trained_client_model = train_gnn_func(nodes_feat,labels,edges,tune=True)\n",
    "    torch.save(trained_client_model.state_dict(), f\"Content_FL_Exp/{client_id}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n_clients = ['201.txt','501.txt','051.txt']\n",
    "learning_rounds = 10\n",
    "\n",
    "for r in range(learning_rounds):\n",
    "    print(f\"Federated Learning Round Number: {r}\\n\")\n",
    "    client_models = perform_federated_learning(n_clients)\n",
    "    global_model = server_aggregate(client_models)\n",
    "\n",
    "for cid in n_clients:\n",
    "    finetune(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(MP,acts,objs,GP,edges,mapp):\n",
    "\n",
    "    all_pids = acts.union(objs)\n",
    "    GN = all_pids - GP\n",
    "    MN = all_pids - MP\n",
    "\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP.intersection(GN)\n",
    "    FN = MN.intersection(GP)\n",
    "    \n",
    "    two_hop_gp = construct_neighborhood(GP,mapp,edges,2)\n",
    "    two_hop_tp = construct_neighborhood(TP,mapp,edges,2)\n",
    "    FP = FP - two_hop_gp\n",
    "    TP = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "\n",
    "    TP,FP,FN = len(TP),len(FP),len(FN)\n",
    "    TN = (len(acts) + len(objs)) - TP - FP - FN\n",
    "    \n",
    "    FPR = FP / (FP+TN)\n",
    "    TPR = TP / (TP+FN)\n",
    "    \n",
    "    return TP,FP,FN,TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def construct_neighborhood(ids,mapp,edges,hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    else:\n",
    "        neighbors = set()\n",
    "        for i in range(len(edges[0])):\n",
    "            if mapp[edges[0][i]] in ids:\n",
    "                neighbors.add(mapp[edges[1][i]])\n",
    "            if mapp[edges[1][i]] in ids:\n",
    "                neighbors.add(mapp[edges[0][i]])\n",
    "        return neighbors.union( construct_neighborhood(neighbors,mapp,edges,hops-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mal_proc.json', \"r\") as json_file:\n",
    "    data_cache_mal = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath = f\"Content_FL_Exp/eval_data/SysClient0402.systemia.com.txt\"\\nf = open(path)\\ncontent = [json.loads(line) for line in f]\\ndocs,labels,edges,mapp = prepare_graph(transform(content))\\ndata_cache_mal[\\'402\\'] = [docs,labels,edges,mapp]\\n\\nfile_path = \"mal_proc.json\"\\n\\n# Write the dictionary to the JSON file\\nwith open(file_path, \"w\") as json_file:\\n    json.dump(data_cache_mal, json_file)\\n\\ntrain_word2vec_func(docs,\\'402.txt\\')\\n\\nGT_mal,acts,objs = ground_truth(\\'402\\')\\nwith open(\\'gt_402.json\\', \"w\") as json_file:\\n    json.dump([list(GT_mal,acts,objs)], json_file)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "path = f\"Content_FL_Exp/eval_data/SysClient0402.systemia.com.txt\"\n",
    "f = open(path)\n",
    "content = [json.loads(line) for line in f]\n",
    "docs,labels,edges,mapp = prepare_graph(transform(content))\n",
    "data_cache_mal['402'] = [docs,labels,edges,mapp]\n",
    "\n",
    "file_path = \"mal_proc.json\"\n",
    "\n",
    "# Write the dictionary to the JSON file\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(data_cache_mal, json_file)\n",
    "\n",
    "train_word2vec_func(docs,'402.txt')\n",
    "\n",
    "GT_mal,acts,objs = ground_truth('402')\n",
    "with open('gt_402.json', \"w\") as json_file:\n",
    "    json.dump([list(GT_mal,acts,objs)], json_file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(fm):\n",
    "    \n",
    "    TP,FP,FN,TN = 0,0,0,0\n",
    "    for data_id in ['201','501','051']:\n",
    "        \n",
    "        #path = f\"Content_FL_Exp/eval_data/SysClient0{data_id}.systemia.com.txt\"\n",
    "        #f = open(path)\n",
    "        #content = [json.loads(line) for line in f]\n",
    "        #docs,labels,edges,mapp = prepare_graph(transform(content))\n",
    "        docs,labels,edges,mapp = data_cache_mal[data_id]\n",
    "\n",
    "        nodes_feat = []\n",
    "        word2vec = Word2Vec.load(f\"Content_FL_Exp/word2vec4.model\")\n",
    "        for x in docs:\n",
    "            nodes_feat.append( infer(x,word2vec) ) \n",
    "\n",
    "        #gt,acts,objs = ground_truth(data_id)\n",
    "        \n",
    "        with open(f\"gt_{data_id}.json\", \"r\") as json_file:\n",
    "            gt,acts,objs = json.load(json_file)  \n",
    "        \n",
    "        gt,acts,objs = set(gt),set(acts),set(objs)\n",
    "            \n",
    "        graph = Data(x=torch.tensor(nodes_feat,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "        graph.x = nn.functional.normalize(graph.x)\n",
    "\n",
    "        model = GCN().to(device)\n",
    "        \n",
    "        if fm:\n",
    "            model.load_state_dict(torch.load(f\"Content_FL_Exp/global.pth\",map_location=torch.device('cpu')))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(f\"Content_FL_Exp/{data_id}.txt.pth\",map_location=torch.device('cpu')))\n",
    "\n",
    "            \n",
    "        model.eval()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "\n",
    "        sorted, indices = out.sort(dim=1,descending=True)\n",
    "        conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "        conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "        pred = indices[:,0]\n",
    "        cond = (pred == graph.y)#  & (conf > 0.9)\n",
    "\n",
    "        index = utils.mask_to_index(~cond).tolist()\n",
    "        ids = set([mapp[x] for x in index])\n",
    "        metrics = helper(set(ids),acts,objs,gt,edges,mapp) \n",
    "        \n",
    "        TP = TP + metrics[0]\n",
    "        FP = FP + metrics[1]\n",
    "        FN = FN + metrics[2]\n",
    "        TN = TN + metrics[3]\n",
    "    \n",
    "    print(f\"Number of True Positives: {TP}\")\n",
    "    print(f\"Number of Fasle Positives: {FP}\")\n",
    "    print(f\"Number of False Negatives: {FN}\")\n",
    "    print(f\"Number of True Negatives: {TN}\\n\")\n",
    "\n",
    "    prec = TP / (TP + FP)\n",
    "    print(f\"Precision: {prec}\")\n",
    "\n",
    "    rec = TP / (TP + FN)\n",
    "    print(f\"Recall: {rec}\")\n",
    "\n",
    "    fscore = (2*prec*rec) / (prec + rec)\n",
    "    print(f\"Fscore: {fscore}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives: 624\n",
      "Number of Fasle Positives: 71\n",
      "Number of False Negatives: 26\n",
      "Number of True Negatives: 1287284\n",
      "\n",
      "Precision: 0.897841726618705\n",
      "Recall: 0.96\n",
      "Fscore: 0.9278810408921933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_evaluation(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
