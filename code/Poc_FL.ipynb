{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding():\n",
    "\n",
    "    def __init__(self, d_model ,max_len = 100000):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        self.pe[:,0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "encoder = PositionalEncoding(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def infer(doc,word2vec):  \n",
    "    word_emb = []\n",
    "    for word in doc:\n",
    "        if word in word2vec.wv:\n",
    "            word_emb.append(word2vec.wv[word])\n",
    "  \n",
    "    if len(word_emb) == 0:\n",
    "        return np.zeros(20)\n",
    "\n",
    "    out_emb = torch.tensor(word_emb,dtype=torch.float)\n",
    "    if len(doc) < 100000:\n",
    "        out_emb = encoder.embed(out_emb)\n",
    "    out_emb = out_emb.detach().cpu().numpy()\n",
    "    out_emb = np.mean(out_emb,axis=0)\n",
    "    return out_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    new_data = {}\n",
    "    for x in data:\n",
    "        check1 = x['object'] in ['PROCESS','FILE','FLOW','MODULE']\n",
    "        check2 = not (x['action'] in ['START','TERMINATE'])\n",
    "        check3 = x['actorID'] != x['objectID']\n",
    "        key = (x['action'],x['actorID'],x['objectID'],x['object'],x['pid'],x['ppid'])\n",
    "        if check1 and check2 and check3:\n",
    "            new_data[key] = x\n",
    "    return list(new_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    action = x[\"action\"]\n",
    "    props = x['properties']\n",
    "    typ = x['object']\n",
    "\n",
    "    phrase = ''\n",
    "    try:\n",
    "        if typ == 'PROCESS':\n",
    "            phrase = f\"{props['parent_image_path']} {action} {props['image_path']} {props['command_line']}\"    \n",
    "\n",
    "        elif typ == 'FILE':\n",
    "            phrase = f\"{props['image_path']} {action} {props['file_path']}\"    \n",
    "\n",
    "        elif typ == 'FLOW':\n",
    "            phrase = f\"{props['image_path']} {action}  {props['dest_ip']} {props['dest_port']} {props['direction']}\"    \n",
    "\n",
    "        else:\n",
    "            phrase = f\"{props['image_path']} {action} {props['module_path']}\"\n",
    "    except:\n",
    "        phrase = ''\n",
    "  \n",
    "    return phrase.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    data = preprocess(text)\n",
    "\n",
    "    temp = [describe(x) for x in data]\n",
    "    temp = [x for x in temp if len(x) != 0]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data[i]['phrase'] = temp[i]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df['timestamp'] = df['timestamp'].str[:-6]\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'],infer_datetime_format=True)\n",
    "    df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(dataset_id):\n",
    "    f = open(f\"content/data/hosts/{dataset_id}\")\n",
    "    content = [json.loads(line) for line in f]\n",
    "    return prepare_graph(transform(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph(df):\n",
    "  nodes = {}\n",
    "  labels = {}\n",
    "  edges = []\n",
    "    \n",
    "  dummies = {'PROCESS':0,'FLOW':1,'FILE':2,'MODULE':3}\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    x = df.iloc[i]\n",
    "\n",
    "    actorid = x['actorID']\n",
    "    if not (actorid in nodes):\n",
    "      nodes[actorid] = []\n",
    "    nodes[actorid] += x['phrase']\n",
    "    labels[actorid] = dummies['PROCESS']\n",
    "\n",
    "    objectid = x[\"objectID\"]\n",
    "    if not (objectid in nodes):\n",
    "      nodes[objectid] = []\n",
    "    nodes[objectid] += x['phrase']\n",
    "    labels[objectid] = dummies[x['object']]\n",
    "    \n",
    "    if x['object'] == 'FLOW':\n",
    "        edges.append(( actorid, objectid, x['properties']['direction'] ))\n",
    "    else:\n",
    "        edges.append(( actorid, objectid, x['action'] ))\n",
    "\n",
    "  features = []\n",
    "  feat_labels = []\n",
    "  edge_index = [[],[]]\n",
    "  index  = {}\n",
    "  mapp = []\n",
    "              \n",
    "  for k,v in nodes.items():\n",
    "    features.append(v)\n",
    "    feat_labels.append(labels[k])\n",
    "    index[k] = len(features) - 1\n",
    "    mapp.append(k)\n",
    "\n",
    "  for x in edges:\n",
    "    src = index[x[0]]\n",
    "    dst = index[x[1]]\n",
    "    \n",
    "    if x[2] in ['READ','inbound']:\n",
    "        edge_index[0].append(dst)\n",
    "        edge_index[1].append(src)    \n",
    "    else:\n",
    "        edge_index[0].append(src)\n",
    "        edge_index[1].append(dst)    \n",
    "    \n",
    "  return features,np.array(feat_labels),edge_index,mapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "Defining the model. The model consists mainly of graph sage and graph attention layers\n",
    "'''\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(20, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, 20, normalize=True)\n",
    "        self.linear = nn.Linear(in_features=20,out_features=4)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.linear(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self,client_id):\n",
    "        self.epoch = 0\n",
    "        self.cid = client_id\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save(f\"Content_FL/{self.cid}.model\")\n",
    "        self.epoch += 1\n",
    "        \n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        pass\n",
    "        #print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        #print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1\n",
    "        \n",
    "def train_word2vec_func(docs,client_id):\n",
    "    logger = EpochLogger()\n",
    "    saver = EpochSaver(client_id)\n",
    "    word2vec = Word2Vec(sentences=docs, vector_size=20, window=5, min_count=1,workers=5,epochs=100,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def train_gnn_func(nodes,labels,edges):\n",
    "    \n",
    "    model = GCN().to(device)\n",
    "    if \"global.pth\" in os.listdir(\"Content_FL\"):\n",
    "        model.load_state_dict(torch.load(\"Content_FL/global.pth\"))\n",
    "   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    \n",
    "    l = np.array(labels)\n",
    "    class_weights = class_weight.compute_class_weight(class_weight = \"balanced\",classes = np.unique(l),y = l)\n",
    "    class_weights = torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "    criterion = CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        out = model(graph.x, graph.edge_index) \n",
    "        loss = criterion(out, graph.y) \n",
    "        loss.backward() \n",
    "        optimizer.step()      \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    out = model(graph.x, graph.edge_index) \n",
    "    sort, indices = out.sort(dim=1,descending=True)\n",
    "    \n",
    "    pred = indices[:,0]\n",
    "    acc = ((pred == graph.y).sum()) / graph.num_nodes\n",
    "    print(f\"Local Model Accuracy: {acc}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "def train_classifier(client_id):\n",
    "    docs,labels,edges,mapp = load_data(client_id)\n",
    "    \n",
    "    wemb = []\n",
    "    word2vec = Word2Vec.load(f\"Content_FL/{client_id}.model\")\n",
    "    for x in docs:\n",
    "        wemb.append( infer(x,word2vec) ) \n",
    "\n",
    "    model = GCN().to(device)\n",
    "    model.load_state_dict(torch.load(f'Content_FL/global.pth'))\n",
    "    \n",
    "    graph = Data(x=torch.tensor(wemb,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "\n",
    "    gemb = model.encode(graph.x, graph.edge_index).detach().cpu().numpy() \n",
    "    x = np.hstack((wemb,gemb))\n",
    "    y = np.array(labels)\n",
    "\n",
    "    xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "    xgb_cl.fit(x,y)\n",
    "    pickle.dump(xgb_cl, open(f\"Content_FL/{client_id}.pkl\", \"wb\"))\n",
    "\n",
    "    preds = xgb_cl.predict(x)\n",
    "    print(f\"Classification Accuracy :{accuracy_score(y, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_handling_loop(client_id):\n",
    "    docs,labels,edges,mapp = load_data(client_id)\n",
    "    if not f\"{client_id}.model\" in os.listdir(\"Content_FL\"):\n",
    "        train_word2vec_func(docs,client_id)\n",
    "    \n",
    "    nodes_feat = []\n",
    "    word2vec = Word2Vec.load(f\"Content_FL/{client_id}.model\")\n",
    "    for x in docs:\n",
    "        nodes_feat.append( infer(x,word2vec) ) \n",
    "        \n",
    "    trained_client_model = train_gnn_func(nodes_feat,labels,edges)\n",
    "    torch.save(trained_client_model.state_dict(), f\"Content_FL/{client_id}.pth\")\n",
    "    return trained_client_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def perform_federated_learning(n_clients):\n",
    "    client_models = []\n",
    "    for c in n_clients:\n",
    "        print(f\"Client: {c}\")\n",
    "        gnn = client_handling_loop(c)\n",
    "        client_models.append(gnn)\n",
    "    return client_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(client_models):\n",
    "    global_model = GCN().to(device)\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    torch.save(global_model.state_dict(), \"Content_FL/global.pth\")\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n_clients = ['201.txt','501.txt','SysClient0051.systemia.com.txt','SysClient0721.systemia.com.txt','SysClient0113.systemia.com.txt','SysClient0122.systemia.com.txt','SysClient0170.systemia.com.txt','SysClient0316.systemia.com.txt','SysClient0520.systemia.com.txt']\n",
    "\n",
    "learning_rounds = 3\n",
    "\n",
    "for r in range(learning_rounds):\n",
    "    print(f\"Federated Learning Round Number: {r}\\n\")\n",
    "    client_models = perform_federated_learning(n_clients)\n",
    "    global_model = server_aggregate(client_models)\n",
    "    \n",
    "for cid in n_clients[:3]:\n",
    "    print(\"Training Classifiers\")\n",
    "    train_classifier(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "def helper(MP,acts,objs,GP,edges,mapp):\n",
    "\n",
    "    all_pids = acts.union(objs)\n",
    "    GN = all_pids - GP\n",
    "    MN = all_pids - MP\n",
    "\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP.intersection(GN)\n",
    "    FN = MN.intersection(GP)\n",
    "    \n",
    "    two_hop_gp = construct_neighborhood(GP,mapp,edges,2)\n",
    "    two_hop_tp = construct_neighborhood(TP,mapp,edges,2)\n",
    "    FP = FP - two_hop_gp\n",
    "    TP = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "\n",
    "    TP,FP,FN = len(TP),len(FP),len(FN)\n",
    "    TN = (len(acts) + len(objs)) - TP - FP - FN\n",
    "    \n",
    "    FPR = FP / (FP+TN)\n",
    "    TPR = TP / (TP+FN)\n",
    "\n",
    "    print(f\"Number of True Positives: {TP}\")\n",
    "    print(f\"Number of Fasle Positives: {FP}\")\n",
    "    print(f\"Number of False Negatives: {FN}\")\n",
    "    print(f\"Number of True Negatives: {TN}\\n\")\n",
    "\n",
    "    prec = TP / (TP + FP)\n",
    "    print(f\"Precision: {prec}\")\n",
    "\n",
    "    rec = TP / (TP + FN)\n",
    "    print(f\"Recall: {rec}\")\n",
    "\n",
    "    fscore = (2*prec*rec) / (prec + rec)\n",
    "    print(f\"Fscore: {fscore}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(data_id):\n",
    "    file = data_id\n",
    "    \n",
    "    path = f\"Content_FL/eval_data/SysClient0{data_id}.systemia.com.txt\"\n",
    "    f = open(path,'r')\n",
    "    content = [json.loads(x) for x in f]\n",
    "    \n",
    "    raw = pd.DataFrame.from_dict(content)\n",
    "    acts = set(raw['actorID']) \n",
    "    objs = set(raw['objectID'])\n",
    "    \n",
    "    thresh = 0\n",
    "    GT_mal = set()\n",
    "    if file == '501':\n",
    "        f = open('content/optc_gt_day2.json')\n",
    "        GT_mal = set(json.load(f))\n",
    "        thresh=0.8\n",
    "\n",
    "    if file == '201':\n",
    "        f = open('content/optc_gt_day1.json')\n",
    "        GT_mal = set(json.load(f))\n",
    "        thresh=0\n",
    "\n",
    "    if file == '051':\n",
    "        f = open('content/optc_gt_day3.json')\n",
    "        GT_mal = set(json.load(f))\n",
    "        thresh=0.8\n",
    "           \n",
    "    return GT_mal,acts,objs,thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def construct_neighborhood(ids,mapp,edges,hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    else:\n",
    "        neighbors = set()\n",
    "        for i in range(len(edges[0])):\n",
    "            if mapp[edges[0][i]] in ids:\n",
    "                neighbors.add(mapp[edges[1][i]])\n",
    "            if mapp[edges[1][i]] in ids:\n",
    "                neighbors.add(mapp[edges[0][i]])\n",
    "        return neighbors.union( construct_neighborhood(neighbors,mapp,edges,hops-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = '051'\n",
    "client_id = 'SysClient0051.systemia.com.txt'\n",
    "\n",
    "f = open(f\"Content_FL/eval_data/SysClient0{data_id}.systemia.com.txt\")\n",
    "content = [json.loads(line) for line in f]\n",
    "\n",
    "docs,labels,edges,mapp = prepare_graph(transform(content))\n",
    "\n",
    "wemb = []\n",
    "word2vec = Word2Vec.load(f\"Content_FL/{client_id}.model\")\n",
    "for x in docs:\n",
    "    wemb.append( infer(x,word2vec) ) \n",
    "\n",
    "graph = Data(x=torch.tensor(wemb,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "\n",
    "model = GCN().to(device)\n",
    "model.load_state_dict(torch.load(f'Content_FL/global.pth'))\n",
    "model.eval()\n",
    "\n",
    "gemb = model.encode(graph.x, graph.edge_index).detach().cpu().numpy() \n",
    "x = np.hstack((wemb,gemb))\n",
    "y = np.array(labels)\n",
    "\n",
    "gt,acts,objs,thresh = ground_truth(data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "xgb_cl = load_pkl(f\"Content_FL/{client_id}.pkl\")\n",
    "\n",
    "pred = xgb_cl.predict(x)\n",
    "proba = xgb_cl.predict_proba(x)\n",
    "\n",
    "sorted = np.sort(proba, axis=1)\n",
    "conf = (sorted[:,-1] - sorted[:,-2]) / sorted[:,-1]\n",
    "conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "check = (pred == y) & (conf > thresh)\n",
    "flag = ~torch.tensor(check)\n",
    "\n",
    "index = utils.mask_to_index(flag).tolist()\n",
    "ids = set([mapp[x] for x in index])\n",
    "helper(set(ids),acts,objs,gt,edges,mapp) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
