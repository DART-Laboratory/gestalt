{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F1op-CbyLuN4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the require libraries here\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import orjson as json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import multiprocessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be/Notebooks_FL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries and setting up working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/weka/scratch/wkw9be/content'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "Setting working directory for rivana jupyter notebook\n",
    "'''\n",
    "os.chdir(\"../content\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nM7KaeCbA_mQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Importing some additional libraries\n",
    "'''\n",
    "from pprint import pprint\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for loading, cleaning and constructing features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BfjmrhfUr3pK"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the main featurizer. It constructs the graph for the cadets dataset.\n",
    "\n",
    "Args:\n",
    "    df (DataFrame): This is the main dataframe containing all the system events from the cadets dataset.\n",
    "\n",
    "return:\n",
    "    features (list): Contains word2vec encoded feature vectors for each node\n",
    "    feat_labels (list): Contains label for each node\n",
    "    edge_index (list): Contains information about edges between nodes in the graph.\n",
    "    mapp (list): contains id of each node\n",
    "'''\n",
    "\n",
    "def prepare_graph(df):\n",
    "    nodes = {}\n",
    "    labels = {}\n",
    "    edges = []\n",
    "    \n",
    "    dummies = {'SUBJECT_PROCESS':\t0, 'FILE_OBJECT_FILE':\t1, 'FILE_OBJECT_UNIX_SOCKET':\t2, 'UnnamedPipeObject':\t3, 'NetFlowObject':\t4, 'FILE_OBJECT_DIR':\t5}\n",
    "    for i in range(len(df)):\n",
    "        x = df.iloc[i]\n",
    "        action = x[\"action\"]\n",
    "        \n",
    "        actorid = x[\"actorID\"]\n",
    "        if not (actorid in nodes):\n",
    "            nodes[actorid] =  []\n",
    "        nodes[actorid].append(x['exec'])\n",
    "        nodes[actorid].append(action)\n",
    "        if x['path'] != '':\n",
    "            nodes[actorid].append(x['path'])\n",
    "        labels[actorid] = dummies[x['actor_type']]\n",
    "\n",
    "        objectid = x[\"objectID\"]\n",
    "        if not (objectid in nodes):\n",
    "            nodes[objectid] =  []\n",
    "        nodes[objectid].append(x['exec'])\n",
    "        nodes[objectid].append(action)\n",
    "        if x['path'] != '':\n",
    "             nodes[objectid].append(x['path'])\n",
    "        labels[objectid] = dummies[x['object']]\n",
    "\n",
    "        edges.append(( actorid, objectid ))\n",
    "\n",
    "    features = []\n",
    "    feat_labels = []\n",
    "    edge_index = [[],[]]\n",
    "    index  = {}\n",
    "    mapp = []\n",
    "\n",
    "    for k,v in nodes.items():\n",
    "      features.append(v)\n",
    "      feat_labels.append(labels[k])\n",
    "      index[k] = len(features) - 1\n",
    "      mapp.append(k)\n",
    "\n",
    "    for x in edges:\n",
    "        src = index[x[0]]\n",
    "        dst = index[x[1]]\n",
    "\n",
    "        edge_index[0].append(src)\n",
    "        edge_index[1].append(dst)\n",
    "\n",
    "    return features,feat_labels,edge_index,mapp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fmXWs1dKIzD8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the model. The model consists of two sageconv layers from the paper GraphSage\n",
    "'''\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channel, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, out_channel, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YBuP_tSq94f4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function helps visualize the output of the model.\n",
    "'''\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PCP6SXwZaif"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This saves the word2vec model after each iteration\n",
    "'''\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('word2vec_cadets_E3.model')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "P8oBL8LFaeOf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This logs the epoch number to the console\n",
    "'''\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Se7Ei4tAapVj"
   },
   "outputs": [],
   "source": [
    "logger = EpochLogger()\n",
    "saver = EpochSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HvH1OSGqgTe7"
   },
   "outputs": [],
   "source": [
    "f = open(\"darpatc/cadets_train.txt\")\n",
    "data = f.read().split('\\n')\n",
    "data = [line.split('\\t') for line in data]\n",
    "df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "df = df.dropna()\n",
    "df.sort_values(by='timestamp', ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding semantic attributes from the raw cadets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used for attributing semnatic information like process names, executable paths,\n",
    "file paths etc using the raw cadets data\n",
    "'''\n",
    "\n",
    "def add_attributes(d,p):\n",
    "    \n",
    "    f = open(p)\n",
    "    data = [json.loads(x) for x in f if \"EVENT\" in x]\n",
    "\n",
    "    info = []\n",
    "    for x in data:\n",
    "        try:\n",
    "            action = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['type']\n",
    "        except:\n",
    "            action = ''\n",
    "        try:\n",
    "            actor = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['subject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            actor = ''\n",
    "        try:\n",
    "            obj = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            obj = ''\n",
    "        try:\n",
    "            timestamp = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['timestampNanos']\n",
    "        except:\n",
    "            timestamp = ''\n",
    "        try:\n",
    "            cmd = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['properties']['map']['exec']\n",
    "        except:\n",
    "            cmd = ''\n",
    "        try:\n",
    "            path = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObjectPath']['string']\n",
    "        except:\n",
    "            path = ''\n",
    "        try:\n",
    "            path2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2Path']['string']\n",
    "        except:\n",
    "            path2 = ''\n",
    "        try:\n",
    "            obj2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "            info.append({'actorID':actor,'objectID':obj2,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path2})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        info.append({'actorID':actor,'objectID':obj,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path})\n",
    "\n",
    "    rdf = pd.DataFrame.from_records(info).astype(str)\n",
    "    d = d.astype(str)\n",
    "\n",
    "    return d.merge(rdf,how='inner',on=['actorID','objectID','action','timestamp']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_word2vec_models(models):\n",
    "    # Create an empty unified model\n",
    "    unified_model = Word2Vec(vector_size=models[0].vector_size, window=models[0].window, min_count=models[0].min_count, sg=models[0].sg)\n",
    "\n",
    "    # Initialize the vocabulary with the words from the first model\n",
    "    unified_model.build_vocab([list(models[0].wv.index_to_key)])\n",
    "\n",
    "    # Copy the vectors from the first model to the unified model for the initial vocabulary\n",
    "    for word in unified_model.wv.index_to_key:\n",
    "        unified_model.wv[word] = models[0].wv[word]\n",
    "\n",
    "    # Iterate through the remaining models and add their unique words and vectors\n",
    "    for model in models[1:]:\n",
    "        # Get the set of unique words in the current model's vocabulary\n",
    "        unique_words = set(model.wv.index_to_key) - set(unified_model.wv.index_to_key)\n",
    "\n",
    "        # Add the unique words to the unified model's vocabulary\n",
    "        unified_model.build_vocab([list(unique_words)], update=True)\n",
    "\n",
    "        # Copy the vectors for the unique words from the current model to the unified model\n",
    "        for word in unique_words:\n",
    "            unified_model.wv[word] = model.wv[word]\n",
    "\n",
    "    return unified_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, n):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into n sub-dataframes of approximately equal size.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame to be split\n",
    "    - n: Number of sub-dataframes to create\n",
    "    \n",
    "    Returns:\n",
    "    - A list of n sub-dataframes\n",
    "    \"\"\"\n",
    "    # Calculate the size of each chunk\n",
    "    chunk_size = len(df) // n\n",
    "    \n",
    "    # Initialize an empty list to store the sub-dataframes\n",
    "    sub_dfs = []\n",
    "    \n",
    "    # Loop through the DataFrame and create sub-dataframes\n",
    "    for i in range(n):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size if i != n - 1 else len(df)\n",
    "        sub_dfs.append(df.iloc[start_idx:end_idx])\n",
    "        \n",
    "    return sub_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_attributes(df,\"ta1-cadets-e3-official.json.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_dfs = split_dataframe(df,3)\n",
    "client_out = [prepare_graph(x) for x in client_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_models = []\n",
    "for data in client_out:\n",
    "    phrases,labels,edges,mapp = data\n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=30, window=5, min_count=1, workers=8,epochs=300)\n",
    "    word_models.append(word2vec)\n",
    "\n",
    "global_word = combine_word2vec_models(word_models)\n",
    "global_word.save(\"../Content_FL_Exp/global_word2vec_cadets.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrases,labels,edges,mapp = prepare_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RDmGME5iPb5"
   },
   "outputs": [],
   "source": [
    "#word2vec = Word2Vec(sentences=phrases, vector_size=30, window=5, min_count=1, workers=8,epochs=300,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3TAi69zI1bO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining the train and test function in this cell \n",
    "'''\n",
    "from sklearn.utils import class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "template = GCN(30,6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vn_pMyt5Jd-6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Encoding function for running word2vec inference\n",
    "'''\n",
    "from collections import Counter\n",
    "word2vec = Word2Vec.load(\"../Content_FL_Exp/global_word2vec_cadets.model\")\n",
    "\n",
    "def infer(doc):\n",
    "  temp = dict(Counter(doc))\n",
    "  emb = np.zeros(30)\n",
    "  count = 0\n",
    "  for k,v in temp.items():\n",
    "    if k in word2vec.wv:\n",
    "      emb = emb + word2vec.wv[k]*v\n",
    "      count = count + 1\n",
    "  emb = emb / count\n",
    "  return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_client_gnn(client_data):\n",
    "    ################################## Training Main Model #####################################\n",
    "    model = copy.deepcopy(template)\n",
    "    if \"cadets_global.pth\" in os.listdir(\"../Content_FL_Exp\"):\n",
    "        model.load_state_dict(torch.load(\"../Content_FL_Exp/cadets_global.pth\"))\n",
    "        \n",
    "    phrases,labels,edges,mapp = client_data\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)  \n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        out = model(graph.x, graph.edge_index) \n",
    "        loss = criterion(out, graph.y) \n",
    "        loss.backward() \n",
    "        optimizer.step()      \n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_aggregate(client_models):\n",
    "    global_model = copy.deepcopy(template)\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        param_list = [client_models[i].state_dict()[k] for i in range(len(client_models))]\n",
    "        global_dict[k] = torch.stack(param_list, 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    torch.save(global_model.state_dict(), \"../Content_FL_Exp/cadets_global.pth\")\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rounds = 10\n",
    "\n",
    "for r in range(learning_rounds):\n",
    "    print(f\"Learning Round {r}\")\n",
    "   \n",
    "    client_models = []\n",
    "    for i in range(len(client_out)):\n",
    "        print(f\"Training Client Model {i}\")\n",
    "        cmodel = train_client_gnn(client_out[i])\n",
    "        client_models.append(cmodel)\n",
    "    \n",
    "    global_model = server_aggregate(client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the trained GNN model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKJ53Fh5ogvy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function is used for constructing neighborhood around a given \n",
    "set of nodes for backwards or forward tracking\n",
    "'''\n",
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def construct_neighborhood(ids,mapp,edges,hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    else:\n",
    "        neighbors = set()\n",
    "        for i in range(len(edges[0])):\n",
    "            if mapp[edges[0][i]] in ids:\n",
    "                neighbors.add(mapp[edges[1][i]])\n",
    "            if mapp[edges[1][i]] in ids:\n",
    "                neighbors.add(mapp[edges[0][i]])\n",
    "        return neighbors.union( construct_neighborhood(neighbors,mapp,edges,hops-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgqyu7E5qPet"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This function logs the evaluation metrics.\n",
    "'''\n",
    "\n",
    "def helper(MP,all_pids,GP,edges,mapp):\n",
    "\n",
    "    GN = all_pids - GP\n",
    "    MN = all_pids - MP\n",
    "\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP.intersection(GN)\n",
    "    FN = MN.intersection(GP)\n",
    "    TN = MN.intersection(GN)\n",
    "    \n",
    "    two_hop_gp = construct_neighborhood(GP,mapp,edges,2)\n",
    "    two_hop_tp = construct_neighborhood(TP,mapp,edges,2)\n",
    "    FPL = FP - two_hop_gp\n",
    "    TPL = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "    \n",
    "    alerts = TP.union(FP)\n",
    "\n",
    "    TP,FP,FN,TN = len(TPL),len(FPL),len(FN),len(TN)\n",
    "    \n",
    "    FPR = FP / (FP+TN)\n",
    "    TPR = TP / (TP+FN)\n",
    "\n",
    "    print(f\"Number of True Positives: {TP}\")\n",
    "    print(f\"Number of Fasle Positives: {FP}\")\n",
    "    print(f\"Number of False Negatives: {FN}\")\n",
    "    print(f\"Number of True Negatives: {TN}\\n\")\n",
    "\n",
    "    prec = TP / (TP + FP)\n",
    "    print(f\"Precision: {prec}\")\n",
    "\n",
    "    rec = TP / (TP + FN)\n",
    "    print(f\"Recall: {rec}\")\n",
    "\n",
    "    fscore = (2*prec*rec) / (prec + rec)\n",
    "    print(f\"Fscore: {fscore}\\n\")\n",
    "    \n",
    "    #return alerts\n",
    "    return TPL,FPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZFrSLVZ29qU"
   },
   "outputs": [],
   "source": [
    "f = open(\"darpatc/cadets_test.txt\")\n",
    "data = f.read().split('\\n')\n",
    "data = [line.split('\\t') for line in data]\n",
    "df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "df = df.dropna()\n",
    "df.sort_values(by='timestamp', ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_attributes(df,\"ta1-cadets-e3-official-2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = open(\"cadets.txt\").read()\n",
    "GT_mal = set(gt.split(\"\\n\"))\n",
    "\n",
    "data = df\n",
    "\n",
    "phrases,labels,edges,mapp = prepare_graph(data)\n",
    "\n",
    "nodes = [infer(x) for x in phrases]\n",
    "nodes = np.array(nodes)  \n",
    "\n",
    "all_ids = list(data['actorID']) + list(data['objectID'])\n",
    "all_ids = set(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(30,6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311255,
     "status": "ok",
     "timestamp": 1673577553815,
     "user": {
      "displayName": "Mati Ur Rehman",
      "userId": "04281203290774044297"
     },
     "user_tz": 300
    },
    "id": "DsLlVS6zpox5",
    "outputId": "e79153e2-2e54-4cbf-c241-1293e0ee2be6"
   },
   "outputs": [],
   "source": [
    "graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../Content_FL_Exp/cadets_global.pth'))\n",
    "model.eval()\n",
    "out = model(graph.x, graph.edge_index)\n",
    "\n",
    "sorted, indices = out.sort(dim=1,descending=True)\n",
    "conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "pred = indices[:,0]\n",
    "cond = (pred == graph.y)# & (conf >= 0.2)\n",
    "flag[cond] = torch.logical_and(flag[cond], torch.tensor([False]*len(flag[cond]), dtype=torch.bool))\n",
    "\n",
    "index = utils.mask_to_index(flag).tolist()\n",
    "ids = set([mapp[x] for x in index])\n",
    "TPL,FPL = helper(set(ids),set(all_ids),GT_mal,edges,mapp) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
